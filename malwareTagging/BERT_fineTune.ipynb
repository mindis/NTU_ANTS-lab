{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Bert pre-train language model\n",
    "* REF: https://github.com/huggingface/pytorch-pretrained-BERT/tree/master/examples/lm_finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Format\n",
    "* expect a single file as input, consisting of untokenized text, with one sentence per line, and one blank line between documents.\n",
    "\n",
    "- using profile to concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import torch\n",
    "from pytorch_pretrained_bert import *\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from apex.optimizers import FP16_Optimizer\n",
    "from multiprocessing import Pool, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def par(file):\n",
    "#     print(file)\n",
    "    max_ = 0\n",
    "    with open(files[0]+file , encoding='ISO 8859-1') as infile:\n",
    "        li = infile.readlines()\n",
    "        for sentence in li:\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            kk = len(tokens)\n",
    "            if kk > max_:\n",
    "                max_ = kk\n",
    "#             if len(tokens)>max_:\n",
    "        return max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== bert-base-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/leoqaz12/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased 211\n",
      "===== bert-large-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /home/leoqaz12/.pytorch_pretrained_bert/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-large-uncased 211\n",
      "===== bert-base-cased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/leoqaz12/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased 236\n",
      "===== bert-large-cased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /home/leoqaz12/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-large-cased 236\n",
      "===== bert-base-multilingual-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/leoqaz12/.pytorch_pretrained_bert/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-uncased 198\n",
      "===== bert-base-multilingual-cased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/leoqaz12/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased 210\n",
      "===== bert-base-chinese =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/leoqaz12/.pytorch_pretrained_bert/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-chinese 230\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Input: dir path, bert model\n",
    "    Print: max length of each model\n",
    "'''\n",
    "for model in ['bert-base-uncased',\"bert-large-uncased\",'bert-base-cased','bert-large-cased','bert-base-multilingual-uncased',\"bert-base-multilingual-cased\",'bert-base-chinese']:\n",
    "    maxx = 0\n",
    "    ext='.hooklog'\n",
    "    in_dir='data/aries_simplified_profile_1002/'\n",
    "#     print('=====',model,'=====')\n",
    "    tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n",
    "    files = next(os.walk(in_dir))\n",
    "#     for file in list(filter(lambda f: f.endswith(ext), files[2])):\n",
    "    p = Pool(processes=35)\n",
    "#     ff = files[0]+file\n",
    "#         print(ff)\n",
    "    maxx_ = p.map(par,list(filter(lambda f: f.endswith(ext), files[2])))\n",
    "#     if maxx_ > maxx:\n",
    "#         maxx = maxx_\n",
    "    print(model,max(maxx_))\n",
    "    \n",
    "#         with Pool(processes=33) as pool:\n",
    "#             pool.starmap(par,file)\n",
    "# def par(file):\n",
    "#     with open(files[0]+file , encoding='ISO 8859-1') as infile:\n",
    "#         li = infile.readlines()\n",
    "#         for sentence in li:\n",
    "#             tokens = tokenizer.tokenize(sentence)\n",
    "# #             if len(tokens)>max_:\n",
    "#             max_ = len(tokens)\n",
    "#     return max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Input: dir path, file extension, output path, bert model\n",
    "    Ouput: concate all file with same extension, split by an empty raw\n",
    "    No return\n",
    "'''\n",
    "def concat_files(in_dir,ext,out_dir):\n",
    "    max_ = 0\n",
    "#     tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n",
    "    files = next(os.walk(in_dir))\n",
    "    with open(out_dir+ext,'w') as outfile:\n",
    "        for file in list(filter(lambda f: f.endswith(ext), files[2])):\n",
    "            with open(files[0]+file , encoding='ISO 8859-1') as infile:\n",
    "                outfile.write(infile.read())\n",
    "                outfile.write(\"\\n\")\n",
    "#             with open(files[0]+file , encoding='ISO 8859-1') as infile:\n",
    "#                 li = infile.readlines()\n",
    "#                 for sentence in li:\n",
    "#                     tokens = tokenizer.tokenize(sentence)\n",
    "#                     if len(tokens)>max_:\n",
    "#                         max_ = len(tokens)\n",
    "    print('done')#,str(max_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "files = next(os.walk('data/aries_simplified_profile_1002/'))\n",
    "with open('./data/bert/profile_input/20190409'+'.trace.hooklog','w') as outfile:\n",
    "    for file in files[2]:\n",
    "        with open(files[0]+file , encoding='ISO 8859-1') as infile:\n",
    "            print(type(infile.readlines()))\n",
    "#             outfile.write(infile.read())\n",
    "#             print(infile.readlines())\n",
    "            outfile.write(\"\\n\")\n",
    "            break\n",
    "# list(filter(lambda f: f.endswith(\".hooklog\"), hl_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# for models in ['bert-base-uncased',\"bert-large-uncased\",'bert-base-cased','bert-large-cased','bert-base-multilingual-uncased',\"bert-base-multilingual-cased\",'bert-base-chinese']:\n",
    "concat_files(in_dir='data/aries_simplified_profile_1002/',ext='.hooklog',\n",
    "             out_dir='./data/bert/profile_input/20190409_pregenerate')\n",
    "#     print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-09 12:06:08--  https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/lm_finetuning/finetune_on_pregenerated.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.64.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16090 (16K) [text/plain]\n",
      "Saving to: ‘finetune_on_pregenerated.py’\n",
      "\n",
      "finetune_on_pregene 100%[===================>]  15.71K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2019-04-09 12:06:08 (496 KB/s) - ‘finetune_on_pregenerated.py’ saved [16090/16090]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/lm_finetuning/finetune_on_pregenerated.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回到putty正確位址(malwareTagging):\n",
    "python3 pregenerate_training_data.py --train_corpus ./data/bert/profile_input/20190409_pregenerate.hooklog --bert_model bert-base-multilingual-uncased --do_lower_case --output_dir ./data/bert/profile_input/ --epochs_to_generate (4) --max_seq_len (200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 finetune_on_pregenerated.py --pregenerated_data ./data/bert/profile_input/ --bert_model bert-base-multilingual-uncased --do_lower_case --output_dir ./data/bert/profile_input/ --epochs (4) (--fp16 --loss_scale) (--train_batch_size 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2 epoch, original token [6:05:53<00:00,  1.63it/s, Loss: 0.09366]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
