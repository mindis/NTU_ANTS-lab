{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "* 環境要先安裝fastText: https://github.com/facebookresearch/fastText\n",
    "* 環境要先安裝Sent2Vec: https://github.com/epfml/sent2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "from keras.utils import *\n",
    "from keras.utils.generic_utils import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.preprocessing.image import *\n",
    "from multiprocessing import *\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.manifold import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "import sent2vec\n",
    "import re\n",
    "import string\n",
    "import unicodedata as udata\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "決定要採用那些API calls (要先自己統計dataset中出現了哪一些API function name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_li = ['LoadLibrary',\n",
    "'CreateProcess',\n",
    "'OpenProcess',\n",
    "'ExitProcess',\n",
    "'TerminateProcess',\n",
    "'WinExec',\n",
    "'CreateRemoteThread',\n",
    "'CreateThread',\n",
    "'CopyFile',\n",
    "'CreateFile',\n",
    "'DeleteFile',\n",
    "'RegSetValue',\n",
    "'RegCreateKey',\n",
    "'RegDeleteKey',\n",
    "'RegDeleteValue',\n",
    "'RegQueryValue',\n",
    "'RegEnumValue',\n",
    "'WinHttpConnect',\n",
    "'WinHttpOpen',\n",
    "'WinHttpOpenRequest',\n",
    "'WinHttpReadData',\n",
    "'WinHttpSendRequest',\n",
    "# 'WinHttpWriteData', #dataset中少了\n",
    "'InternetOpen',\n",
    "'InternetConnect',\n",
    "'HttpSendRequest',\n",
    "'GetUrlCacheEntryInfo']\n",
    "api_li = [x.lower() for x in api_li] #lowrer case?小寫?\n",
    "len(api_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備Sent2Vec演算法所需要的訓練資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "dil= r\"[,.;\\-+^()/@#?!&$:{}\\\\*%~\\'\\\"\\=\\_]+\\ *\" \n",
    "\n",
    "root_dir = './data/tree-rep-profiles_o2o/normal/' #訓練資料集\n",
    "# rasmma_dir  = next(os.walk(root_dir))[1]\n",
    "# for dir_ in rasmma_dir:\n",
    "fam_dir = next(os.walk(root_dir ))[1]\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir  + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        for profile in hl_list:\n",
    "            with open(profile,encoding='ISO 8859-1') as f:\n",
    "                lines = f.read()\n",
    "            lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "            lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "            lines = re.sub('=\\\\n','',lines)\n",
    "            lines = re.sub('y\\\\n','',lines)\n",
    "            lines = lines.splitlines()\n",
    "            for line in lines:\n",
    "                temp = re.sub(dil,\" \",line.lower())#.lower()) #小寫?\n",
    "                temp = temp.split(\" \")\n",
    "                temp = list(filter(None, temp))\n",
    "                temp = ' '.join(temp)\n",
    "                if temp.startswith('winh'.lower()):\n",
    "                    print(profile,temp)\n",
    "                if temp.split(' ')[0] not in api_li:\n",
    "                    print('=o2o_ERR:=',profile,'=>',temp) #API function name不正確，通常是編碼或是亂碼問題\n",
    "                    continue\n",
    "                corpus.append(temp)\n",
    "\n",
    "with open('data/sent2vec_corpus/o2o_Sent2Vec_lower_woParam_0630.txt','w') as f: #輸出corpus給Sent2Vec用，要先自行創好資料夾\n",
    "    f.write('\\n'.join(corpus))\n",
    "corpus[-10:] #範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了corpus以後就可以依照https://github.com/epfml/sent2vec#train-a-new-sent2vec-model 進行訓練\n",
    "* 超參數說明皆未於Sent2Vec github\n",
    "* 超參數選擇可以依照以下的evaluation方式來挑選調整\n",
    "    *  TY自己tune出來的超參數: -minCount 0 -dim 768 -epoch 15 -lr 0.3 -wordNgrams 16 -loss ns -neg 10 -thread 30 -dropoutK 1 -minCountLabel 0 -bucket 2000000 -t 0.00001 -numCheckPoints 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sent2vec_corpus/o2o_Sent2Vec_lower_woParam_0630.txt','r') as f: #讀入sent2vec訓練用的corpus\n",
    "    corpus = f.read().splitlines() \n",
    "corpus = sorted(corpus)\n",
    "corpus[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('model/o2o_o2m_Sent2Vec_woParam_0616.bin') #讀入最終訓練完成的Sent2Vec model\n",
    "emb_all = model.embed_sentences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一種API finctoin name出現了幾次?\n",
    "api = \"\"\n",
    "count = 0\n",
    "count_all = {}\n",
    "for row in corpus:\n",
    "    try:\n",
    "        count_all[row.split(' ')[0]] += 1\n",
    "    except KeyError:\n",
    "        count_all[row.split(' ')[0]] = 1\n",
    "print(len(count_all))\n",
    "sorted(count_all.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 少了那些該出現的API function name?\n",
    "# 多了哪些不該出現的API function name?\n",
    "kk = list(count_all.keys())\n",
    "for k in api_li:\n",
    "    if k not in kk:\n",
    "        print('少了:',k)\n",
    "for k in kk:\n",
    "    if k not in api_li:\n",
    "        print('多了',k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以cosine similarity方式評估，隨機挑選100筆，看自己的API function name可否還原到自己那一類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= 0\n",
    "df = pd.DataFrame(0, index=api_li, columns=api_li)\n",
    "index_ = df.index.tolist()\n",
    "avg = 100\n",
    "for _ in range(avg):\n",
    "    c=0\n",
    "    for k,v in count_all.items():\n",
    "        pick = random.randrange(c,c+v) #cosine similarity的左邊\n",
    "        pick = emb_all[pick,:] #sorted corpus的embedding_all\n",
    "        c1 = 0\n",
    "        try:\n",
    "            row = index_.index(k)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        for k1,v1 in count_all.items(): # cosine similarity的右邊\n",
    "            pick1 = random.randrange(c1,c1+v1)\n",
    "            pick1 = emb_all[pick1,:]\n",
    "            score = cosine_similarity([pick],[pick1]) #cosine_similarity\n",
    "    #         print(score)\n",
    "            c1 = c1+v1\n",
    "            try:\n",
    "                col = index_.index(k1)\n",
    "                df.iloc[row,col] += float(score)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        c = c+v\n",
    "df = df/avg #similarity\n",
    "# df = (df-np.min(df.values))/(np.max(df.values)-np.min(df.values)) #distance normalize\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = ['LoadLibrary']\n",
    "proc = ['CreateProcess', 'OpenProcess', 'ExitProcess', 'TerminateProcess', 'WinExec',\n",
    "        'CreateRemoteThread', 'CreateThread']\n",
    "file = ['CopyFile', 'CreateFile', 'DeleteFile']\n",
    "reg = ['RegSetValue', 'RegCreateKey', 'RegDeleteKey', 'RegDeleteValue',\n",
    "       'RegQueryValue', 'RegEnumValue']\n",
    "net = ['WinHttpConnect', 'WinHttpOpen', 'WinHttpOpenRequest', 'WinHttpReadData', 'WinHttpSendRequest', #'WinHttpWriteData',\n",
    "        'InternetOpen', 'InternetConnect', 'HttpSendRequest', 'GetUrlCacheEntryInfo']\n",
    "\n",
    "#lower?小寫?要跟前面一致\n",
    "lib = [x.lower() for x in lib]\n",
    "proc = [x.lower() for x in proc]\n",
    "file = [x.lower() for x in file]\n",
    "reg =[x.lower() for x in reg]\n",
    "net = [x.lower() for x in net]\n",
    "\n",
    "# df.loc['']\n",
    "x = [lib,proc,file,reg,net]\n",
    "x = sum(x,[])\n",
    "index_ , len(index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up=0\n",
    "down = len(index_)\n",
    "down_cat = len(x)\n",
    "up_cat = 0\n",
    "up_cat2 = 0\n",
    "lib_lib=lib_proc=lib_file=lib_reg=lib_net = 0\n",
    "proc_lib=proc_proc=proc_file=proc_reg=proc_net = 0\n",
    "file_lib=file_proc=file_file=file_reg=file_net = 0\n",
    "reg_lib=reg_proc=reg_file=reg_reg=reg_net = 0\n",
    "net_lib=net_proc=net_file=net_reg=net_net = 0\n",
    "for api in index_:\n",
    "    if df.loc[api].idxmax() == api:\n",
    "        up+=1\n",
    "    else:\n",
    "        print(api)\n",
    "    baseline_lib = df.loc[api,lib].mean()\n",
    "    baseline_proc =  df.loc[api,proc].mean()\n",
    "    baseline_file = df.loc[api,file].mean()\n",
    "    baseline_reg = df.loc[api,reg].mean()\n",
    "    baseline_net = df.loc[api,net].mean()\n",
    "    max_value = np.max([baseline_lib,baseline_proc,baseline_file,baseline_reg,baseline_net])\n",
    "    if (api in lib):\n",
    "        lib_lib += baseline_lib\n",
    "        lib_proc += baseline_proc\n",
    "        lib_file += baseline_file\n",
    "        lib_reg += baseline_reg\n",
    "        lib_net += baseline_net\n",
    "        if (max_value==baseline_lib):\n",
    "            up_cat+=1\n",
    "    elif api in proc :\n",
    "        proc_lib += baseline_lib\n",
    "        proc_proc += baseline_proc\n",
    "        proc_file += baseline_file\n",
    "        proc_reg += baseline_reg\n",
    "        proc_net += baseline_net\n",
    "        if max_value==baseline_proc:\n",
    "            up_cat+=1\n",
    "    elif api in file :\n",
    "        file_lib += baseline_lib\n",
    "        file_proc += baseline_proc\n",
    "        file_file += baseline_file\n",
    "        file_reg += baseline_reg\n",
    "        file_net += baseline_net\n",
    "        if max_value==baseline_file:\n",
    "            up_cat+=1    \n",
    "    elif api in reg:\n",
    "        reg_lib += baseline_lib\n",
    "        reg_proc += baseline_proc\n",
    "        reg_file += baseline_file\n",
    "        reg_reg += baseline_reg\n",
    "        reg_net += baseline_net\n",
    "        if max_value==baseline_reg:\n",
    "            up_cat+=1    \n",
    "    elif api in net:\n",
    "        net_lib += baseline_lib\n",
    "        net_proc += baseline_proc\n",
    "        net_file += baseline_file\n",
    "        net_reg += baseline_reg\n",
    "        net_net += baseline_net\n",
    "        if max_value==baseline_net:\n",
    "            up_cat+=1\n",
    "if np.max([lib_lib,lib_proc,lib_file,lib_reg,lib_net]) == lib_lib:\n",
    "    up_cat2+=1\n",
    "if np.max([proc_lib,proc_proc,proc_file,proc_reg,proc_net]) == proc_proc:\n",
    "    up_cat2+=1\n",
    "if np.max([file_lib,file_proc,file_file,file_reg,file_net]) == file_file:\n",
    "    up_cat2+=1\n",
    "if np.max([reg_lib,reg_proc,reg_file,reg_reg,reg_net]) == reg_reg:\n",
    "    up_cat2+=1\n",
    "if np.max([net_lib,net_proc,net_file,net_reg,net_net]) == net_net:\n",
    "    up_cat2+=1\n",
    "        \n",
    "print('自己還原自己-ACC:',(up/down)*100,'%') #self\n",
    "print('自己屬於自己那一類-ACC(他自己那類的分數平均是不是所有類別分數最高的):',(up_cat/down_cat)*100,'%') #micro\n",
    "print('自己那類屬於自己那一類-ACC:',(up_cat2/5)*100,'%') #macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 綜合性決定性評估指標:\n",
    "score = []\n",
    "for i in range(len(df)):\n",
    "    score.append(df.iloc[i,i])\n",
    "print('對角線分數(越高越好):',np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TY的Sent2Vec model表現:\n",
    "* 自己還原自己-ACC: 96.15384615384616 %\n",
    "* 自己屬於自己那一類-ACC(他自己那類的分數平均是不是所有類別分數最高的): 73.07692307692307 %\n",
    "* 自己那類屬於自己那一類-ACC: 100.0 %\n",
    "* 對角線分數(越高越好): 0.843296217550523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix Spplementary\n",
    "* 因相關套件版本變動過大而成為Deprecated code. Need lots of modifications!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer weight initialization\n",
    "**建議使用他們推薦的model、tokenizer進行訓練: bert-base-multilingual-cased**\n",
    "\n",
    "1. 需先安裝pytorch與BERT於環境: https://github.com/huggingface/pytorch-transformers#installation、https://huggingface.co/pytorch-transformers/index.html\n",
    "2. 依照說明創造finetune pre-trained model所需的corpus: https://github.com/huggingface/pytorch-transformers/tree/master/examples/lm_finetuning\n",
    "3. 依照相同說明Pregenerating training data: https://github.com/huggingface/pytorch-transformers/tree/master/examples/lm_finetuning#pregenerating-training-data\n",
    "4. 依照相同說明Training on pregenerated data(非常耗時且不可中斷，需要多張GPU): https://github.com/huggingface/pytorch-transformers/tree/master/examples/lm_finetuning#training-on-pregenerated-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 假設讀入兩種經過finetune的BERT model，並利用內建推薦的tokenizer: bert-base-multilingual-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path1 = '/DATA/r06725035/model/less_pretrain_bert/bert/pytorch_model.bin' #model1 path\n",
    "model_state_dict1 = torch.load(model_path1,  map_location='cpu')\n",
    "model1 = BertModel.from_pretrained(pretrained_model_name_or_path='bert-base-multilingual-cased',state_dict=model_state_dict1)\n",
    "model1 = torch.nn.DataParallel(model1)\n",
    "model1.eval()\n",
    "\n",
    "model_path2 = './model/bert_pretrained/bert/pytorch_model.bin' #model2 path\n",
    "model_state_dict2 = torch.load(model_path2,  map_location='cpu')\n",
    "model2 = BertModel.from_pretrained(pretrained_model_name_or_path='bert-base-multilingual-cased',state_dict=model_state_dict2)\n",
    "model2 = torch.nn.DataParallel(model2)\n",
    "model2.eval()\n",
    "# model = BertModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path='bert-base-multilingual-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "獲取兩種model最後四層hidden layer所輸出的feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(hl_path,model1=model1,model2=model2,tokenizer=tokenizer,sent=True):\n",
    "    '''\n",
    "    Input: hl_path=> 一個execution profile的路徑位子\n",
    "    Return: np.array=> 該execution profile在其中一種model所萃取出的768 feature\n",
    "    '''\n",
    "    last = [-1,-2,-3,-4] #要拿最後幾層?\n",
    "    cls_np_all = []\n",
    "    token_np_all = []\n",
    "    with open(hl_path,encoding='ISO 8859-1') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        line = '[CLS] ' + line + ' [SEP]'\n",
    "        tokenized_text = tokenizer.tokenize(line)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [0]*len(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "#         tokens_tensor = tokens_tensor.cuda() #.to(device)\n",
    "#         segments_tensors = segments_tensors.cuda() #.to(device)\n",
    "        if sent:\n",
    "            model1.cuda()\n",
    "            tokens_tensor = tokens_tensor.cuda()\n",
    "            segments_tensors = segments_tensors.cuda()\n",
    "        else:\n",
    "            model2.cuda()\n",
    "            tokens_tensor = tokens_tensor.cuda() #.to(device)\n",
    "            segments_tensors = segments_tensors.cuda() #.to(device)\n",
    "#         print(len(tokenized_text))\n",
    "        if sent: #只拿[CLS] 最後四層\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = model1(tokens_tensor, segments_tensors)\n",
    "            cls_np = []\n",
    "            for layer in last:\n",
    "                cls_np.append(np.array(encoded_layers[layer].cpu())[0][0])\n",
    "            last_np = np.sum(cls_np,axis=0)\n",
    "            cls_np_all.append(last_np)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = model2(tokens_tensor, segments_tensors)\n",
    "            length = len(encoded_layers[-1][0])\n",
    "            all_np = []\n",
    "            for layer in last:\n",
    "                for token in range(1,length-1):\n",
    "                    array = np.array(encoded_layers[layer].cpu())[0][token]\n",
    "                    all_np.append(array)\n",
    "            last_np = np.average(all_np,axis=0)\n",
    "            token_np_all.append(last_np) #只有一個lline!!\n",
    "    try:\n",
    "        del model1\n",
    "        del model2\n",
    "    except:\n",
    "        torch.cuda.empty_cache()\n",
    "    if sent:\n",
    "        return np.array(cls_np_all) #[CLS]\n",
    "    else:\n",
    "        return np.array(token_np_all) # tokens\n",
    "            \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得每一個profile兩種model的feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent=False是二種model，存到XXX_tokenfeature.pkl\n",
    "#sent=True是第一種model，存到_clsfeature.pkl\n",
    "#所以要跑兩次\n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        for hl in hl_list:\n",
    "            clean_profile(hl)\n",
    "#         in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile_clean\"), hl_list))\n",
    "        for hl in hl_list:\n",
    "            tokens_feature = get_feature(hl,model1,model2,sent=False) #回傳該hkl的兩種feature\n",
    "#             cls_file = hl.replace('.profile_clean','_clsfeature.pkl') #一種model的輸出\n",
    "            token_file = hl.replace('.profile_clean','_tokenfeature.pkl') #另一種model的輸出\n",
    "#             pickle.dump(file=open(cls_file,'wb'),obj=cls_feature)\n",
    "            pickle.dump(file=open(token_file,'wb'),obj=tokens_feature)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抓出每一種API function name之所有API invocation call vectors進行平均\n",
    "* 要先有已經定義好的api_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dict ={}\n",
    "whole_dict_count ={}\n",
    "pretrain_dict ={}\n",
    "whole_dict = whole_dict.fromkeys(api_li,np.array([0.0]*768))\n",
    "pretrain_dict = pretrain_dict.fromkeys(api_li,np.array([0.0]*768))\n",
    "whole_dict_count = whole_dict.fromkeys(api_li,0)\n",
    "\n",
    "root_dir = './data/tree-rep-profiles_one2many/normal/' #已經存了各別feature pkl的路徑\n",
    "\n",
    "# whole_dict = {}\n",
    " \n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\"_clsfeature.pkl\"), hl_list))\n",
    "        for pkl in hl_list:\n",
    "            temp = pickle.load(open(pkl,'rb'))\n",
    "            path2 = pkl.replace('_clsfeature.pkl','.profile_clean')\n",
    "            with open(path2,encoding='ISO 8859-1') as f:\n",
    "                lines = f.read().splitlines()\n",
    "            assert len(temp) == len(lines)\n",
    "            for num,line in zip(temp,lines):\n",
    "                api = line.split(' ')[0]\n",
    "#                 if api == 'GetUrlCacheEntryInfo':\n",
    "#                     print(api)\n",
    "                whole_dict[api] = whole_dict[api]+num\n",
    "                whole_dict_count[api] = whole_dict_count[api] + 1\n",
    "whole_dict_count , whole_dict['LoadLibrary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for api in api_li:\n",
    "    pretrain_dict[api] = whole_dict[api] / (whole_dict_count[api] + 1e-8)\n",
    "# pretrain_dict[api]\n",
    "\n",
    "embedding_matrix = np.zeros((len(pretrain_dict) + 1, 768))\n",
    "for word, i in encode_dict.items():\n",
    "    embedding_vector = pretrain_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "pickle.dump(file=open('data/tree-rep-profiles-partial/api_emb_matrix.pkl','wb'),obj=embedding_matrix) \n",
    "#儲存每一種API function name的initial weight vector for embedding layer\n",
    "# embedding_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
