{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisite packages\n",
    "* import error: 環境沒有的套件google一下就會有教學怎麼裝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil,pickle,tqdm,sys,random,re,string,pause, datetime,glob\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "# from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from keras.models import model_from_json, model_from_yaml\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score,hamming_loss , roc_auc_score\n",
    "\n",
    "# from collections import Counter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import unicodedata as udata\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "from keras import backend \n",
    "from imblearn.ensemble import *\n",
    "from imblearn.combine import *\n",
    "# from python.keras import backend \n",
    "# Embedding(10,20)\n",
    "from keras_transformer.extras import ReusableEmbedding, TiedOutputEmbedding\n",
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "from keras_transformer.bert import (\n",
    "    BatchGeneratorForBERT, masked_perplexity,\n",
    "    MaskedPenalizedSparseCategoricalCrossentropy)\n",
    "\n",
    "import keras_metrics as km\n",
    "from keras_trans_mask import RemoveMask, RestoreMask\n",
    "\n",
    "from keras_multi_head import *\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在與此code相同的目錄裡，要包含本github( https://github.com/tychen5/NTU_ANTS-lab/tree/master/seqgen_famclf )中的相關額外檔案，才可以順利import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import transformer_bert_model\n",
    "from bpe import BPEEncoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read & Restore data prepare by 0003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/dev/test\n",
    "root_dir = 'data/tree-rep-profiles_o2o/' #0003存放各個dataset的pkl目錄\n",
    "train_emb_api,train_emb , train_rep_ans = pickle.load(open(root_dir + 'pids_train.pkl','rb'))\n",
    "valid_emb_api,valid_emb, valid_rep_ans = pickle.load(open(root_dir + 'pids_valid.pkl','rb'))\n",
    "test_emb_api,test_emb ,test_rep_ans = pickle.load(open(root_dir + 'pids_test.pkl','rb'))\n",
    "exp_api,exp_emb = pickle.load(open(root_dir + 'pids_exp.pkl','rb'))\n",
    "# print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "# print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "# train_emb_api,train_emb , train_rep_ans = pickle.load(open(root_dir + 'pids_train_only.pkl','rb'))\n",
    "# train_rep_ans = np.expand_dims(train_rep_ans,axis=-1)\n",
    "train_rep_ans = np.expand_dims(train_rep_ans,axis=-1)\n",
    "valid_rep_ans = np.expand_dims(valid_rep_ans,axis=-1)\n",
    "test_rep_ans = np.expand_dims(test_rep_ans,axis=-1)\n",
    "print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_rep_ans.shape)\n",
    "emb_matrix = pickle.load(open('data/api_emb_matrix.pkl','rb')) #embedding layer initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, X2 ,X3):\n",
    "    '''\n",
    "    將train set資料順序打亂，不然NN會train壞\n",
    "    '''\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    return (X[randomize], X2[randomize],X3[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb, train_emb_api, train_rep_ans = _shuffle(train_emb, train_emb_api, train_rep_ans)\n",
    "print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_rep_ans.shape)\n",
    "print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_rep_ans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = valid_emb.shape[-1] #被除數\n",
    "num_heads = 8 #Transformer multi-head實驗才要用\n",
    "max_length = test_emb_api.shape[1] # max sequence length\n",
    "# fam_num = train_fam_ans.shape[1]\n",
    "vocabulary_size = emb_matrix.shape[0]-1 #API function name種類\n",
    "transformer_depth = 1 # transformer實驗 block數目\n",
    "transformer_dropout = 0.1 #transformer 實驗模型預設do\n",
    "l2_reg_penalty = 1e-6#1e-4\n",
    "# dp_rate = 0.01 #dropout，沒用到\n",
    "\n",
    "# traina = True #沒用到\n",
    "batch_size = 128 \n",
    "learning_rate = 0.001#5e-4#2e-4 #初始learning rate\n",
    "num_epochs = 1000\n",
    "patien = 30 # early stopping條件\n",
    "model_save_path = './model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5' #模型儲存位置\n",
    "tensorboard_log_path = './logs/'+ model_save_path.split('/')[-1].split('.')[0] #history log path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer model 實驗才會用到\n",
    "* transformer encoder: https://github.com/kpot/keras-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constrain = keras.constraints.MinMaxNorm(min_value=0.0, max_value=0.0, rate=1.0, axis=0)\n",
    "# init = keras.initializers.Ones() #\n",
    "coordinate_embedding_layer = TransformerCoordinateEmbedding(\n",
    "        transformer_depth , name='coordinate_embedding') #transformer的 positional encoding\n",
    "act_layer = TransformerACT(\n",
    "            name='adaptive_computation_time')\n",
    "\n",
    "transformer_block = TransformerBlock(\n",
    "            name='transformer', num_heads=num_heads,\n",
    "            residual_dropout=transformer_dropout,\n",
    "            attention_dropout=transformer_dropout,\n",
    "            # Allow bi-directional attention\n",
    "            use_masking=False)\n",
    "add_segment_layer = Add(name='add_segment')\n",
    "l2_regularizer = (regularizers.l2(l2_reg_penalty) if l2_reg_penalty else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb') #輸入sent2vec vectors\n",
    "sentemb = Masking(mask_value=0)(sentemb1) #0 is padding\n",
    "\n",
    "sent_ids1 = Input(shape=(max_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\n",
    "sent_ids = Masking(mask_value=0)(sent_ids1) #API整數編碼。0 is padding\n",
    "api_emb = Embedding(vocabulary_size+1, emb_dim,weights=[emb_matrix],input_length=max_length\n",
    "                    ,trainable=True,name='api_emb',trainable=True)(sent_ids) #embedding layer\n",
    "\n",
    "segment_embeddings = Add()([sentemb,api_emb]) #相加兩種embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU & self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_rnn = GRU(int(emb_dim/4),return_sequences=True,return_state=False,name='common_extract'\n",
    "                      ,trainable=True)(segment_embeddings) #RNN\n",
    "att_rnn = BatchNormalization(name='bn')(att_rnn) #batch feature scaling\n",
    "self_att = SeqSelfAttention(attention_activation='sigmoid',name='self_attention')(att_rnn) #github預設的self-attention mechanism: https://pypi.org/project/keras-self-attention/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_prediction = (\n",
    "        TimeDistributed(Dense(1, name='0_1_predict', activation='sigmoid',trainable=True),name='out_rep') # hard_sigmoid\n",
    "    (self_att)) #prediction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[sent_ids1,sentemb1], outputs=[rep_prediction]) #build model graph\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Optimizer/Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "available metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_multi_label_metric(y_true, y_pred):\n",
    "    comp = K.equal(y_true, K.round(y_pred))\n",
    "    return K.cast(K.all(comp, axis=-1), K.floatx())\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "\n",
    "def custom_acc1(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred,k=3)\n",
    "from keras.metrics import binary_accuracy\n",
    "def bin_acc(y_true, y_pred):\n",
    "    return binary_accuracy(y_true, y_pred)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    return tf.keras.metrics.Precision(y_true,y_pred)[1]\n",
    "def recall(y_true, y_pred):\n",
    "    return tf.keras.metrics.Recall(y_true,y_pred)[1]\n",
    "\n",
    "\n",
    "metric = [bin_acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "available loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hamming_loss(y_true, y_pred):\n",
    "    tmp = K.abs(y_true-y_pred)\n",
    "    return K.mean(K.cast(K.greater(tmp,0.5),dtype=float))\n",
    "def hn_multilabel_loss(y_true, y_pred):\n",
    "    # Avoid divide by 0\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # Multi-task loss\n",
    "    return K.mean(K.sum(- y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred), axis=1))\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "los = [losses.binary_crossentropy]\n",
    "loss_weight = [1] #如果是multi-task learning才要改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(\n",
    "            lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False, clipvalue=1.) #clipnorm=1. , clipvalue=1.防止gradient explosion\n",
    "# optimizer = keras.optimizers.Nadam(lr=learning_rate, clipvalue=1.) #另外一種adam optomizer\n",
    "\n",
    "lr_scheduler1 = callbacks.LearningRateScheduler( # lr隨著epoch遞減\n",
    "        CosineLRSchedule(lr_high=learning_rate, lr_low=1e-8, #learning_rate \n",
    "                         initial_period=num_epochs),\n",
    "        verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=int(patien/3),\n",
    "                                      min_lr=1e-8,mode='min') #當model的loss卡住的時候(連續三分之一個patien epoch沒進步)會立刻把lr砍半\n",
    "\n",
    "model.compile(\n",
    "            optimizer,\n",
    "            loss=los,\n",
    "            metrics=metric ,loss_weights=loss_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save best model only, early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "stop_nan = callbacks.TerminateOnNaN() # train 壞了立刻中止\n",
    "model_callbacks = [\n",
    "        callbacks.ModelCheckpoint( #只保留model在DEV表現最好的那個epoch之模型參數\n",
    "            model_save_path, \n",
    "            monitor='val_loss',mode='min' ,save_best_only=True, verbose=1,save_weights_only=True), \n",
    "            EarlyStopping(patience=patien,monitor='val_loss',verbose=1,mode='min'), #依據valid loss決定是否終止訓練\n",
    "        lr_scheduler,history,stop_nan ,lr_scheduler1\n",
    "    ]\n",
    "model_callbacks.append(callbacks.TensorBoard(tensorboard_log_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備每一個batch的data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_batch(batch_size, X_train1, X_train2 , Y_train1):#, Y_train2):\n",
    "    '''\n",
    "    X_train1 = sent_ids: shape為(N, max_seq_length)=>API function name整數編碼\n",
    "    X_train2 = sentemb: shape為(N,max_seq_length, word_embedding_size)\n",
    "    Y_train1 = binary vector prediction: shape為(N, max_seq_length, 1)\n",
    "    '''\n",
    "    idx = np.arange(len(X_train1))\n",
    "    np.random.shuffle(idx) # batch內隨機打亂\n",
    "\n",
    "    while True:\n",
    "        for i in idx:\n",
    "            train_X1 = X_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_X2 = X_train2[idx[i]:idx[i]+batch_size]\n",
    "            train_Y1 = Y_train1[idx[i]:idx[i]+batch_size]\n",
    "            yield ([train_X1,train_X2],[train_Y1]) #改\n",
    "            if i == idx[-1]:\n",
    "                idx = np.arange(len(X_train1))\n",
    "                np.random.shuffle(idx)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "H = model.fit_generator( #train_rep_ans\n",
    "    generator=training_batch(batch_size=batch_size,X_train1=train_emb_api,X_train2=train_emb ,\n",
    "                                             Y_train1=train_rep_ans)\n",
    "                        , steps_per_epoch=int(np.ceil(len(train_emb_api)/batch_size)) ,\n",
    "                    epochs=num_epochs,callbacks=model_callbacks\n",
    "                   ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans]) \n",
    "                    ,max_queue_size=10 \n",
    "                    ,workers=10,use_multiprocessing=True   \n",
    "                   ,shuffle=True,verbose=1)\n",
    "model.save(model_save_path+\"_all.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter功能模組\n",
    "* 挑選模型於驗證資料集上的最佳threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_acc(y_true, y_pred):\n",
    "    return binary_accuracy(y_true, y_pred)\n",
    "# model_save_path =  './model/o2o_embEXP_gru_selfatt/sent2vec_mask_0718.h5' #實驗的model\n",
    "model = load_model(model_save_path+'_all.h5',custom_objects={'bin_acc': bin_acc,'RemoveMask':RemoveMask,'RestoreMask':RestoreMask, 'SeqSelfAttention': SeqSelfAttention})\n",
    "model.load_weights(model_save_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict on DEV/Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_t = model.predict([test_emb_api,test_emb])\n",
    "ans_v = model.predict([valid_emb_api,valid_emb])\n",
    "print(ans_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_rep_ans.squeeze(axis=-1)\n",
    "ans_t = ans_t.squeeze(axis=-1)\n",
    "area_score_t = roc_auc_score(y_true,ans_t,average='micro')\n",
    "y_true = valid_rep_ans.squeeze(axis=-1)\n",
    "ans_v = ans_v.squeeze(axis=-1)\n",
    "area_score_v = roc_auc_score(y_true,ans_v,average='micro')\n",
    "print('valid roc auc score:',area_score_v,'test:',area_score_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose final threshold for the model\n",
    "* 實驗1-1與1-2的表現\n",
    "* 先選出模型在validation Set score最高(F1-hamming loss)的那個threshold\n",
    "* 再去看那個threshold再testing Set上的F1、precision、recall、hamming_loss表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "score_list_t = []\n",
    "f1_list_v = []\n",
    "pre_list_v = []\n",
    "rec_list_v = []\n",
    "f1_list_t = []\n",
    "pre_list_t = []\n",
    "rec_list_t = []\n",
    "hloss_list_v = []\n",
    "hloss_list_t = []\n",
    "threashold_list = []\n",
    "max_score = 0\n",
    "for i in tqdm(range(1,100)): #0.01~0.99 #system: 500 (0.002)\n",
    "    thr = i/100 # system: 500 (0.002)\n",
    "    final_ans_t = []\n",
    "    for sample in ans_t:\n",
    "        sample_ans = []\n",
    "        for value in sample:\n",
    "            if value < thr: #0.26 #0.33\n",
    "                sample_ans.append(0)\n",
    "            else:\n",
    "                sample_ans.append(1)\n",
    "        final_ans_t.append(sample_ans)\n",
    "    final_ans_t = np.array(final_ans_t)\n",
    "#     print(final_ans_t.shape , sum(final_ans_t[0]))\n",
    "\n",
    "    final_ans_v = []\n",
    "    for sample in ans_v:\n",
    "        sample_ans = []\n",
    "        for value in sample:\n",
    "            if value < thr: #0.26 #0.33\n",
    "                sample_ans.append(0)\n",
    "            else:\n",
    "                sample_ans.append(1)\n",
    "        final_ans_v.append(sample_ans)\n",
    "    final_ans_v = np.array(final_ans_v)\n",
    "#     print(final_ans_v.shape , sum(final_ans_v[0]))\n",
    "\n",
    "    y_true = test_rep_ans.squeeze(axis=-1)\n",
    "    recall_t = recall_score(y_true=y_true, y_pred=final_ans_t, average='micro')\n",
    "    precision_t = precision_score(y_true=y_true, y_pred=final_ans_t, average='micro')\n",
    "    f1_t = f1_score(y_true=y_true, y_pred=final_ans_t, average='micro')\n",
    "    h_loss_t = hamming_loss(y_true,final_ans_t)\n",
    "#     print('Test: (recall/precision/f1/h_loss)',recall_t ,precision_t, f1_t , h_loss_t)\n",
    "\n",
    "    y_true = valid_rep_ans.squeeze(axis=-1)\n",
    "    recall_v = recall_score(y_true=y_true, y_pred=final_ans_v, average='micro')\n",
    "    precision_v = precision_score(y_true=y_true, y_pred=final_ans_v, average='micro')\n",
    "    f1_v = f1_score(y_true=y_true, y_pred=final_ans_v, average='micro')\n",
    "    h_loss_v = hamming_loss(y_true,final_ans_v)\n",
    "    score = f1_v - h_loss_v\n",
    "    score_t = f1_t - h_loss_t\n",
    "    if score>max_score:\n",
    "        print('Best choice threashold now is:',thr,\n",
    "              '本模型最佳的validation F1,該thr對應的testing F1,最佳的validation hamming_loss,對應的testing hamming_loss:',\n",
    "              f1_v,f1_t,h_loss_v,h_loss_t)\n",
    "        max_score = score\n",
    "    score_list.append(score)\n",
    "    score_list_t.append(score_t)\n",
    "    threashold_list.append(thr)\n",
    "    f1_list_v.append(f1_v)\n",
    "    pre_list_v.append(precision_v)\n",
    "    rec_list_v.append(recall_v)\n",
    "    f1_list_t.append(f1_t)\n",
    "    pre_list_t.append(precision_t)\n",
    "    rec_list_t.append(recall_t)\n",
    "    hloss_list_v.append(h_loss_v)\n",
    "    hloss_list_t.append(h_loss_t)\n",
    "#     print('Valid: (recall/precision/f1/h_loss)=>',recall_v ,precision_v, f1_v , h_loss_v)\n",
    "print('final thr(best in DEV set) is the last threshold that print out. 相減Score =',max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不同Filter threshold實驗與匯出excel畫圖\n",
    "* 實驗#1-3\n",
    "* Sec. 4.2 Filter實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(data={'threshold':threashold_list,'Precision_valid':pre_list_v,'Recall_valid':rec_list_v\n",
    "                              ,'F1_valid':f1_list_v,'Hloss_valid':hloss_list_v\n",
    "                              ,'score_valid':score_list,'Precision_test':pre_list_t,'Recall_test':rec_list_t,\n",
    "                              'F1_test':f1_list_t,'Hloss_test':hloss_list_t\n",
    "                              ,'score_test':score_list_t})\n",
    "score_df.to_excel('data/tree-rep-profiles_o2o/threxp_F1_rec_pre_Hloss.xlsx',index=False) #輸出檔案位置\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for EXP#1-1、EXP#1-2，觀察不同模型_test的表現 (要把threshold改成該模型再DEV set最佳的thr)\n",
    "score_df[score_df['threshold']== 0.51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training process之loss與acc輸出至excel畫圖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用history file輸出\n",
    "* 記得更改H檔案存放路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './logs/MLP_2dense_0718/'\n",
    "history = pickle.load(open(log_dir+'MLP_3dense_gruatt_H.pkl','rb')) #train過程所存放\n",
    "keys = list(history.keys())\n",
    "keys #有哪些指標可供提供"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(data={keys[0]:history[keys[0]], keys[1]:history[keys[1]],\n",
    "                               keys[2]:history[keys[2]], keys[3]:history[keys[3]]})\n",
    "history_df.to_excel('data/tree-rep-profiles_o2o/encEXP_MLP_history_0718.xlsx',index=True) #輸出志excel\n",
    "history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用TF events輸出\n",
    "* 記得更改event file位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_file = log_dir+'events.out.tfevents.1562415044.superpc4'\n",
    "for event in tf.train.summary_iterator(event_file):\n",
    "    for value in event.summary.value:\n",
    "        print(value.tag) #有哪些指標可供提供\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e，即event，代表某一個batch的日誌記錄\n",
    "for e in tf.train.summary_iterator(event_file):\n",
    "    # v，即value，代表這個batch的某個已記錄的觀測值，loss或者accuracy\n",
    "    for v in e.summary.value:\n",
    "        if v.tag == 'loss' or v.tag == 'bin_acc': #輸出自己想要關注的指標\n",
    "            print(v.tag,v.simple_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix Spplementary\n",
    "* 比較不同方法實驗用的code (實驗一的MLP和Transformer)\n",
    "* Need modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXP#1-1: 更改NN architecture input的embedding layer或是Sent2Vec input就好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXP#1-2: \n",
    "* GRU只要architecture的地方留下GRU拿掉self-attention就好了\n",
    "* Dense則只要把GRU跟self-attention改成以下，再輸出到output layer\n",
    "```\n",
    "dense1 = Dense(int(emb_dim/4),activation='relu',name='dense1')(segment_embeddings)\n",
    "dense2 = Dense(int(emb_dim/4),activation='relu',name='dense2')(dense1)\n",
    "```\n",
    "* Transformer Encoder architecture 完整範例如以下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\n",
    "sentemb = Masking(mask_value=0)(sentemb1)\n",
    "sent_ids1 = Input(shape=(max_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\n",
    "sent_ids = Masking(mask_value=0)(sent_ids1)\n",
    "\n",
    "api_emb = Embedding(vocabulary_size+1, emb_dim,input_length=max_length,weights=[emb_matrix]\n",
    "                    ,trainable=True,name='api_emb')(sent_ids) #改\n",
    "\n",
    "\n",
    "segment_embeddings = Add()([sentemb,api_emb])\n",
    "removed_seg = RemoveMask()(segment_embeddings)\n",
    "next_step_input = coordinate_embedding_layer(removed_seg, step=0)\n",
    "next_step_input = add_segment_layer([next_step_input, removed_seg])\n",
    "for i in range(transformer_depth): \n",
    "    next_step_input = (\n",
    "        TransformerBlock(\n",
    "            name='transformer' + str(i), num_heads=num_heads,\n",
    "            residual_dropout=transformer_dropout,\n",
    "            attention_dropout=transformer_dropout,\n",
    "            use_masking=False,  # Allow bi-directional attention\n",
    "            vanilla_wiring=True)\n",
    "        (next_step_input))\n",
    "restored_seg = RestoreMask()([next_step_input,segment_embeddings])\n",
    "att_rnn = BatchNormalization(name='bn')(restored_seg)\n",
    "\n",
    "rep_prediction = (\n",
    "        TimeDistributed(Dense(1, name='0_1_predict', activation='sigmoid',trainable=True),name='out_rep') # hard_sigmoid\n",
    "    (att_rnn)) \n",
    "\n",
    "model = Model(inputs=[sent_ids1,sentemb1], outputs=[rep_prediction]) #out\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
