{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "from keras.utils import *\n",
    "from keras.utils.generic_utils import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.preprocessing.image import *\n",
    "from multiprocessing import *\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.manifold import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "import sent2vec\n",
    "import re\n",
    "import string\n",
    "import unicodedata as udata\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_li = ['LoadLibrary',\n",
    "'CreateProcess',\n",
    "'OpenProcess',\n",
    "'ExitProcess',\n",
    "'TerminateProcess',\n",
    "'WinExec',\n",
    "'CreateRemoteThread',\n",
    "'CreateThread',\n",
    "'CopyFile',\n",
    "'CreateFile',\n",
    "'DeleteFile',\n",
    "'RegSetValue',\n",
    "'RegCreateKey',\n",
    "'RegDeleteKey',\n",
    "'RegDeleteValue',\n",
    "'RegQueryValue',\n",
    "'RegEnumValue',\n",
    "'WinHttpConnect',\n",
    "'WinHttpOpen',\n",
    "'WinHttpOpenRequest',\n",
    "'WinHttpReadData',\n",
    "'WinHttpSendRequest',\n",
    "# 'WinHttpWriteData', #少了\n",
    "'InternetOpen',\n",
    "'InternetConnect',\n",
    "'HttpSendRequest',\n",
    "'GetUrlCacheEntryInfo']\n",
    "api_li = [x.lower() for x in api_li] #lowrer case?小寫?\n",
    "len(api_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:06<00:00, 20.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['createfile pr sys exe pr generic read pr open existing pr file share delete file share read ret 0',\n",
       " 'loadlibrary pr sys explorer exe ret 0',\n",
       " 'createfile pr sys exe pr generic read pr open existing pr file share delete file share read ret 0',\n",
       " 'regqueryvalue pr hkcu soft ms win explorer shellfolders pr subk cache pr 0 pr 12e8f4 ret 0',\n",
       " 'loadlibrary pr sys advapi32 dll ret 0',\n",
       " 'loadlibrary pr sys oleaut32 dll ret 0',\n",
       " 'regqueryvalue pr hklm software microsoft windows html help pr subk hlp pr 0 pr cb0398 ret p',\n",
       " 'regqueryvalue pr hklm software microsoft windows help pr subk hlp pr 0 pr cb0398 ret p',\n",
       " 'exitprocess pr 0',\n",
       " 'deletefile pr usr tmp ret 0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "dil= r\"[,.;\\-+^()/@#?!&$:{}\\\\*%~\\'\\\"\\=\\_]+\\ *\" #等號、底線被保留\n",
    "\n",
    "root_dir = './data/tree-rep-profiles_o2o/normal/'\n",
    "# rasmma_dir  = next(os.walk(root_dir))[1]\n",
    "# for dir_ in rasmma_dir:\n",
    "fam_dir = next(os.walk(root_dir ))[1]\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir  + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        for profile in hl_list:\n",
    "            with open(profile,encoding='ISO 8859-1') as f:\n",
    "                lines = f.read()\n",
    "            lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "            lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c]+','', lines)\n",
    "            lines = lines.splitlines()\n",
    "            for line in lines:\n",
    "                temp = re.sub(dil,\" \",line.lower())#.lower()) #小寫?\n",
    "                temp = temp.split(\" \")\n",
    "                temp = list(filter(None, temp))\n",
    "                temp = ' '.join(temp)\n",
    "                if temp.startswith('winh'.lower()):\n",
    "                    print(profile,temp)\n",
    "                if temp.split(' ')[0] not in api_li:\n",
    "                    print('=o2o_ERR:=',profile,'=>',temp)\n",
    "                    continue\n",
    "                corpus.append(temp)\n",
    "\n",
    "with open('data/sent2vec_corpus/o2o_Sent2Vec_lower_woParam_0630.txt','w') as f:\n",
    "    f.write('\\n'.join(corpus))\n",
    "corpus[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sent2Vec Training time \n",
    "* cd ~/Downloads/sent2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sent2vec_corpus/o2o_Sent2Vec_lower_woParam_0630.txt','r') as f: #改\n",
    "    corpus = f.read().splitlines()\n",
    "corpus = sorted(corpus)\n",
    "corpus[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('model/o2o_o2m_Sent2Vec_woParam_0616.bin') #改\n",
    "emb_all = model.embed_sentences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = \"\"\n",
    "count = 0\n",
    "count_all = {}\n",
    "for row in corpus:\n",
    "    try:\n",
    "        count_all[row.split(' ')[0]] += 1\n",
    "    except KeyError:\n",
    "        count_all[row.split(' ')[0]] = 1\n",
    "print(len(count_all))\n",
    "sorted(count_all.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = list(count_all.keys())\n",
    "for k in api_li:\n",
    "    if k not in kk:\n",
    "        print('少了:',k)\n",
    "for k in kk:\n",
    "    if k not in api_li:\n",
    "        print('多了',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= 0\n",
    "df = pd.DataFrame(0, index=api_li, columns=api_li)\n",
    "index_ = df.index.tolist()\n",
    "avg = 100\n",
    "for _ in range(avg):\n",
    "    c=0\n",
    "    for k,v in count_all.items():\n",
    "        pick = random.randrange(c,c+v) #cosine similarity的左邊\n",
    "        pick = emb_all[pick,:] #sorted corpus的embedding_all\n",
    "        c1 = 0\n",
    "        try:\n",
    "            row = index_.index(k)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        for k1,v1 in count_all.items(): # cosine similarity的右邊\n",
    "            pick1 = random.randrange(c1,c1+v1)\n",
    "            pick1 = emb_all[pick1,:]\n",
    "            score = cosine_similarity([pick],[pick1]) #cosine_similarity\n",
    "    #         print(score)\n",
    "            c1 = c1+v1\n",
    "            try:\n",
    "                col = index_.index(k1)\n",
    "                df.iloc[row,col] += float(score)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        c = c+v\n",
    "df = df/avg #similarity\n",
    "# df = (df-np.min(df.values))/(np.max(df.values)-np.min(df.values)) #distance normalize\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = ['LoadLibrary']\n",
    "proc = ['CreateProcess', 'OpenProcess', 'ExitProcess', 'TerminateProcess', 'WinExec',\n",
    "        'CreateRemoteThread', 'CreateThread']\n",
    "file = ['CopyFile', 'CreateFile', 'DeleteFile']\n",
    "reg = ['RegSetValue', 'RegCreateKey', 'RegDeleteKey', 'RegDeleteValue',\n",
    "       'RegQueryValue', 'RegEnumValue']\n",
    "net = ['WinHttpConnect', 'WinHttpOpen', 'WinHttpOpenRequest', 'WinHttpReadData', 'WinHttpSendRequest', #'WinHttpWriteData',\n",
    "        'InternetOpen', 'InternetConnect', 'HttpSendRequest', 'GetUrlCacheEntryInfo']\n",
    "\n",
    "#lower?\n",
    "lib = [x.lower() for x in lib]\n",
    "proc = [x.lower() for x in proc]\n",
    "file = [x.lower() for x in file]\n",
    "reg =[x.lower() for x in reg]\n",
    "net = [x.lower() for x in net]\n",
    "\n",
    "# df.loc['']\n",
    "x = [lib,proc,file,reg,net]\n",
    "x = sum(x,[])\n",
    "index_ , len(index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up=0\n",
    "down = len(index_)\n",
    "down_cat = len(x)\n",
    "up_cat = 0\n",
    "up_cat2 = 0\n",
    "lib_lib=lib_proc=lib_file=lib_reg=lib_net = 0\n",
    "proc_lib=proc_proc=proc_file=proc_reg=proc_net = 0\n",
    "file_lib=file_proc=file_file=file_reg=file_net = 0\n",
    "reg_lib=reg_proc=reg_file=reg_reg=reg_net = 0\n",
    "net_lib=net_proc=net_file=net_reg=net_net = 0\n",
    "for api in index_:\n",
    "    if df.loc[api].idxmax() == api:\n",
    "        up+=1\n",
    "    else:\n",
    "        print(api)\n",
    "    baseline_lib = df.loc[api,lib].mean()\n",
    "    baseline_proc =  df.loc[api,proc].mean()\n",
    "    baseline_file = df.loc[api,file].mean()\n",
    "    baseline_reg = df.loc[api,reg].mean()\n",
    "    baseline_net = df.loc[api,net].mean()\n",
    "    max_value = np.max([baseline_lib,baseline_proc,baseline_file,baseline_reg,baseline_net])\n",
    "    if (api in lib):\n",
    "        lib_lib += baseline_lib\n",
    "        lib_proc += baseline_proc\n",
    "        lib_file += baseline_file\n",
    "        lib_reg += baseline_reg\n",
    "        lib_net += baseline_net\n",
    "        if (max_value==baseline_lib):\n",
    "            up_cat+=1\n",
    "    elif api in proc :\n",
    "        proc_lib += baseline_lib\n",
    "        proc_proc += baseline_proc\n",
    "        proc_file += baseline_file\n",
    "        proc_reg += baseline_reg\n",
    "        proc_net += baseline_net\n",
    "        if max_value==baseline_proc:\n",
    "            up_cat+=1\n",
    "    elif api in file :\n",
    "        file_lib += baseline_lib\n",
    "        file_proc += baseline_proc\n",
    "        file_file += baseline_file\n",
    "        file_reg += baseline_reg\n",
    "        file_net += baseline_net\n",
    "        if max_value==baseline_file:\n",
    "            up_cat+=1    \n",
    "    elif api in reg:\n",
    "        reg_lib += baseline_lib\n",
    "        reg_proc += baseline_proc\n",
    "        reg_file += baseline_file\n",
    "        reg_reg += baseline_reg\n",
    "        reg_net += baseline_net\n",
    "        if max_value==baseline_reg:\n",
    "            up_cat+=1    \n",
    "    elif api in net:\n",
    "        net_lib += baseline_lib\n",
    "        net_proc += baseline_proc\n",
    "        net_file += baseline_file\n",
    "        net_reg += baseline_reg\n",
    "        net_net += baseline_net\n",
    "        if max_value==baseline_net:\n",
    "            up_cat+=1\n",
    "if np.max([lib_lib,lib_proc,lib_file,lib_reg,lib_net]) == lib_lib:\n",
    "    up_cat2+=1\n",
    "if np.max([proc_lib,proc_proc,proc_file,proc_reg,proc_net]) == proc_proc:\n",
    "    up_cat2+=1\n",
    "if np.max([file_lib,file_proc,file_file,file_reg,file_net]) == file_file:\n",
    "    up_cat2+=1\n",
    "if np.max([reg_lib,reg_proc,reg_file,reg_reg,reg_net]) == reg_reg:\n",
    "    up_cat2+=1\n",
    "if np.max([net_lib,net_proc,net_file,net_reg,net_net]) == net_net:\n",
    "    up_cat2+=1\n",
    "        \n",
    "print('自己還原自己-ACC:',(up/down)*100,'%') #self\n",
    "print('自己屬於自己那一類-ACC(他自己那類的分數平均是不是所有類別分數最高的):',(up_cat/down_cat)*100,'%') #micro\n",
    "print('自己那類屬於自己那一類-ACC:',(up_cat2/5)*100,'%') #macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for i in range(len(df)):\n",
    "    score.append(df.iloc[i,i])\n",
    "print('對角線分數(越高越好):',np.mean(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
