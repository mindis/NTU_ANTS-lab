{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本上可以跳過不用跑\n",
    "\n",
    "除非要重train bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch , json, glob\n",
    "from pytorch_pretrained_bert import *\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import os,sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "from keras.utils import *\n",
    "from keras.utils.generic_utils import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.preprocessing.image import *\n",
    "from multiprocessing import *\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.manifold import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "import sent2vec\n",
    "import re\n",
    "import string\n",
    "import unicodedata as udata\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "from multiprocessing import Pool, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_pid_train = pickle.load(open('./data/tree-rep-profiles-partial/TRAIN_pidNames.pkl','rb'))\n",
    "samples_pid_valid = pickle.load(open('./data/tree-rep-profiles-partial/DEV_pidNames.pkl','rb'))\n",
    "samples_pid_test = pickle.load(open('./data/tree-rep-profiles-partial/TEST_pidNames.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [00:00<00:00, 1547.65it/s]\n",
      "100%|██████████| 183/183 [00:00<00:00, 3255.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# all many2many pid\n",
    "all_m2m_pid = []\n",
    "root_dir = './data/tree-rep-profiles_one2many/'\n",
    "rasmma_dir  = next(os.walk(root_dir))[1]\n",
    "for dir_ in rasmma_dir:\n",
    "    fam_dir = next(os.walk(root_dir + dir_))[1]\n",
    "    for fam in tqdm(fam_dir):\n",
    "        tree_dir = next(os.walk(root_dir + dir_+ '/' + fam))[1]\n",
    "        for tree in tree_dir:\n",
    "            in_directory = root_dir + dir_ + '/' + fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            all_m2m_pid.extend(hl_list)\n",
    "all_m2m_pid = set(all_m2m_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_li = ['LoadLibrary',\n",
    "'CreateProcess',\n",
    "'OpenProcess',\n",
    "'ExitProcess',\n",
    "'TerminateProcess',\n",
    "'WinExec',\n",
    "'CreateRemoteThread',\n",
    "'CreateThread',\n",
    "'CopyFile',\n",
    "'CreateFile',\n",
    "'DeleteFile',\n",
    "'RegSetValue',\n",
    "'RegCreateKey',\n",
    "'RegDeleteKey',\n",
    "'RegDeleteValue',\n",
    "'RegQueryValue',\n",
    "'RegEnumValue',\n",
    "'WinHttpConnect',\n",
    "'WinHttpOpen',\n",
    "'WinHttpOpenRequest',\n",
    "'WinHttpReadData',\n",
    "'WinHttpSendRequest',\n",
    "# 'WinHttpWriteData', #少了\n",
    "'InternetOpen',\n",
    "'InternetConnect',\n",
    "'HttpSendRequest',\n",
    "'GetUrlCacheEntryInfo']\n",
    "# api_li = [x.lower() for x in api_li] #lowrer case?\n",
    "len(api_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [00:40<00:00,  4.52it/s]\n",
      "100%|██████████| 183/183 [00:09<00:00, 20.11it/s]\n",
      "100%|██████████| 61/61 [00:10<00:00, 11.65it/s]\n",
      "100%|██████████| 166/166 [00:22<00:00, 10.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'DeleteFile SYS EXE Ret 0',\n",
       " '\\n',\n",
       " 'CreateFile USR EXE GENERIC_READ OPEN_ALWAYS FILE_SHARE_READ;FILE_SHARE_WRITE Ret 0',\n",
       " '\\n',\n",
       " 'CreateFile SYS EXE GENERIC_WRITE OPEN_ALWAYS FILE_SHARE_READ;FILE_SHARE_WRITE Ret 0',\n",
       " '\\n',\n",
       " 'RegCreateKey HKCU hkey_current_user Software\\\\VB and VBA Program Settings\\\\Explorer\\\\Process Ret 0',\n",
       " '\\n',\n",
       " 'RegSetValue HKCU software\\\\vb and vba program settings\\\\explorer\\\\process\\\\lo REG_SZ 1 Ret 0',\n",
       " '\\n',\n",
       " 'CreateProcess CMD Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS version DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS explorer EXE Ret 0',\n",
       " '\\n',\n",
       " 'CreateFile SYS EXE GENERIC_READ OPEN_EXISTING FILE_SHARE_DELETE;FILE_SHARE_READ Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS explorer EXE Ret 0',\n",
       " '\\n',\n",
       " 'CreateFile SYS EXE GENERIC_READ OPEN_EXISTING FILE_SHARE_DELETE;FILE_SHARE_READ Ret 0',\n",
       " '\\n',\n",
       " 'RegQueryValue HKCU soft_ms_win_explorer_shellFolders\\\\* SUBK cache 0 12e8f4 Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS advapi32 DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS oleaut32 DLL Ret 0',\n",
       " '\\n',\n",
       " 'RegQueryValue HKLM software\\\\microsoft\\\\windows\\\\html help SUBK .hlp 0 cb0398 Ret P',\n",
       " '\\n',\n",
       " 'RegQueryValue HKLM software\\\\microsoft\\\\windows\\\\help SUBK .hlp 0 cb0398 Ret P',\n",
       " '\\n',\n",
       " 'ExitProcess 0',\n",
       " '\\n',\n",
       " 'DeleteFile USR TMP Ret 0',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'RegQueryValue HKLM sys_curCtlSet_ctl_sessionManager\\\\* SUBK criticalsectiontimeout 0 12f9b0 Ret 0',\n",
       " '\\n',\n",
       " 'RegQueryValue HKLM soft_ms_ole\\\\* SUBK rwlockresourcetimeout 0 12f9b4 Ret P',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS uxtheme DLL Ret 0',\n",
       " '\\n',\n",
       " 'RegQueryValue HKCU soft_ms_win_thememanager\\\\* SUBK compositing 12e66c 12e67c Ret P',\n",
       " '\\n',\n",
       " 'RegQueryValue HKCU desktop\\\\ SUBK lamebuttontext 12e878 5ada1620 Ret P',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS uxtheme DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS oleaut32 DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS sxs DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS oleaut32 DLL Ret 0',\n",
       " '\\n',\n",
       " 'RegQueryValue HKLM soft_ms_winNT_imm\\\\* SUBK ime file 0 12f44c Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS version DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS msctfime IME Ret 0',\n",
       " '\\n',\n",
       " 'CreateFile SYS IME GENERIC_READ OPEN_EXISTING FILE_SHARE_DELETE;FILE_SHARE_READ Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS msctfime IME Ret 0',\n",
       " '\\n',\n",
       " 'CreateFile SYS IME GENERIC_READ OPEN_EXISTING FILE_SHARE_DELETE;FILE_SHARE_READ Ret 0',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'RegQueryValue HKLM sys_curCtlSet_ctl_sessionManager\\\\* SUBK criticalsectiontimeout 0 12f9b0 Ret 0',\n",
       " '\\n',\n",
       " 'RegQueryValue HKLM soft_ms_ole\\\\* SUBK rwlockresourcetimeout 0 12f9b4 Ret P',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS uxtheme DLL Ret 0',\n",
       " '\\n',\n",
       " 'RegQueryValue HKCU soft_ms_win_thememanager\\\\* SUBK compositing 12e66c 12e67c Ret P',\n",
       " '\\n',\n",
       " 'RegQueryValue HKCU desktop\\\\ SUBK lamebuttontext 12e878 5ada1620 Ret P',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS uxtheme DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS oleaut32 DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS sxs DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS oleaut32 DLL Ret 0',\n",
       " '\\n',\n",
       " 'RegQueryValue HKLM soft_ms_winNT_imm\\\\* SUBK ime file 0 12f44c Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS version DLL Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS msctfime IME Ret 0',\n",
       " '\\n',\n",
       " 'CreateFile SYS IME GENERIC_READ OPEN_EXISTING FILE_SHARE_DELETE;FILE_SHARE_READ Ret 0',\n",
       " '\\n',\n",
       " 'LoadLibrary SYS msctfime IME Ret 0',\n",
       " '\\n',\n",
       " 'CreateFile SYS IME GENERIC_READ OPEN_EXISTING FILE_SHARE_DELETE;FILE_SHARE_READ Ret 0',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "dil= r\"[@#]+\\ *\" #等號、底線被保留\n",
    "\n",
    "root_dir = './data/tree-rep-profiles_one2many/'\n",
    "rasmma_dir  = next(os.walk(root_dir))[1]\n",
    "for dir_ in rasmma_dir:\n",
    "    fam_dir = next(os.walk(root_dir + dir_))[1]\n",
    "    for fam in tqdm(fam_dir):\n",
    "        tree_dir = next(os.walk(root_dir + dir_+ '/' + fam))[1]\n",
    "        for tree in tree_dir:\n",
    "            in_directory = root_dir + dir_ + '/' + fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            for hl in hl_list:\n",
    "                try:\n",
    "                    if hl in samples_pid_test: #移除在test裡面的hkl\n",
    "                        hl_list.remove(hl)\n",
    "                    if hl in samples_pid_valid: #移除在valid裡面的hkl\n",
    "                        hl_list.remove(hl)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "#             hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            for profile in hl_list:\n",
    "                with open(profile,encoding='ISO 8859-1') as f:\n",
    "                    lines = f.read()\n",
    "                lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "                lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c]+','', lines)\n",
    "                lines = lines.splitlines()\n",
    "                for line in lines:\n",
    "                    temp = re.sub(dil,\" \",line) #lower小寫?\n",
    "                    temp = temp.replace('PR',' ')\n",
    "                    temp = temp.split(\" \")\n",
    "                    temp = list(filter(None, temp))\n",
    "                    temp = ' '.join(temp)\n",
    "                    if temp.split(' ')[0] not in api_li:\n",
    "                        print('=o2m_ERR:=',profile,'=>',temp)\n",
    "                        continue\n",
    "                    corpus.append(temp)\n",
    "                    corpus.append('\\n')\n",
    "                corpus.append('\\n')\n",
    "\n",
    "root_dir = './data/tree-rep-profiles_one2one/'\n",
    "rasmma_dir  = next(os.walk(root_dir))[1]\n",
    "for dir_ in rasmma_dir:\n",
    "    fam_dir = next(os.walk(root_dir + dir_))[1]\n",
    "    for fam in tqdm(fam_dir):\n",
    "        tree_dir = next(os.walk(root_dir + dir_+ '/' + fam))[1]\n",
    "        for tree in tree_dir:\n",
    "            in_directory = root_dir + dir_ + '/' + fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            for hl in hl_list:\n",
    "                try:\n",
    "                    if hl in all_m2m_pid: #移除已經有的pid\n",
    "                        hl_list.remove(hl)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            for profile in hl_list:\n",
    "                with open(profile,encoding='ISO 8859-1') as f:\n",
    "                    lines = f.read()\n",
    "                lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "                lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c]+','', lines)\n",
    "                lines = lines.splitlines()\n",
    "                for line in lines:\n",
    "                    temp = re.sub(dil,\" \",line) #小寫?\n",
    "                    temp = temp.replace('PR','')\n",
    "                    temp = temp.split(\" \")\n",
    "                    temp = list(filter(None, temp))\n",
    "                    temp = ' '.join(temp)\n",
    "                    if temp.split(' ')[0] not in api_li:\n",
    "                        print('=o2o_ERR:=',profile,'=>',temp)\n",
    "                        continue\n",
    "                    corpus.append(temp)\n",
    "                    corpus.append('\\n')\n",
    "                corpus.append('\\n')\n",
    "\n",
    "with open('data/bert_corpus/o2o_o2m_Bert_0530.txt','w') as f:\n",
    "    f.write(''.join(corpus))\n",
    "corpus[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/leoqaz12/.cache/torch/pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "100%|██████████| 4666391/4666391 [32:57<00:00, 2359.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/bert_corpus/o2o_o2m_Bert_0530.txt','r') as f: #asp1_sentences_rep_0116\n",
    "    corpus = f.read().splitlines()\n",
    "model = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=False)\n",
    "lens = []\n",
    "for line in tqdm(corpus):\n",
    "    tokens = tokenizer.tokenize(line)\n",
    "    length = len(tokens)\n",
    "    if length < 2:\n",
    "        continue\n",
    "    else:\n",
    "        lens.append(length)\n",
    "print(max(lens),min(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n",
      "239\n",
      "239\n",
      "239\n",
      "239\n",
      "239\n",
      "239\n",
      "239\n",
      "239\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "lens_sort = sorted(lens)\n",
    "for i in [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10]:\n",
    "    idx = lens.index(lens_sort[-1])\n",
    "    print(max(lens[idx-1] + lens[idx] , lens[idx+1] + lens[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 pregenerate_training_data.py --train_corpus ../../../data/bert_corpus/o2o_o2m_Bert_0530 --bert_model bert-base-multilingual-cased --output_dir ../../../model/bert_pretrained/less_pretrained_bert/ --epochs_to_generate 3 --max_seq_len 239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 finetune_on_pregenerated.py --pregenerated_data ../../../model/bert_pretrained/less_pretrained_bert/ --bert_model bert-base-multilingual-cased --output_dir /SAMBA/r06725035/model/less_pretrain_bert/bert/  --epochs 3 --gradient_accumulation_steps 3 --learning_rate 2e-5  --train_batch_size 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dil= r\"[@#]+\\ *\" #等號、底線被保留\n",
    "def clean_profile(profile_path,dil=dil):\n",
    "    '''\n",
    "    input: profile (path string)\n",
    "    output: clean profile (corresponding path)\n",
    "    '''\n",
    "    corpus = []\n",
    "    with open(profile_path,encoding='ISO 8859-1') as f:\n",
    "        lines = f.read()\n",
    "    lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "    lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c]+','', lines)\n",
    "    lines = lines.splitlines()\n",
    "    for line in lines:\n",
    "        temp = re.sub(dil,\" \",line) #小寫?\n",
    "        temp = temp.replace('PR','')\n",
    "        temp = temp.split(\" \")\n",
    "        temp = list(filter(None, temp))\n",
    "        temp = ' '.join(temp)\n",
    "        corpus.append(temp)\n",
    "        corpus.append('\\n')\n",
    "    with open(profile_path.replace('.profile','.profile_clean'),'w') as f:\n",
    "        f.write(''.join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def store_feature(profile_clean,model_path,out,max_seq_length=239,batch_size=32):\n",
    "    '''\n",
    "    input: profile_clean (path string)\n",
    "    output: profile feature json dicts\n",
    "    '''\n",
    "#     out = profile_clean.replace('.profile_clean','.feature')\n",
    "    !python3 ./pytorch-pretrained-BERT/examples/extract_features.py --input_file $profile_clean --output_file $out --bert_model $model_path --batch_size $batch_size #&>/tmp/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz from cache at /home/leoqaz12/.cache/torch/pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /home/leoqaz12/.cache/torch/pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9 to temp dir /tmp/tmpd1nf5u0i\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz from cache at /home/leoqaz12/.cache/torch/pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /home/leoqaz12/.cache/torch/pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9 to temp dir /tmp/tmpr5co59hs\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/leoqaz12/.cache/torch/pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
     ]
    }
   ],
   "source": [
    "model_path1 = '/DATA/r06725035/model/less_pretrain_bert/bert/pytorch_model.bin'\n",
    "model_state_dict1 = torch.load(model_path1,  map_location='cpu')\n",
    "model1 = BertModel.from_pretrained(pretrained_model_name_or_path='bert-base-multilingual-cased',state_dict=model_state_dict1)\n",
    "model1 = torch.nn.DataParallel(model1)\n",
    "model1.eval()\n",
    "\n",
    "model_path2 = './model/bert_pretrained/bert/pytorch_model.bin'\n",
    "model_state_dict2 = torch.load(model_path2,  map_location='cpu')\n",
    "model2 = BertModel.from_pretrained(pretrained_model_name_or_path='bert-base-multilingual-cased',state_dict=model_state_dict2)\n",
    "model2 = torch.nn.DataParallel(model2)\n",
    "model2.eval()\n",
    "# model = BertModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path='bert-base-multilingual-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_feature(hl_path,model1=model1,model2=model2,tokenizer=tokenizer,sent=True):\n",
    "    last = [-1,-2,-3,-4] #要拿最後幾層?\n",
    "    cls_np_all = []\n",
    "    token_np_all = []\n",
    "    with open(hl_path,encoding='ISO 8859-1') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        line = '[CLS] ' + line + ' [SEP]'\n",
    "        tokenized_text = tokenizer.tokenize(line)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [0]*len(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "#         tokens_tensor = tokens_tensor.cuda() #.to(device)\n",
    "#         segments_tensors = segments_tensors.cuda() #.to(device)\n",
    "        if sent:\n",
    "            model1.cuda()\n",
    "            tokens_tensor = tokens_tensor.cuda()\n",
    "            segments_tensors = segments_tensors.cuda()\n",
    "        else:\n",
    "            model2.cuda()\n",
    "            tokens_tensor = tokens_tensor.cuda() #.to(device)\n",
    "            segments_tensors = segments_tensors.cuda() #.to(device)\n",
    "#         print(len(tokenized_text))\n",
    "        if sent: #只拿[CLS] 最後四層\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = model1(tokens_tensor, segments_tensors)\n",
    "            cls_np = []\n",
    "            for layer in last:\n",
    "                cls_np.append(np.array(encoded_layers[layer].cpu())[0][0])\n",
    "            last_np = np.sum(cls_np,axis=0)\n",
    "            cls_np_all.append(last_np)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = model2(tokens_tensor, segments_tensors)\n",
    "            length = len(encoded_layers[-1][0])\n",
    "            all_np = []\n",
    "            for layer in last:\n",
    "                for token in range(1,length-1):\n",
    "                    array = np.array(encoded_layers[layer].cpu())[0][token]\n",
    "                    all_np.append(array)\n",
    "            last_np = np.average(all_np,axis=0)\n",
    "            token_np_all.append(last_np) #只有一個lline!!\n",
    "    try:\n",
    "        del model1\n",
    "        del model2\n",
    "    except:\n",
    "        torch.cuda.empty_cache()\n",
    "    if sent:\n",
    "        return np.array(cls_np_all) #[CLS]\n",
    "    else:\n",
    "        return np.array(token_np_all) # tokens\n",
    "            \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = [-1,-2,-3,-4]\n",
    "# bert_model = 'bert-base-multilingual-cased'\n",
    "# load_model = '/DATA/r06725035/model/less_pretrain_bert/bert/pytorch_model.bin'\n",
    "# do_lower_case = False\n",
    "# input_file = 'test.txt'\n",
    "# max_seq_length = 239\n",
    "# batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/tree-rep-profiles_one2many/normal/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 123/184 [6:20:48<4:00:37, 236.67s/it] "
     ]
    }
   ],
   "source": [
    "#[CLS]\n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        for hl in hl_list:\n",
    "            clean_profile(hl)\n",
    "#         in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile_clean\"), hl_list))\n",
    "        for hl in hl_list:\n",
    "            cls_feature = get_feature(hl,model1,model2,sent=True) #還傳該hkl的兩種feature\n",
    "            cls_file = hl.replace('.profile_clean','_clsfeature.pkl')\n",
    "#             token_file = hl.replace('.profile_clean','_tokenfeature.pkl')\n",
    "            pickle.dump(file=open(cls_file,'wb'),obj=cls_feature)\n",
    "#             pickle.dump(file=open(token_file,'wb'),obj=tokens_feature)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy corresponding feature\n",
    "* src= './data/tree-rep-profiles_one2many/normal/'\n",
    "* dest = './data/tree-rep-profiles-partial/normal/'\n",
    "    * './data/tree-rep-profiles-partial/DEV/'\n",
    "    * './data/tree-rep-profiles-partial/TEST/'\n",
    "    \n",
    "- 需要先將src dir也進行合併以後才可以使用下列code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_dir = './data/tree-rep-profiles_one2many/normal/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def restore_feature(root_dir,src_dir=src_dir):\n",
    "#     fam_dir = next(os.walk(root_dir))[1]\n",
    "#     for fam in tqdm(fam_dir):\n",
    "#         tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "#         for tree in tree_dir:\n",
    "#             in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "#             hl_list = next(os.walk(in_directory))[2]\n",
    "#             hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "#             hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "#             for hl in hl_list:\n",
    "#                 files = glob.glob(src_dir + fam + '/' + tree + '/'+hl.split('/')[-1].replace('.profile','*'))\n",
    "#             for file in files:\n",
    "#                 shutil.copy(file,in_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:12<00:00,  4.13it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 161.15it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 158.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# root_dir = './data/tree-rep-profiles-partial/normal/'\n",
    "# restore_feature(root_dir)\n",
    "# root_dir = './data/tree-rep-profiles-partial/DEV/'\n",
    "# restore_feature(root_dir)\n",
    "# root_dir = './data/tree-rep-profiles-partial/TEST/'\n",
    "# restore_feature(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
