{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, csv\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import xlsxwriter\n",
    "import random\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import unicodedata as udata\n",
    "import pause, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from distutils.dir_util import copy_tree\n",
    "import sklearn\n",
    "from sklearn.metrics import *\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算list中的各項統計資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements e.g.[1,1,1,3,5,9,4,2,1,3,54,78,5...]\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算總共有幾個family、Tree、samples、processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_hash_pids(root_dir):\n",
    "    '''\n",
    "    dir hierachical: root_dir/family_dirs/trees/XXX.profiles\n",
    "    '''\n",
    "    fam_dir = next(os.walk(root_dir))[1]\n",
    "    all_pids= []\n",
    "    all_pid_list = []\n",
    "    tree_count = 0\n",
    "    for fam in tqdm(fam_dir):\n",
    "        tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "        tree_count += len(tree_dir)\n",
    "        for tree in tree_dir:\n",
    "            in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            hash_list = [f.split('/')[-1].split('_')[0] for f in hl_list]\n",
    "            pid_list = [f.split('/')[-1] for f in hl_list]\n",
    "            all_pids.extend(hash_list)\n",
    "            all_pid_list.extend(pid_list)\n",
    "#             print(fam,len(hash_list)) #DEBUG\n",
    "    all_hash = set(all_pids)\n",
    "    print('Families#:',len(fam_dir),'Samples#:',len(all_hash),',Processes#:',len(set(all_pid_list)),',Trees#:',tree_count)\n",
    "    return all_hash, list(set(all_pid_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將list 或是 dict 輸出至excel當中畫分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_xlsx(statistics,i):\n",
    "    '''\n",
    "    Input: list or dict. i for iterations\n",
    "    Output: xlsx\n",
    "    '''\n",
    "    output_root_path = 'data/tree-rep-profiles_o2o/df' + str(i+1) + '.xlsx'\n",
    "    try:\n",
    "        df = basic_statistics(statistics)\n",
    "    except TypeError:\n",
    "        df = basic_statistics(list(statistics.values()))\n",
    "    df.to_excel(output_root_path,index=True,sheet_name='df'+str(i+1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "某個資料夾下的各項統計數值計算: process長度、rep長度、tree樣本數、tree執行程序數、一個sample有幾個process、一個family有幾個processes、一個family有幾個samples、一個family有幾個trees\n",
    "\n",
    "(包含小樹短樹的整體統計or僅正常樹的統計)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = './data/tree-rep-profiles_one2one/'\n",
    "'''\n",
    "root_dir hierachy: root_dir/normal(or small_short)/family_dirs/tree_dirs/XXX.profiles\n",
    "'''\n",
    "# api_length = []\n",
    "process_lengths_normal = [] #濾掉小行為樹與短行為樹最後所剩之processes長度統計\n",
    "process_lengths_overall = [] #包含小行為樹與短行為樹整體之processes長度統計\n",
    "rep_length_normal = []  #濾掉小行為樹與短行為樹最後所剩行為樹之representative call sequence長度\n",
    "rep_length_overall = [] #包含小行為樹與短行為樹之representative call sequence長度\n",
    "tree_samples_normal = [] #濾掉小行為樹與短行為樹最後剩下行為樹所包含之sample數量統計\n",
    "tree_samples_overall = [] #包含小行為樹與短行為樹之sample數量統計\n",
    "tree_processes_normal = [] #濾掉小行為樹與短行為樹最後剩下行為樹所包含之sample數量統計\n",
    "tree_processes_overall = [] #包含小行為樹與短行為樹之processes數量統計\n",
    "sample_processes_normal = {} #濾掉小行為樹與短行為樹後一個sample所擁有的process數量\n",
    "sample_processes_overall = {} #包含小行為樹與短行為樹一個sample所擁有的process數量\n",
    "fam_processes_normal = [] #濾掉小行為樹與短行為樹後一個Family所含之processes數量\n",
    "fam_processes_overall = dict.fromkeys(fam_dir_normal,0) #包含小行為樹與短行為樹一個Family所含之processes數量\n",
    "fam_samples_normal = [] #濾掉小行為樹與短行為樹後一個Family所含之samples數量\n",
    "fam_samples_overall = dict.fromkeys(fam_dir_normal,0) #包含小行為樹與短行為樹一個Family所含之samples數量\n",
    "# processed_profile = []\n",
    "trees_normal = [] #濾掉小行為樹與短行為樹最後所剩Family所含之行為樹個數統計\n",
    "trees_overall = dict.fromkeys(fam_dir_normal,0)  #包含小樹短樹Family所含之行為樹個數統計\n",
    "\n",
    "rasmma_dir = next(os.walk(root_dir))[1]\n",
    "for rasmma in rasmma_dir:\n",
    "    fam_dir = next(os.walk(root_dir+rasmma))[1]\n",
    "    for fam in tqdm(fam_dir):\n",
    "        if fam not in fam_dir_normal:\n",
    "            continue\n",
    "        tree_dir = next(os.walk(root_dir +rasmma+'/'+ fam))[1]\n",
    "        if rasmma == 'normal':\n",
    "#             assert len(tree_dir)>2\n",
    "            trees_normal.append(len(tree_dir))\n",
    "        trees_overall[fam] = trees_overall[fam] + len(tree_dir)\n",
    "        fam_processes = 0\n",
    "        fam_sample = []\n",
    "        fam_sample_all = []\n",
    "        for tree in tree_dir:\n",
    "            in_directory = root_dir+ rasmma+'/'+ fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            rep = pickle.load(open(in_directory + 'rep.pickle','rb'))\n",
    "            rep = sum(rep,[])\n",
    "            fam_processes_overall[fam] = fam_processes_overall[fam] + len(hl_list)\n",
    "            if rasmma == 'normal':\n",
    "                assert len(rep) > 10\n",
    "                assert len(hl_list) > 2\n",
    "                rep_length_normal.append(len(rep))\n",
    "                tree_processes_normal.append(len(hl_list))\n",
    "                fam_processes = fam_processes + len(hl_list)\n",
    "            rep_length_overall.append(len(rep))\n",
    "            tree_processes_overall.append(len(hl_list))\n",
    "            \n",
    "            tree_samples = []\n",
    "            for profile in hl_list:\n",
    "                hash_sample = profile.split('/')[-1].split('_')[0]\n",
    "                tree_samples.append(hash_sample)\n",
    "                try:\n",
    "                    sample_processes_overall[hash_sample] = sample_processes_overall[hash_sample] + 1\n",
    "                except KeyError:\n",
    "                    sample_processes_overall[hash_sample] = 1\n",
    "                \n",
    "#                 tree_samples_overall.append(hash_sample)\n",
    "#                 name = profile.split('/')[-1]\n",
    "#                 if name not in processed_profile:\n",
    "                with open(profile,encoding='ISO 8859-1') as f:\n",
    "                    lines = f.read()\n",
    "                lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "                lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "                lines = re.sub('y\\\\n','',lines)\n",
    "                lines = re.sub('=\\\\n','',lines)\n",
    "                lines = lines.splitlines()\n",
    "#                     processed_profile.append(name)\n",
    "                fam_sample_all.append(hash_sample)\n",
    "                if rasmma == 'normal':\n",
    "                    process_lengths_normal.append(len(lines))\n",
    "                    fam_sample.append(hash_sample)\n",
    "                    try:\n",
    "                        sample_processes_normal[hash_sample] = sample_processes_normal[hash_sample] + 1\n",
    "                    except KeyError:\n",
    "                        sample_processes_normal[hash_sample] = 1\n",
    "                process_lengths_overall.append(len(lines))\n",
    "            tree_samples_overall.append(len(set(tree_samples)))\n",
    "            if rasmma == 'normal':\n",
    "                tree_samples_normal.append(len(set(tree_samples)))\n",
    "        fam_samples_overall[fam] = fam_samples_overall[fam] + len(set(fam_sample_all))\n",
    "        if rasmma == 'normal':\n",
    "            fam_processes_normal.append(fam_processes)\n",
    "            fam_samples_normal.append(len(set(fam_sample)))\n",
    "#                 else:\n",
    "#                     paths = glob.glob(root_dir + '*/*/*/'+profile.split('/')[-1])\n",
    "#                     print('duplicate ERR:',paths)\n",
    "\n",
    "assert len(trees_normal) == len(fam_dir_normal) == len(fam_processes_normal) == len(fam_samples_normal)\n",
    "assert sum(trees_normal) == len(tree_samples_normal) == len(tree_processes_normal)\n",
    "# len(processed_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_list = []\n",
    "for i,stat in enumerate([trees_normal,trees_overall,process_lengths_normal,process_lengths_overall\n",
    "                         ,rep_length_normal,rep_length_overall,tree_samples_normal,tree_processes_normal,\n",
    "                        tree_samples_overall,tree_processes_overall,fam_samples_normal,fam_processes_normal,\n",
    "                        fam_samples_overall,fam_processes_overall,sample_processes_normal,sample_processes_overall]):\n",
    "    df_all_list.append(output_xlsx(stat,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outlier removal\n",
    "* 把profile從root_dir移動到mv_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_len_profile = df_all_list[2].loc['outlier'].values[0] #outlier\n",
    "print(np.percentile(process_lengths_normal,82)) #總共涵蓋?比例的profiles ?要自己調到最接近outlier value\n",
    "outlier_len_profile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_dir = './data/tree-rep-profiles_o2o/5.long/' #要先自己創立移動目的地\n",
    "root_dir = './data/tree-rep-profiles_o2o/normal/' #原始要被篩選的資料夾\n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "remove_pids = 0\n",
    "nobyteseq = 0\n",
    "\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        for profile in hl_list:\n",
    "            with open(profile,encoding='ISO 8859-1') as f:\n",
    "                lines = f.read().splitlines()\n",
    "            if len(lines)> outlier_len_profile:\n",
    "                dest_path = mv_dir + fam + '/' + tree + '/'\n",
    "                if not os.path.exists(dest_path):\n",
    "                    os.makedirs(dest_path,exist_ok=True)\n",
    "                shutil.move(profile,dest_path+profile.split('/')[-1])\n",
    "                byteseq = profile.split('.profile')[0] + '_byterep.pickle'\n",
    "                try:\n",
    "                    shutil.move(byteseq,dest_path+byteseq.split('/')[-1])\n",
    "                except FileNotFoundError:\n",
    "                    print(fam,tree)\n",
    "                    nobyteseq +=1\n",
    "                remove_pids +=1\n",
    "print('remove processes:',remove_pids,'No byteseq:',nobyteseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此block要多執行幾次直到把空fam或是tree皆移除 (i.e., Empty Tree#: 0 Empty Fam#: 0)\n",
    "root_dir = './data/tree-rep-profiles_o2o/normal/'\n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "empty_tree = 0\n",
    "empty_fam = 0\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    fam_tree = {}\n",
    "    if len(tree_dir) == 0:\n",
    "        shutil.move(root_dir + fam  ,mv_dir+fam)\n",
    "        empty_fam +=1\n",
    "        print(fam)\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        if len(hl_list) == 0: #移除tree底下的member僅有0個者\n",
    "            shutil.move(root_dir + fam +  '/' + tree ,mv_dir+fam+'/'+tree)\n",
    "            empty_tree +=1\n",
    "print('Empty Tree#:',empty_tree,'Empty Fam#:',empty_fam)\n",
    "\n",
    "\n",
    "all_hash,all_pid_list = statistic_hash_pids(root_dir) # 65.smshoax_0.8 148.dldpk_0.8 114.rbot_0.8 147.ranpax_0.8 107.jorik_0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev, Test split\n",
    "* 拿tree數量sample的10%給valid、10%給test\n",
    "* profile\\*10%不足1的tree跳過"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/tree-rep-profiles_o2o/normal/' #原始p前處理後的rofile資料夾\n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "\n",
    "valid_dir = './data/tree-rep-profiles_o2o/DEV/' #validation 資料夾要先創好\n",
    "test_dir = './data/tree-rep-profiles_o2o/TEST/' #testing 資料夾要先創好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此做法為還原tree，依照process來分\n",
    "\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "#         hash_list = list(set([x.split('/')[-1].split('_')[0] for x in hl_list]))\n",
    "        if len(hl_list)>10:# 從各tree拿一個sample給dev一個給test\n",
    "#             dest_path = valid_dir + fam + '/' + tree + '/'\n",
    "#             if not os.path.exists(dest_path):\n",
    "#                 os.makedirs(dest_path,exist_ok=True)\n",
    "            inf_idx = random.sample(range(0,len(hl_list)),int(len(hl_list)*0.2))\n",
    "            valid_idx = random.sample(inf_idx,int(len(inf_idx)/2))\n",
    "            test_idx = list(set(inf_idx)-set(valid_idx))\n",
    "            valid_paths = [hl_list[x] for x in valid_idx]\n",
    "            test_paths = [hl_list[x] for x in test_idx]\n",
    "#             valid_paths = []\n",
    "#             for valid in valid_pid:\n",
    "#                 paths = glob.glob(root_dir + '*/*/' + valid + '*')\n",
    "#                 valid_paths.extend(paths)\n",
    "#             test_paths = []\n",
    "#             for test in test_pid:\n",
    "#                 paths = glob.glob(root_dir + '*/*/' + test + '*')\n",
    "#                 test_paths.extend(paths)  \n",
    "            for source_path in valid_paths:\n",
    "                dest_path = source_path.replace(root_dir.split('/')[-2],valid_dir.split('/')[-2])\n",
    "                dev_dir = '/'.join(dest_path.split('/')[:-1]) + '/'\n",
    "                if not os.path.exists(dev_dir):\n",
    "                    os.makedirs(dev_dir,exist_ok=True)\n",
    "                shutil.move(source_path,dest_path)\n",
    "                source_path = source_path.replace('.profile','_byterep.pickle')\n",
    "                dest_path = dest_path.replace('.profile','_byterep.pickle')\n",
    "                shutil.move(source_path,dest_path)\n",
    "            for source_path in test_paths:\n",
    "                dest_path = source_path.replace(root_dir.split('/')[-2],test_dir.split('/')[-2])\n",
    "                dev_dir = '/'.join(dest_path.split('/')[:-1]) + '/'\n",
    "                if not os.path.exists(dev_dir):\n",
    "                    os.makedirs(dev_dir,exist_ok=True)\n",
    "                shutil.move(source_path,dest_path)\n",
    "                source_path = source_path.replace('.profile','_byterep.pickle')\n",
    "                dest_path = dest_path.replace('.profile','_byterep.pickle')\n",
    "                shutil.move(source_path,dest_path)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/dev/test所含的family/tree/sample/process數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = statistic_hash_pids(root_dir)\n",
    "temp = statistic_hash_pids(valid_dir)\n",
    "temp = statistic_hash_pids(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每個dataset所含的各家族之tree/sample/process數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "train_dir = './data/tree-rep-profiles_o2o/normal/' #Train Set\n",
    "valid_dir = './data/tree-rep-profiles_o2o/DEV/' # DEV Set\n",
    "test_dir = './data/tree-rep-profiles_o2o/TEST/' # Test Set\n",
    "fam_dir = next(os.walk(train_dir))[1]\n",
    "rows = [x.split('_')[0].split('.')[1] for x in fam_dir]\n",
    "columns = ['train_trees_num','train_samples_num','train_processes_num',\n",
    "          'valid_trees_num','valid_samples_num','valid_processes_num',\n",
    "          'test_trees_num','test_samples_num','test_processes_num']\n",
    "stat_df = pd.DataFrame(0, index=rows,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要iterate多次計算才可以得到三個dataset的總表\n",
    "def stat_fam(root_dir,root_trees_num,root_samples_num,root_processes_num,stat_df=stat_df):\n",
    "    fam_dir = next(os.walk(root_dir))[1]\n",
    "    for fam in tqdm(fam_dir):\n",
    "        tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "        trees_num = len(tree_dir)\n",
    "        fam_name = fam.split('_')[0].split('.')[1]\n",
    "        profiles_num = 0\n",
    "        samples_num_li = []\n",
    "        for tree in tree_dir:\n",
    "            in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            profiles_num = profiles_num + len(hl_list)\n",
    "            hash_list = list(set([x.split('/')[-1].split('_')[0] for x in hl_list]))\n",
    "            samples_num_li.extend(hash_list)\n",
    "#             samples_num = samples_num + len(hash_list)\n",
    "        stat_df.loc[fam_name,root_trees_num] = trees_num\n",
    "        stat_df.loc[fam_name,root_samples_num] = len(set(samples_num_li))\n",
    "        stat_df.loc[fam_name,root_processes_num] = profiles_num\n",
    "    return stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df = stat_fam(train_dir,'train_trees_num','train_samples_num','train_processes_num',stat_df=stat_df)\n",
    "stat_df = stat_fam(valid_dir,'valid_trees_num','valid_samples_num','valid_processes_num',stat_df=stat_df)\n",
    "stat_df = stat_fam(test_dir,'test_trees_num','test_samples_num','test_processes_num',stat_df=stat_df)\n",
    "stat_df.to_excel('./data/tree-rep-profiles_o2o/train_valid_test_fam_stat_correct.xlsx') # 儲存至excel位置 (資料夾要先創好)\n",
    "stat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 準備實驗3要用的Loner Set (thesis sec. 4.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 選取loner有在dev跟test裡面的fam就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/tree-rep-profiles_o2o/DEV/' #要選取的依據家族\n",
    "loner_dir = './data/tree-rep-profiles_o2o_loner/' #原始loner Set資料夾 (這個資料很髒，有很多的empty file或是過短的file可能要自行額外處理)\n",
    "exp_dir = './data/tree-rep-profiles_o2o/EXP2/' #要存放的目的資料夾\n",
    "# outlier_len_profile = 226\n",
    "\n",
    "exp_fam = next(os.walk(root_dir))[1] # 要進行家族歸類的家族\n",
    "exp_fam_dir = [x.split('_')[0] for x in exp_fam]\n",
    "# aliase_fam = [x.split('.')[-1] for x in exp_fam_dir]\n",
    "len(exp_fam_dir),exp_fam_dir #, aliase_fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_profile = []\n",
    "for fam in tqdm(exp_fam_dir):\n",
    "    in_directory =  loner_dir + fam + '/G0/'\n",
    "    profiles = next(os.walk(in_directory))[2]  # 可能某些familiy會沒loner\n",
    "    hl_list = [os.path.join(in_directory, f) for f in profiles]\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "    for profile in hl_list:\n",
    "        with open(profile,encoding='ISO 8859-1') as f:\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines() #Ri\n",
    "        if len(lines) > outlier_len_profile: #太長的放不進去model 需要紀錄?\n",
    "            outlier_profile.append(profile)\n",
    "            continue\n",
    "        profile_name = profile.split('/')[-1]\n",
    "        fam_name = profile.split('/')[3].split('.')[-1]\n",
    "\n",
    "        dest_dir = exp_dir + fam_name + \"/G0/\"\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir,exist_ok=True)\n",
    "        shutil.copy(profile,dest_dir+profile_name)\n",
    "# 所移動到的exp_dir仍很髒，需要再進一步處理才可以拿去做使用，當前的統計資訊不會是對的或是最終採用的\n",
    "# 建議自己這邊先額外寫前處理，篩選出要的profile就好再往0003前進"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除了多少個太長的loner?\n",
    "outlier_profile_hash = [x.split('/')[-1].split('_')[0] for x in outlier_profile]\n",
    "outlier_profile_fam = [x.split('/')[3] for x in outlier_profile]\n",
    "print(len(set(outlier_profile_hash)),len(outlier_profile),len(set(outlier_profile_fam))) #sample#,process#,哪一些家族有發生"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 選擇Sent2Vec演算法要用的n-gram windows size 要多大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dil= r\"[,.;\\-+^()/@#?!&$:{}\\\\*%~\\'\\\"\\=\\_]+\\ *\" \n",
    "root_dir = './data/tree-rep-profiles_o2o/normal/' #訓練資料集資料夾/families/trees/profiles\n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "api_length = []\n",
    "profile_length = []\n",
    "\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        for profile in hl_list:\n",
    "            with open(profile,encoding='ISO 8859-1') as f: #X2\n",
    "                lines = f.read()\n",
    "            lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "            lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "            lines = re.sub('=\\\\n','',lines)\n",
    "            lines = re.sub('y\\\\n','',lines)\n",
    "            lines = lines.splitlines()\n",
    "            for line in lines:\n",
    "                temp = re.sub(dil,\" \",line.lower())\n",
    "                temp = temp.split(\" \")\n",
    "                temp = list(filter(None, temp))\n",
    "                api_length.append(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_statistics(api_length) #選取眾數(mode)做為Sent2Vec演算法的超參數 n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix Spplementary\n",
    "* 與任務主線無關，只是實驗用的小工具\n",
    "* Deprecated code. Need lots of modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主線是依據tree的process來分，以下是以sample來分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此做法為希望系統可以還原family，而非評估individual tree rep，依照sample來分並且會拿process\n",
    "'''for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        hash_list = list(set([x.split('/')[-1].split('_')[0] for x in hl_list]))\n",
    "        if len(hash_list)>10:# 從各tree拿一個sample給dev一個給test\n",
    "#             dest_path = valid_dir + fam + '/' + tree + '/'\n",
    "#             if not os.path.exists(dest_path):\n",
    "#                 os.makedirs(dest_path,exist_ok=True)\n",
    "            inf_idx = random.sample(range(0,len(hash_list)),int(len(hash_list)*0.2))\n",
    "            valid_idx = random.sample(inf_idx,int(len(inf_idx)/2))\n",
    "            test_idx = list(set(inf_idx)-set(valid_idx))\n",
    "            valid_hash = [hash_list[x] for x in valid_idx]\n",
    "            test_hash = [hash_list[x] for x in test_idx]\n",
    "            valid_paths = []\n",
    "            for valid in valid_hash:\n",
    "                paths = glob.glob(root_dir + '*/*/' + valid + '*')\n",
    "                valid_paths.extend(paths)\n",
    "            test_paths = []\n",
    "            for test in test_hash:\n",
    "                paths = glob.glob(root_dir + '*/*/' + test + '*')\n",
    "                test_paths.extend(paths)  \n",
    "            for source_path in valid_paths:\n",
    "                dest_path = source_path.replace(root_dir.split('/')[-2],valid_dir.split('/')[-2])\n",
    "                dev_dir = '/'.join(dest_path.split('/')[:-1]) + '/'\n",
    "                if not os.path.exists(dev_dir):\n",
    "                    os.makedirs(dev_dir,exist_ok=True)\n",
    "                shutil.move(source_path,dest_path)\n",
    "            for source_path in test_paths:\n",
    "                dest_path = source_path.replace(root_dir.split('/')[-2],test_dir.split('/')[-2])\n",
    "                dev_dir = '/'.join(dest_path.split('/')[:-1]) + '/'\n",
    "                if not os.path.exists(dev_dir):\n",
    "                    os.makedirs(dev_dir,exist_ok=True)\n",
    "                shutil.move(source_path,dest_path)\n",
    "            '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loner Set 家族名稱別名分析與合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/FamilyName_aliases.csv') # 要先自己創好，範例放在google drive的table裡面FamilyName_aliases\n",
    "df = df.set_index('Unnamed: 0')\n",
    "name_df_aliases = df[aliase_fam]\n",
    "name_df_aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aliases\n",
    "print('sklearn version need >0.21 now:',sklearn.__version__)\n",
    "aliases_li = []\n",
    "all_process_fam = []\n",
    "for a,b in it.combinations(name_df_aliases.columns.tolist(),2): \n",
    "    score = jaccard_score(name_df_aliases.loc[:,a].values , name_df_aliases.loc[:,b].values)\n",
    "    if score > 0.6: #可改相似度thr\n",
    "        print(a,b,score)\n",
    "        if (a in all_process_fam) and (b not in all_process_fam):\n",
    "            for aliases in aliases_li:\n",
    "                if a in aliases:\n",
    "                    aliases_li.remove(aliases)\n",
    "                    aliases.append(b)\n",
    "                    aliases_li.append(aliases)\n",
    "                    all_process_fam.append(b)\n",
    "        elif (b in all_process_fam) and (b not in all_process_fam):\n",
    "            for aliases in aliases_li:\n",
    "                if b in aliases:\n",
    "                    aliases_li.remove(aliases)\n",
    "                    aliases.append(a)\n",
    "                    aliases_li.append(aliases)\n",
    "                    all_process_fam.append(a)\n",
    "        elif (a in all_process_fam) and (b in all_process_fam):\n",
    "            pass\n",
    "        else:\n",
    "            aliases_li.append([a,b])\n",
    "            all_process_fam.append(b)\n",
    "            all_process_fam.append(a)\n",
    "# ['allaple','rahack']\n",
    "# []\n",
    "aliases_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_profile = []\n",
    "for fam in tqdm(exp_fam_dir):\n",
    "    in_directory =  loner_dir + fam + '/G0/'\n",
    "    profiles = next(os.walk(in_directory))[2]  # 可能某些familiy會沒loner\n",
    "    hl_list = [os.path.join(in_directory, f) for f in profiles]\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "    for profile in hl_list:\n",
    "        with open(profile,encoding='ISO 8859-1') as f:\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines() #Ri\n",
    "        if len(lines) > outlier_len_profile: #太長的放不進去model 需要紀錄?\n",
    "            outlier_profile.append(profile)\n",
    "            continue\n",
    "        profile_name = profile.split('/')[-1]\n",
    "        fam_name = profile.split('/')[3].split('.')[-1]\n",
    "        for aliases in aliases_li:\n",
    "            if fam_name in aliases:\n",
    "                fam_name = aliases[0]+'_'+aliases[1] #aliases analyze and combine\n",
    "\n",
    "        dest_dir = exp_dir + fam_name + \"/G0/\"\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir,exist_ok=True)\n",
    "        shutil.copy(profile,dest_dir+profile_name)\n",
    "temp = statistic_hash_pids(exp_dir) #真正拿來做實驗的loners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loners similarity computation\n",
    "* 取出實驗家族exp_fam所含有的profiles\n",
    "* 利用RasMMA計算是否是跟自己家族的行為樹最相近\n",
    "* 如果是就把它移到exp_dir\n",
    "* Deprecation原因(需要修改以下程式碼來達到下兩點目的):\n",
    "    * 不應該profile跟rep比\n",
    "    * 應該要profile跟rep(tree)的memeber比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/tree-rep-profiles_o2o/DEV/'\n",
    "loner_dir = './data/tree-rep-profiles_o2o_loner/'\n",
    "exp_dir = './data/tree-rep-profiles_o2o/EXP_rasmma/'\n",
    "\n",
    "exp_fam = next(os.walk(root_dir))[1] #改align root_dir? 要進行家族歸類的家族\n",
    "exp_fam_dir = [x.split('_')[0] for x in exp_fam]\n",
    "len(exp_fam_dir),exp_fam_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RasMMA similarity function\n",
    "* 需要有偉志撰寫經我修改後的相關程式碼檔案(放在github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Alignment_Fast3.ipynb\n",
    "\n",
    "# Doing global alignment and find commonAPISequence.\n",
    "def do_globalAlignment(rep1, rep2):\n",
    "    '''\n",
    "    Input: 兩個1D list (profile(rep1) , rep(rep2))\n",
    "    Output: 一個1D list (common)\n",
    "    '''\n",
    "    # Aligment\n",
    "    commonAPISequence = []\n",
    "    alignment_result = globalAlign( rep1, rep2, score_matched=10, score_mismatched=-1, score_gap=0)[2]\n",
    "    common_motif_sequence = motif_delimit(alignment_result)\n",
    "    return common_motif_sequence\n",
    "\n",
    "# input: two R\n",
    "# output: new Rep's common motif sequence of input CMS;\n",
    "def get_Rep_CommMotifSeq(Ri, Rj):\n",
    "    '''\n",
    "    Input: 兩個1D list (profile(Ri) , rep(Rj))\n",
    "    Output: common motif 1D list\n",
    "    '''\n",
    "    rep1 = Ri\n",
    "    rep2 = Rj\n",
    "    repNew_CMS = []\n",
    "#     print(rep2)\n",
    "    if(rep1 and rep2):\n",
    "#         print(\"===in rep1 and rep2===\")\n",
    "        commonSequence = do_globalAlignment(rep1, rep2) # do Alignment\n",
    "#         print(commonSequence)\n",
    "        repNew = commonSequence\n",
    "    else: ##ADD\n",
    "        repNew = [] #ADD\n",
    "#     print('===out===')\n",
    "    return repNew\n",
    "\n",
    "# compute score of Rnew\n",
    "# the score calculate method is the length ratio of new to origin one\n",
    "def compute_Score(Ri, Rj, Rnew):\n",
    "    '''\n",
    "    Input: 3個 1D list (Profile(Ri), Rep(Rj), Common(Rnew))\n",
    "    Output: 相似度分數\n",
    "    '''\n",
    "    if(Rnew):\n",
    "        repI = Ri\n",
    "        repJ = Rj\n",
    "        repNew = Rnew\n",
    "        L_Ri = len(repI)\n",
    "        L_Rj = len(repJ)\n",
    "        Lorg = max(L_Ri, L_Rj)\n",
    "        Lnew = len(repNew)\n",
    "        return float(Lnew)/Lorg\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_accept_loner = 0 # 1 hr\n",
    "accept_loner_paths = []\n",
    "max_scores = []\n",
    "\n",
    "for fam in tqdm(exp_fam_dir):\n",
    "    in_directory = loner_dir+ fam+'/' + 'G0' + '/'\n",
    "    profiles = next(os.walk(in_directory))[2]  # 可能某些familiy會沒loner\n",
    "    hl_list = [os.path.join(in_directory, f) for f in profiles]\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "    for profile in hl_list:\n",
    "        with open(profile,encoding='ISO 8859-1') as f:\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines() #Ri\n",
    "        if len(lines) > outlier_len_profile: #太長的放不進去model\n",
    "            continue\n",
    "        ori_fam = fam+'_0.8'\n",
    "        ori_rep_trees = next(os.walk(root_dir + ori_fam + '/'))[1] # 原所屬家族tree的rep找出來\n",
    "        ori_scores = []\n",
    "        for tree in ori_rep_trees:\n",
    "            rep_directory = root_dir + ori_fam + '/' + tree + '/' #+ 'rep.pickle'\n",
    "#             rep = pickle.load(open(rep_directory,'rb'))\n",
    "#             rep = sum(rep,[])\n",
    "            root_dir_tree_profiles = next(os.walk(rep_directory))[2]\n",
    "            hl_list_ori = [os.path.join(root_dir_tree_profiles, f) for f in profiles]\n",
    "            hl_list_ori = list(filter(lambda f: f.endswith(\".profile\"), hl_list_ori))\n",
    "            for profile_ori in hl_list_ori:\n",
    "                with open(profile_ori,encoding='ISO 8859-1') as f:\n",
    "                    lines_ori = f.read()\n",
    "                lines_ori = re.sub(r'[^\\x00-\\x7F]+','', lines_ori)\n",
    "                lines_ori = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines_ori)\n",
    "                lines_ori = re.sub('=\\\\n','',lines_ori)\n",
    "                lines_ori = re.sub('y\\\\n','',lines_ori)\n",
    "                lines_ori = lines_ori.splitlines()                \n",
    "                common = get_Rep_CommMotifSeq(lines,lines_ori)\n",
    "                score = compute_Score(lines,lines_ori,common)\n",
    "            ori_scores.append(score)\n",
    "        ori_max_score = max(ori_scores)\n",
    "        flag = True\n",
    "        rest_dir = list(set(exp_fam) - set([ori_fam]))\n",
    "        for rep_dir in rest_dir:\n",
    "            rep_trees = next(os.walk(root_dir + rep_dir + '/'))[1]\n",
    "            for tree in rep_trees:\n",
    "                rep_directory = root_dir + rep_dir + '/' + tree + '/' + 'rep.pickle'\n",
    "                rep = pickle.load(open(rep_directory,'rb'))\n",
    "                rep = sum(rep,[])\n",
    "                common = get_Rep_CommMotifSeq(lines,rep)\n",
    "                score = compute_Score(lines,rep,common)\n",
    "                if score > ori_max_score: #loner跟別的家族tree分數比較高\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag == False:\n",
    "                break\n",
    "        if (flag == True) :#and (score>0):\n",
    "            accept_loner_paths.append(profile)\n",
    "            max_scores.append(ori_max_score)\n",
    "            count_accept_loner+=1\n",
    "assert len(accept_loner_paths) == count_accept_loner == len(max_scores)\n",
    "print(count_accept_loner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lon = []\n",
    "final_hash_lon = []\n",
    "final_fam_lon = []\n",
    "final_score = []\n",
    "for lon,score in zip(accept_loner_paths,max_scores):\n",
    "    hash_lon = lon.split('/')[-1].split('_')[0]\n",
    "    fam_name = lon.split('/')[3]\n",
    "    if score>0:\n",
    "        final_lon.append(lon)\n",
    "        final_hash_lon.append(hash_lon)\n",
    "        final_fam_lon.append(fam_name)\n",
    "        final_score.append(score)\n",
    "final_hash_lon = list(set(final_hash_lon))\n",
    "assert len(final_hash_lon) <= len(final_lon)\n",
    "print('Processes#:',len(final_lon),'Hash#:',len(final_hash_lon))\n",
    "final_fam_lon = dict(Counter(final_fam_lon)) #loner\n",
    "len(final_fam_lon) #loner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(final_score) , max(final_score) ,np.mean(final_score) , np.std(final_score) , np.percentile(final_score,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把最後loner跟原本自己家族夠相近的profile copy to root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lon in tqdm(final_lon):\n",
    "    dest_dir = exp_dir + lon.split('/')[3] +'/'+ 'G0/'\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir,exist_ok=True)\n",
    "    shutil.copy(lon,dest_dir+lon.split('/')[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
