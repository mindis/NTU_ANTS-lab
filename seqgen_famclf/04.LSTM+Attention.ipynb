{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,shutil,pickle,tqdm,sys,random,re,string,pause, datetime,glob\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "# from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "\n",
    "# from collections import Counter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import unicodedata as udata\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "from keras import backend \n",
    "from imblearn.ensemble import *\n",
    "from imblearn.combine import *\n",
    "# from python.keras import backend \n",
    "# Embedding(10,20)\n",
    "from keras_transformer.extras import ReusableEmbedding, TiedOutputEmbedding\n",
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "from keras_transformer.bert import (\n",
    "    BatchGeneratorForBERT, masked_perplexity,\n",
    "    MaskedPenalizedSparseCategoricalCrossentropy)\n",
    "\n",
    "import keras_metrics as km\n",
    "from keras_trans_mask import RemoveMask, RestoreMask\n",
    "\n",
    "from keras_multi_head import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import transformer_bert_model\n",
    "from bpe import BPEEncoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test of sent2vec vector: (424, 213, 768) (424, 213) (424, 44) (424, 213, 1)\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_emb_api, train_fam_ans, train_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/TRAIN_vec.pkl','rb'))\n",
    "valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/DEV_vec.pkl','rb'))\n",
    "test_emb, test_emb_api,test_fam_ans,test_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/TEST_vec.pkl','rb'))\n",
    "# print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "# print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "train_rep_ans = np.expand_dims(train_rep_ans,axis=-1)\n",
    "valid_rep_ans = np.expand_dims(valid_rep_ans,axis=-1)\n",
    "test_rep_ans = np.expand_dims(test_rep_ans,axis=-1)\n",
    "print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_fam_ans.shape,test_rep_ans.shape)\n",
    "emb_matrix = pickle.load(open('data/tree-rep-profiles-partial/api_emb_matrix.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, X2 ,X3,X4):\n",
    "#     X3 = np.take(train_fam_ans,[0],axis=-1) #只train第幾個familiy\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], X2[randomize],X3[randomize],X4[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train of sent2vec vector: (11141, 213, 768) (11141, 213) (11141, 44) (11141, 213, 1)\n",
      "valid of sent2vec vector: (437, 213, 768) (437, 213) (437, 44) (437, 213, 1)\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_emb_api, train_fam_ans, train_rep_ans = _shuffle(train_emb, train_emb_api, train_fam_ans, train_rep_ans)\n",
    "valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans = _shuffle(valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans)\n",
    "\n",
    "# test_emb, test_emb_api,test_fam_ans,test_rep_ans  = _shuffle(test_emb,test_emb_api,test_fam_ans,test_rep_ans)\n",
    "\n",
    "print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "\n",
    "# print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_fam_ans.shape,test_rep_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale = 'no'\n",
    "\n",
    "# dim-wise scaling\n",
    "def scaling(trainX,validX,testX,scale='min_max'):\n",
    "#     if scale == 'min_max':\n",
    "    max_value = max([np.max(trainX) , np.max(validX),np.max(testX)])\n",
    "    min_value = min([np.min(trainX),np.min(validX),np.min(testX)])\n",
    "\n",
    "    trainX = (trainX - min_value) / (max_value - min_value)\n",
    "    validX = (validX - min_value) / (max_value - min_value )\n",
    "    testX = (testX - min_value) / (max_value - min_value )\n",
    "    print(np.max(trainX),np.max(validX))\n",
    "    return trainX,validX,testX , max_value , min_value\n",
    "def scaling(trainX,validX,testX,scale='mean_dim',):\n",
    "#     if scale == 'min_max':\n",
    "    alls = np.concatenate((trainX,validX,testX),axis=0)\n",
    "    mean = np.mean(alls,axis=-1)\n",
    "    mean = np.mean(mean,axis=0)\n",
    "    mean = np.expand_dims(mean,axis=-1)\n",
    "    mean = np.repeat(mean,trainX.shape[2],axis=-1)\n",
    "    mean = np.expand_dims(mean,axis=0)\n",
    "    mean_train = np.repeat(mean,trainX.shape[0],axis=0)\n",
    "    mean_valid = np.repeat(mean,validX.shape[0],axis=0)\n",
    "    mean_test = np.repeat(mean,testX.shape[0],axis=0)\n",
    "    std = np.std(alls,axis=-1)\n",
    "    std = np.std(std,axis=0)\n",
    "    std = np.expand_dims(std,axis=-1)\n",
    "    std = np.repeat(std,validX.shape[2],axis=-1)\n",
    "    std_train = np.repeat(std,trainX.shape[0],axis=0)\n",
    "    std_valid = np.repeat(std,validX.shape[0],axis=0)\n",
    "    std_test = np.repeat(std,testX.shape[0],axis=0)\n",
    "#     min_value = min([np.min(trainX),np.min(validX),np.min(testX)])\n",
    "\n",
    "    trainX = (trainX - mean) / (std + 1e-10)\n",
    "    validX = (validX - mean) / (std + 1e-10)\n",
    "    testX = (testX - mean) / (std + 1e-10)\n",
    "#     print(np.max(trainX),np.max(validX))\n",
    "    return trainX,validX,testX , mean , std\n",
    "\n",
    "# train_emb,valid_emb,test_emb , max_value,min_value = scaling(train_emb,valid_emb,test_emb)   \n",
    "# print(valid_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33855778, -0.32893312, -0.72717506, ...,  0.91245544,\n",
       "         1.18298519, -0.31610405],\n",
       "       [-0.48375982, -0.76325315, -1.20088184, ...,  0.87430948,\n",
       "         1.2489109 , -0.08723355],\n",
       "       [-0.22453758, -0.52563572, -1.29923677, ...,  0.74041533,\n",
       "         1.04407859, -0.49547631],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kk = np.mean(train_emb,axis=-1)\n",
    "# kk = np.mean(kk,axis=0)\n",
    "# kk = np.expand_dims(kk,axis=0)\n",
    "# kk = np.repeat(kk,100,axis=0)\n",
    "# kk = np.expand_dims(kk,axis=-1)\n",
    "# kk = np.repeat(kk,768,axis=-1)\n",
    "# kk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kkk = (emb_matrix - kk)/kk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kk = np.std(emb_matrix,axis=-1)\n",
    "# kk = np.expand_dims(kk,axis=-1)\n",
    "# kk = np.repeat(kk,768,axis=-1)\n",
    "# kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc = SMOTEENN()\n",
    "# N,t,d = train_emb.shape\n",
    "# train_emb_ = train_emb.reshape(N,t*d)\n",
    "# train_fam_ans_ = train_fam_ans.reshape(N,)\n",
    "# train_emb_ , train_fam_ans_  = bc.fit_resample(train_emb_, train_fam_ans_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emb = train_emb_.reshape(-1,t,d)\n",
    "# train_fam_ans = train_fam_ans_.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4.963642428285524,\n",
       " 1: 3.4256445672191527,\n",
       " 2: 11.222473604826547,\n",
       " 3: 17.163783160322954,\n",
       " 4: 50.6156462585034,\n",
       " 5: 63.59401709401709,\n",
       " 6: 49.60333333333333,\n",
       " 7: 105.53900709219859,\n",
       " 8: 56.36742424242424,\n",
       " 9: 54.70955882352941,\n",
       " 10: 148.81,\n",
       " 11: 73.66831683168317,\n",
       " 12: 73.66831683168317,\n",
       " 13: 256.5689655172414,\n",
       " 14: 195.80263157894737,\n",
       " 15: 25.39419795221843,\n",
       " 16: 201.09459459459458,\n",
       " 17: 212.58571428571432,\n",
       " 18: 153.41237113402062,\n",
       " 19: 228.93846153846152,\n",
       " 20: 195.80263157894737,\n",
       " 21: 144.47572815533982,\n",
       " 22: 33.7437641723356,\n",
       " 23: 215.66666666666669,\n",
       " 24: 165.34444444444443,\n",
       " 25: 130.53508771929825,\n",
       " 26: 165.34444444444443,\n",
       " 27: 338.20454545454544,\n",
       " 28: 222.1044776119403,\n",
       " 29: 256.5689655172414,\n",
       " 30: 64.98253275109171,\n",
       " 31: 354.3095238095238,\n",
       " 32: 190.7820512820513,\n",
       " 33: 163.52747252747253,\n",
       " 34: 98.54966887417218,\n",
       " 35: 94.78343949044586,\n",
       " 36: 362.9512195121951,\n",
       " 37: 150.31313131313132,\n",
       " 38: 744.05,\n",
       " 39: 620.0416666666666,\n",
       " 40: 391.60526315789474,\n",
       " 41: 316.6170212765958,\n",
       " 42: 496.03333333333336,\n",
       " 43: 132.86607142857142}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = sum(train_fam_ans) / sum(sum(train_fam_ans))\n",
    "fam_weights={}\n",
    "for i in range(len(class_weights)):\n",
    "    fam_weights[i] = 1/class_weights[i]\n",
    "fam_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2998,\n",
       " 1: 4344,\n",
       " 2: 1326,\n",
       " 3: 867,\n",
       " 4: 294,\n",
       " 5: 234,\n",
       " 6: 300,\n",
       " 7: 141,\n",
       " 8: 264,\n",
       " 9: 272,\n",
       " 10: 100,\n",
       " 11: 202,\n",
       " 12: 202,\n",
       " 13: 58,\n",
       " 14: 76,\n",
       " 15: 586,\n",
       " 16: 74,\n",
       " 17: 70,\n",
       " 18: 97,\n",
       " 19: 65,\n",
       " 20: 76,\n",
       " 21: 103,\n",
       " 22: 441,\n",
       " 23: 69,\n",
       " 24: 90,\n",
       " 25: 114,\n",
       " 26: 90,\n",
       " 27: 44,\n",
       " 28: 67,\n",
       " 29: 58,\n",
       " 30: 229,\n",
       " 31: 42,\n",
       " 32: 78,\n",
       " 33: 91,\n",
       " 34: 151,\n",
       " 35: 157,\n",
       " 36: 41,\n",
       " 37: 99,\n",
       " 38: 20,\n",
       " 39: 24,\n",
       " 40: 38,\n",
       " 41: 47,\n",
       " 42: 30,\n",
       " 43: 112}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fam = sum(train_fam_ans)\n",
    "for i in range(len(all_fam)):\n",
    "    fam_weights[i] = all_fam[i]\n",
    "fam_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.3664174982464374,\n",
       " 1: 1.0,\n",
       " 2: 2.1821960061631724,\n",
       " 3: 2.6070792001284384,\n",
       " 4: 3.688538409570299,\n",
       " 5: 3.916797061551279,\n",
       " 6: 3.6683357022527794,\n",
       " 7: 4.423358286530812,\n",
       " 8: 3.7961690737626643,\n",
       " 9: 3.766316110612983,\n",
       " 10: 4.766947990920889,\n",
       " 11: 4.063850479507775,\n",
       " 12: 4.063850479507775,\n",
       " 13: 5.311675166362561,\n",
       " 14: 5.041384836622649,\n",
       " 15: 2.9987983873319677,\n",
       " 16: 5.068053083704811,\n",
       " 17: 5.123622934859622,\n",
       " 18: 4.7974071984055975,\n",
       " 19: 5.1977309070133435,\n",
       " 20: 5.041384836622649,\n",
       " 21: 4.737389188679344,\n",
       " 22: 3.2830733014621343,\n",
       " 23: 5.138011672311721,\n",
       " 24: 4.872308506578715,\n",
       " 25: 4.635919728514485,\n",
       " 26: 4.872308506578715,\n",
       " 27: 5.587928542990719,\n",
       " 28: 5.167425557518015,\n",
       " 29: 5.311675166362561,\n",
       " 30: 3.938396173354741,\n",
       " 31: 5.634448558625612,\n",
       " 32: 5.015409350219389,\n",
       " 33: 4.8612586703921306,\n",
       " 34: 4.354838340094056,\n",
       " 35: 4.315872371560673,\n",
       " 36: 5.658546110204672,\n",
       " 37: 4.7769983267743905,\n",
       " 38: 6.376385903354989,\n",
       " 39: 6.194064346561035,\n",
       " 40: 5.734532017182595,\n",
       " 41: 5.521970575198922,\n",
       " 42: 5.970920795246825,\n",
       " 43: 4.653619305613886}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def create_class_weight(labels_dict,mu=0.79):\n",
    "    total = np.sum(np.array(list(labels_dict.values())))\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "\n",
    "    return class_weight\n",
    "fam_weights = create_class_weight(fam_weights)\n",
    "fam_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# temp = list(fam_weights.values())\n",
    "# max_value = np.max(temp)\n",
    "# for i in range(len(fam_weights)):\n",
    "#     fam_weights[i] = fam_weights[i]/max_value\n",
    "# fam_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 768 #被除數\n",
    "num_heads = 1#除數，要整除\n",
    "max_length = 213 # max sequence length\n",
    "fam_num = train_fam_ans.shape[1]\n",
    "vocabulary_size = 26\n",
    "transformer_depth = 1\n",
    "transformer_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "constrain = keras.constraints.MinMaxNorm(min_value=0.0, max_value=0.0, rate=1.0, axis=0)\n",
    "init = keras.initializers.Ones()\n",
    "coordinate_embedding_layer = TransformerCoordinateEmbedding(\n",
    "        transformer_depth , name='coordinate_embedding')\n",
    "act_layer = TransformerACT(\n",
    "            name='adaptive_computation_time')\n",
    "\n",
    "transformer_block = TransformerBlock(\n",
    "            name='transformer', num_heads=num_heads,\n",
    "            residual_dropout=transformer_dropout,\n",
    "            attention_dropout=transformer_dropout,\n",
    "            # Allow bi-directional attention\n",
    "            use_masking=False)\n",
    "add_segment_layer = Add(name='add_segment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\n",
    "sentemb = Masking(mask_value=0)(sentemb1)\n",
    "sent_ids1 = Input(shape=(max_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\n",
    "sent_ids = Masking(mask_value=0)(sent_ids1)\n",
    "api_emb = Embedding(vocabulary_size+1, emb_dim,weights=[emb_matrix],input_length=max_length\n",
    "                    ,trainable=True,name='api_emb')(sent_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "next_step_input = sentemb\n",
    "act_output = sentemb\n",
    "segment_embeddings = api_emb\n",
    "\n",
    "for i in range(transformer_depth):\n",
    "            next_step_input1 = RemoveMask()(next_step_input)\n",
    "            next_step_input1 = coordinate_embedding_layer(\n",
    "                next_step_input1, step=i,trainable=False)\n",
    "            next_step_input = RestoreMask()([next_step_input1,next_step_input])\n",
    "            next_step_input1 = add_segment_layer(\n",
    "                [next_step_input, segment_embeddings])\n",
    "            next_step_input = RemoveMask()(next_step_input1)\n",
    "            next_step_input = transformer_block(next_step_input)\n",
    "#             next_step_input = RestoreMask()([next_step_input,next_step_input1])\n",
    "            next_step_input, act_output = act_layer(next_step_input,trainable=False)\n",
    "            next_step_input = RestoreMask()([next_step_input,next_step_input1])\n",
    "            act_output = RestoreMask()([act_output,next_step_input1])\n",
    "\n",
    "act_layer.finalize()\n",
    "next_step_input = act_output\n",
    "# next_step_input = RestoreMask()([next_step_input,act_output])\n",
    "next_step_input = BatchNormalization()(next_step_input)\n",
    "att_in = Dense(32,kernel_initializer=keras.initializers.lecun_normal(),activation='selu',\n",
    "               name='attention_in',trainable=False)(next_step_input)\n",
    "rep_prediction = (\n",
    "        Dense(1, name='0_1_prediction', activation='sigmoid')\n",
    "    (att_in))\n",
    "# rep = Lambda(lambda x: keras.backend.round(x),name='round')(rep_prediction)\n",
    "final_emb = Add()([sentemb,api_emb])\n",
    "mul = multiply([final_emb,rep_prediction],name='mul')\n",
    "mul = BatchNormalization()(mul)\n",
    "# dense = Dense(4,kernel_initializer=keras.initializers.lecun_normal(),activation='selu')\n",
    "multi = MultiHead(GRU(16),layer_num=44,name='multi')(mul)\n",
    "flat = Flatten(name='Flatten')(multi)\n",
    "dense = BatchNormalization()(flat)\n",
    "dense = Dense(176,kernel_initializer=keras.initializers.lecun_normal(),activation='selu')(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "out = Dense(44,activation='sigmoid', kernel_initializer='lecun_normal',name='family')(dense)\n",
    "\n",
    "# timesteps,state_h,state_c = LSTM(int(emb_dim/16),return_sequences=True,return_state=True,name='extract'\n",
    "#                                 , kernel_initializer='lecun_normal',trainable=False)(final_emb) #final_emb\n",
    "# state = Concatenate()([state_h,state_c])\n",
    "# fc = Dense(8,kernel_initializer=keras.initializers.lecun_normal(),name='attention_in')(state)\n",
    "# fc = BatchNormalization()(fc)\n",
    "# fc = Activation('selu')(fc)\n",
    "# fc = Lambda(lambda x: keras.backend.expand_dims(x,axis=-2),name='expand')(fc)\n",
    "# fc = Lambda(lambda x: keras.backend.repeat_elements(x,max_length,axis=-2),name='re')(fc)\n",
    "# timesteps = Concatenate()([timesteps,fc])\n",
    "# rep = Dense(1,activation=hard_sigmoid,name='rep',kernel_initializer='lecun_normal')(timesteps)\n",
    "\n",
    "# fc = Dense(int(emb_dim/2),kernel_initializer=keras.initializers.lecun_normal(),name='attention_in')(state) #multiply_1\n",
    "# fc = BatchNormalization()(fc)\n",
    "# fc = Activation('selu')(fc)\n",
    "# fc = Dense(max_length,activation=hard_sigmoid,name='attention_out')(fc)\n",
    "# fc = Lambda(lambda x: keras.backend.expand_dims(x,axis=-1),name='expand')(fc)\n",
    "# fc = Lambda(lambda x: keras.backend.repeat_elements(x,int(emb_dim/4),axis=-1),name='re')(fc)\n",
    "\n",
    "# fc = keras.backend.repeat_elements(fc,256,axis=-1)\n",
    "# fc = keras.backend.expand_dims(fc,axis=-1)\n",
    "\n",
    "\n",
    "# mul = multiply([final_emb,rep],name='mul')\n",
    "# # mul = BatchNormalization()(mul)\n",
    "# alls = []\n",
    "# gru = GRU(int(emb_dim/2), kernel_initializer='lecun_normal',dropout=0.2,recurrent_dropout=0.2,name='fam_input')(mul) #/8\n",
    "# dense1 = Dense(int(emb_dim/4),kernel_initializer=keras.initializers.lecun_normal(),name='fc1')(gru)\n",
    "# # gru = GRU(1)\n",
    "# bn = BatchNormalization()(dense1)\n",
    "# af = Activation('selu')(bn)\n",
    "# dp = Dropout(0.2)(af)\n",
    "\n",
    "# dense1 = Dense(int(emb_dim/8),kernel_initializer=keras.initializers.lecun_normal(),name='fc2')(dp)\n",
    "# bn = BatchNormalization()(dense1)\n",
    "# af = Activation('selu')(bn)\n",
    "# dp = Dropout(0.2)(af)\n",
    "\n",
    "\n",
    "# dense1 = Dense(int(emb_dim/16),kernel_initializer=keras.initializers.lecun_normal())\n",
    "# bn = BatchNormalization()\n",
    "# af = Activation('selu')\n",
    "# dense = Dense(1,activation='sigmoid')\n",
    "# for i in range(fam_num):\n",
    "# #     alls.append(dense(bn(gru(mul))))\n",
    "#     alls.append(dense(af(bn(dense1(gru)))))\n",
    "# #     alls.append(gru(mul))\n",
    "# out = Concatenate(name='family')(alls)\n",
    "\n",
    "# out = Dense(128,activation='selu',kernel_initializer=keras.initializers.lecun_normal())(out)\n",
    "\n",
    "\n",
    "# out = Dense(44,activation='sigmoid', kernel_initializer='lecun_normal',name='family')(dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_emb (InputLayer)           (None, 213, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 213, 768)     0           sent_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sent_ids (InputLayer)           (None, 213)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "remove_mask_1 (RemoveMask)      (None, 213, 768)     0           masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 213)          0           sent_ids[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "coordinate_embedding (Transform (None, 213, 768)     164352      remove_mask_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "api_emb (Embedding)             (None, 213, 768)     20736       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "restore_mask_1 (RestoreMask)    (None, 213, 768)     0           coordinate_embedding[0][0]       \n",
      "                                                                 masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_segment (Add)               (None, 213, 768)     0           restore_mask_1[0][0]             \n",
      "                                                                 api_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "remove_mask_2 (RemoveMask)      (None, 213, 768)     0           add_segment[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "transformer_self_attention (Mul (None, 213, 768)     2359296     remove_mask_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "transformer_add (Add)           (None, 213, 768)     0           remove_mask_2[0][0]              \n",
      "                                                                 transformer_self_attention[0][0] \n",
      "                                                                 transformer_normalization1[0][0] \n",
      "                                                                 transformer_transition[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "transformer_dropout (Dropout)   (None, 213, 768)     0           transformer_add[0][0]            \n",
      "                                                                 transformer_add[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_normalization1 (Lay (None, 213, 768)     1536        transformer_dropout[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "transformer_transition (Transfo (None, 213, 768)     4722432     transformer_normalization1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "transformer_normalization2 (Lay (None, 213, 768)     1536        transformer_dropout[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adaptive_computation_time (Tran [(None, 213, 768), ( 769         transformer_normalization2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "restore_mask_3 (RestoreMask)    (None, 213, 768)     0           adaptive_computation_time[0][1]  \n",
      "                                                                 add_segment[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 213, 768)     3072        restore_mask_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_in (Dense)            (None, 213, 32)      24608       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 213, 768)     0           masking_1[0][0]                  \n",
      "                                                                 api_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "0_1_prediction (Dense)          (None, 213, 1)       33          attention_in[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mul (Multiply)                  (None, 213, 768)     0           add_1[0][0]                      \n",
      "                                                                 0_1_prediction[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 213, 768)     3072        mul[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "multi (MultiHead)               (None, 16, 44)       1657920     batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Flatten (Flatten)               (None, 704)          0           multi[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 704)          2816        Flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 176)          124080      batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 176)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "family (Dense)                  (None, 44)           7788        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,094,046\n",
      "Trainable params: 9,064,958\n",
      "Non-trainable params: 29,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[sent_ids1,sentemb1], outputs=[out]) #out\n",
    "model.load_weights('./model/att_clf/2ndStage_44fam_0611.h5',by_name=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = multi_gpu_model(model , gpus=3)\n",
    "\n",
    "# model.load_weights('./model/LSTM_att/1stStage_44fam_0607.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nsentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\\nsentemb = Masking(mask_value=0)(sentemb1)\\nsent_ids1 = Input(shape=(max_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\\nsent_ids = Masking(mask_value=0)(sent_ids1)\\napi_emb = Embedding(vocabulary_size+1, emb_dim,weights=[emb_matrix],input_length=max_length,trainable=True,name='api_emb')(sent_ids)\\n\\nfinal_emb = Add()([sentemb,api_emb])\\n\\ntimesteps,state_h,state_c = LSTM(int(emb_dim/2),return_sequences=True,return_state=True,name='lstm1')(final_emb) #final_emb\\nstate = Concatenate()([state_h,state_c])\\nfc = Dense(max_length,activation='sigmoid',bias_constraint=None,kernel_initializer=init,name='attention')(state)\\nfc = Lambda(lambda x: keras.backend.expand_dims(x,axis=-1),name='RasMMA')(fc)\\nfc = Lambda(lambda x: keras.backend.repeat_elements(x,int(emb_dim/2),axis=-1))(fc)\\n# fc = keras.backend.repeat_elements(fc,256,axis=-1)\\n# fc = keras.backend.expand_dims(fc,axis=-1)\\nmul = Multiply()([fc,timesteps])\\n# mul = BatchNormalization()(mul)\\nalls = []\\ngru = (GRU(int(emb_dim/4))) #/8\\n# gru = GRU(1)\\nbn = BatchNormalization()\\ndp = Dropout(0.01)\\n\\ndense = Dense(1,activation='sigmoid')\\nfor i in range(fam_num):\\n#     alls.append(dense(bn(gru(mul))))\\n    alls.append(dense(dp(bn(gru(mul)))))\\n#     alls.append(gru(mul))\\nout = Concatenate(name='family')(alls)\\n# out = Dense(44,activation='sigmoid')(out)\\nmodel_old = Model(inputs=[sent_ids1,sentemb1], outputs=[out]) #out\\nmodel_old = multi_gpu_model(model_old , gpus=3)\\nmodel_old.load_weights('./model/LSTM_att/1stStage_44fam_0607.h5')\\nmodel_old.summary()\\n\\n# model = load_model('./model/LSTM_att/1stStage_44fam_0607.h5_all.h5')\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "sentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\n",
    "sentemb = Masking(mask_value=0)(sentemb1)\n",
    "sent_ids1 = Input(shape=(max_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\n",
    "sent_ids = Masking(mask_value=0)(sent_ids1)\n",
    "api_emb = Embedding(vocabulary_size+1, emb_dim,weights=[emb_matrix],input_length=max_length,trainable=True,name='api_emb')(sent_ids)\n",
    "\n",
    "final_emb = Add()([sentemb,api_emb])\n",
    "\n",
    "timesteps,state_h,state_c = LSTM(int(emb_dim/2),return_sequences=True,return_state=True,name='lstm1')(final_emb) #final_emb\n",
    "state = Concatenate()([state_h,state_c])\n",
    "fc = Dense(max_length,activation='sigmoid',bias_constraint=None,kernel_initializer=init,name='attention')(state)\n",
    "fc = Lambda(lambda x: keras.backend.expand_dims(x,axis=-1),name='RasMMA')(fc)\n",
    "fc = Lambda(lambda x: keras.backend.repeat_elements(x,int(emb_dim/2),axis=-1))(fc)\n",
    "# fc = keras.backend.repeat_elements(fc,256,axis=-1)\n",
    "# fc = keras.backend.expand_dims(fc,axis=-1)\n",
    "mul = Multiply()([fc,timesteps])\n",
    "# mul = BatchNormalization()(mul)\n",
    "alls = []\n",
    "gru = (GRU(int(emb_dim/4))) #/8\n",
    "# gru = GRU(1)\n",
    "bn = BatchNormalization()\n",
    "dp = Dropout(0.01)\n",
    "\n",
    "dense = Dense(1,activation='sigmoid')\n",
    "for i in range(fam_num):\n",
    "#     alls.append(dense(bn(gru(mul))))\n",
    "    alls.append(dense(dp(bn(gru(mul)))))\n",
    "#     alls.append(gru(mul))\n",
    "out = Concatenate(name='family')(alls)\n",
    "# out = Dense(44,activation='sigmoid')(out)\n",
    "model_old = Model(inputs=[sent_ids1,sentemb1], outputs=[out]) #out\n",
    "model_old = multi_gpu_model(model_old , gpus=3)\n",
    "model_old.load_weights('./model/LSTM_att/1stStage_44fam_0607.h5')\n",
    "model_old.summary()\n",
    "\n",
    "# model = load_model('./model/LSTM_att/1stStage_44fam_0607.h5_all.h5')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_old.layers[-2].save_weights('./model/LSTM_att/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_multi_label_metric(y_true, y_pred):\n",
    "    comp = K.equal(y_true, K.round(y_pred))\n",
    "    return K.cast(K.all(comp, axis=-1), K.floatx())\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def custom_acc1(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred,k=3)\n",
    "from keras.metrics import binary_accuracy\n",
    "def bin_acc(y_true, y_pred):\n",
    "    return binary_accuracy(y_true, y_pred)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    return tf.keras.metrics.Precision(y_true,y_pred)[1]\n",
    "def recall(y_true, y_pred):\n",
    "    return tf.keras.metrics.Recall(y_true,y_pred)[1]\n",
    "# from sklearn.metrics import f1_score\n",
    "# def f1_sk(y_true,y_pred):\n",
    "#     score = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "#     return score\n",
    "\n",
    "# 訓練參數\n",
    "los = [losses.binary_crossentropy,binary_focal_loss(alpha=.25, gamma=2)] # 1st stage.  f1_loss\n",
    "#SINGLE\n",
    "los = [binary_focal_loss(alpha=.25, gamma=2)]\n",
    "\n",
    "# los = [losses.binary_crossentropy]\n",
    "#MML\n",
    "'''los = []\n",
    "for i in range(fam_num):\n",
    "    los.append(binary_focal_loss(alpha=.25, gamma=2))\n",
    "los = [losses.binary_crossentropy] + los'''\n",
    "\n",
    "\n",
    "metric = {'RasMMA': 'acc','family': f1} # 1st stage. km.f1_score()\n",
    "#SINGLE\n",
    "metric = [f1_metric,bin_acc]\n",
    "# metric = [km.f1_score(),bin_acc,km.binary_f1_score()]\n",
    "# metric = {'RasMMA': 'acc'}\n",
    "# metric = [bin_acc]\n",
    "#MML\n",
    "'''metrics = []\n",
    "for i in range(fam_num+1):\n",
    "    metrics.append('acc')\n",
    "# metrics = {}\n",
    "# metrics['RasMMA'] = 'acc'\n",
    "# for i in range(fam_num):\n",
    "#     metrics['fam'+str(i)]='acc'\n",
    "metric = metrics'''\n",
    "\n",
    "\n",
    "loss_weight = [1,1] #stage1 0.95,0.05  #1st stage # 2nd stage [0.01,0.99]\n",
    "#SINGLE\n",
    "loss_weight = [1]\n",
    "#MML\n",
    "'''loss_weight = []\n",
    "for i in range(fam_num):\n",
    "    loss_weight.append(0.95)\n",
    "loss_weight = [0.05] + loss_weight'''\n",
    "\n",
    "learning_rate = 5e-4#2e-4 # 2nd stage: 1e-4 @1st:2e-4 0.002\n",
    "batch_size = 64 #32 #128\n",
    "\n",
    "num_epochs = 1000\n",
    "patien = 50\n",
    "\n",
    "model_save_path = './model/att_clf/2ndStage_44fam_0611.h5'\n",
    "tensorboard_log_path = './logs/'+ model_save_path.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "optimizer = optimizers.Adam(\n",
    "            lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False) #clipnorm=1. , clipvalue=1.\n",
    "optimizer = keras.optimizers.Nadam(lr=learning_rate, clipvalue=1.)\n",
    "# tf.keras.optimizers.Nadam\n",
    "lr_scheduler1 = callbacks.LearningRateScheduler(\n",
    "        CosineLRSchedule(lr_high=0.0008, lr_low=1e-8, #learning_rate\n",
    "                         initial_period=num_epochs),\n",
    "        verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=int(patien/3),\n",
    "                                      min_lr=1e-8,mode='min')\n",
    "\n",
    "model.compile(\n",
    "            optimizer,\n",
    "            loss=los,\n",
    "            metrics=metric ,loss_weights=loss_weight)#{'word_predictions': masked_perplexity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best, early stopping, 2 models ens weight:(best=0.8,last=0.2)\n",
    "history = History()\n",
    "stop_nan = callbacks.TerminateOnNaN()\n",
    "model_callbacks = [\n",
    "        callbacks.ModelCheckpoint(\n",
    "            model_save_path,\n",
    "            monitor='val_f1_metric',mode='max' ,save_best_only=True, verbose=1,save_weights_only=True),\n",
    "            EarlyStopping(patience=patien,monitor='val_loss',verbose=1,mode='min'),\n",
    "        lr_scheduler, lr_scheduler1,history,stop_nan\n",
    "    ]\n",
    "model_callbacks.append(callbacks.TensorBoard(tensorboard_log_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_batch(batch_size, X_train1, X_train2 , Y_train1, Y_train2):\n",
    "    '''\n",
    "    X_train1 = sent_ids: shape為(N, max_seq_length)\n",
    "    X_train2 = sentemb: shape為(N,max_seq_length, word_embedding_size)\n",
    "    Y_train1 = class_prediction: shape為(N, max_seq_length, 1)\n",
    "    Y_train2 = family_prediction(stage2): shape為(N, fam_num)\n",
    "    '''\n",
    "    idx = np.arange(len(X_train1))\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    while True:\n",
    "        for i in idx:\n",
    "            train_X1 = X_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_X2 = X_train2[idx[i]:idx[i]+batch_size]\n",
    "            train_Y1 = Y_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_Y2 = Y_train2[idx[i]:idx[i]+batch_size]\n",
    "#             yield(train_X2,train_Y2)\n",
    "#             yield ([train_X1,train_X2],[train_Y1,train_Y2]) #ori\n",
    "            yield ([train_X1,train_X2],[train_Y2])\n",
    "            if i == idx[-1]:\n",
    "                idx = np.arange(len(X_train1))\n",
    "                np.random.shuffle(idx)\n",
    "                break\n",
    "            \n",
    "#     data_size = X_train.shape[0]\n",
    "#     ep = data_size / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/.local/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0008.\n",
      "175/175 [==============================] - 2157s 12s/step - loss: 47.4915 - f1_metric: 0.5974 - bin_acc: 0.9804 - val_loss: 44.2031 - val_f1_metric: 0.3618 - val_bin_acc: 0.9672\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.36176, saving model to ./model/att_clf/2ndStage_44fam_0611.h5\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0007999980261054173.\n",
      "175/175 [==============================] - 2279s 13s/step - loss: 38.9961 - f1_metric: 0.5980 - bin_acc: 0.9810 - val_loss: 50.6199 - val_f1_metric: 0.3444 - val_bin_acc: 0.9620\n",
      "\n",
      "Epoch 00002: val_f1_metric did not improve from 0.36176\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0007999921044411505.\n",
      "175/175 [==============================] - 2296s 13s/step - loss: 37.8292 - f1_metric: 0.6058 - bin_acc: 0.9817 - val_loss: 43.2802 - val_f1_metric: 0.3598 - val_bin_acc: 0.9689\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.36176\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0007999822350656444.\n",
      "175/175 [==============================] - 2283s 13s/step - loss: 37.6679 - f1_metric: 0.5664 - bin_acc: 0.9807 - val_loss: 43.1760 - val_f1_metric: 0.4421 - val_bin_acc: 0.9692\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.36176 to 0.44208, saving model to ./model/att_clf/2ndStage_44fam_0611.h5\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0007999684180763055.\n",
      "175/175 [==============================] - 2297s 13s/step - loss: 37.0946 - f1_metric: 0.5634 - bin_acc: 0.9803 - val_loss: 38.2947 - val_f1_metric: 0.3890 - val_bin_acc: 0.9699\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.44208\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.0007999506536095018.\n",
      "175/175 [==============================] - 2307s 13s/step - loss: 37.6087 - f1_metric: 0.5413 - bin_acc: 0.9800 - val_loss: 46.4324 - val_f1_metric: 0.3946 - val_bin_acc: 0.9665\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.44208\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.0007999289418405617.\n",
      "175/175 [==============================] - 2289s 13s/step - loss: 38.1949 - f1_metric: 0.5367 - bin_acc: 0.9800 - val_loss: 44.5466 - val_f1_metric: 0.3289 - val_bin_acc: 0.9692\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.44208\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.0007999032829837715.\n",
      "175/175 [==============================] - 2293s 13s/step - loss: 36.4886 - f1_metric: 0.5331 - bin_acc: 0.9798 - val_loss: 43.2999 - val_f1_metric: 0.0972 - val_bin_acc: 0.9627\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.44208\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.0007998736772923737.\n",
      "175/175 [==============================] - 2296s 13s/step - loss: 37.9485 - f1_metric: 0.5192 - bin_acc: 0.9793 - val_loss: 36.9866 - val_f1_metric: 0.3599 - val_bin_acc: 0.9708\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.44208\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007998401250585643.\n",
      "175/175 [==============================] - 2312s 13s/step - loss: 37.4742 - f1_metric: 0.5087 - bin_acc: 0.9790 - val_loss: 53.9874 - val_f1_metric: 0.1724 - val_bin_acc: 0.9550\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.44208\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007998026266134909.\n",
      "175/175 [==============================] - 2302s 13s/step - loss: 38.9279 - f1_metric: 0.4990 - bin_acc: 0.9791 - val_loss: 37.4573 - val_f1_metric: 0.2772 - val_bin_acc: 0.9680\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.44208\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.0007997611823272474.\n",
      "175/175 [==============================] - 2311s 13s/step - loss: 37.8981 - f1_metric: 0.5116 - bin_acc: 0.9793 - val_loss: 39.1585 - val_f1_metric: 0.3198 - val_bin_acc: 0.9700\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.44208\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.0007997157926088725.\n",
      "175/175 [==============================] - 2298s 13s/step - loss: 37.0657 - f1_metric: 0.5248 - bin_acc: 0.9798 - val_loss: 36.3311 - val_f1_metric: 0.3220 - val_bin_acc: 0.9693\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.44208\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.0007996664579063443.\n",
      "175/175 [==============================] - 2308s 13s/step - loss: 36.4065 - f1_metric: 0.4990 - bin_acc: 0.9792 - val_loss: 36.8005 - val_f1_metric: 0.3492 - val_bin_acc: 0.9682\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.44208\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.0007996131787065766.\n",
      "175/175 [==============================] - 2287s 13s/step - loss: 37.3102 - f1_metric: 0.4952 - bin_acc: 0.9793 - val_loss: 33.2679 - val_f1_metric: 0.3948 - val_bin_acc: 0.9713\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.44208\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.0007995559555354132.\n",
      "175/175 [==============================] - 2304s 13s/step - loss: 37.8740 - f1_metric: 0.5057 - bin_acc: 0.9792 - val_loss: 35.0423 - val_f1_metric: 0.3982 - val_bin_acc: 0.9694\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.44208\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.000799494788957624.\n",
      "175/175 [==============================] - 2315s 13s/step - loss: 38.0983 - f1_metric: 0.5314 - bin_acc: 0.9797 - val_loss: 34.8578 - val_f1_metric: 0.3587 - val_bin_acc: 0.9701\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.44208\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.0007994296795768983.\n",
      "175/175 [==============================] - 2283s 13s/step - loss: 41.5303 - f1_metric: 0.4724 - bin_acc: 0.9786 - val_loss: 34.6687 - val_f1_metric: 0.3649 - val_bin_acc: 0.9694\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.44208\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.0007993606280358395.\n",
      "175/175 [==============================] - 2308s 13s/step - loss: 39.1346 - f1_metric: 0.5040 - bin_acc: 0.9793 - val_loss: 36.7194 - val_f1_metric: 0.3844 - val_bin_acc: 0.9707\n",
      "\n",
      "Epoch 00019: val_f1_metric did not improve from 0.44208\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.0007992876350159584.\n",
      "175/175 [==============================] - 2295s 13s/step - loss: 37.4549 - f1_metric: 0.5027 - bin_acc: 0.9794 - val_loss: 35.1358 - val_f1_metric: 0.4020 - val_bin_acc: 0.9716\n",
      "\n",
      "Epoch 00020: val_f1_metric did not improve from 0.44208\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.0007992107012376665.\n",
      "174/175 [============================>.] - ETA: 12s - loss: 40.2042 - f1_metric: 0.5019 - bin_acc: 0.9790"
     ]
    }
   ],
   "source": [
    "\n",
    "H = model.fit_generator(\n",
    "    generator=training_batch(batch_size=batch_size,X_train1=train_emb_api,X_train2=train_emb ,\n",
    "                                             Y_train1=train_rep_ans,Y_train2=train_fam_ans) #Y_train2\n",
    "#                     generator=training_batch(batch_size=batch_size,X_train1=valid_emb_api,X_train2=valid_emb ,\n",
    "#                                              Y_train1=train_rep_ans,Y_train2=train_fam_ans)\n",
    "                        , steps_per_epoch=int(np.ceil(len(train_emb_api)/batch_size)) ,\n",
    "                    epochs=num_epochs,callbacks=model_callbacks\n",
    "#                    ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans,valid_fam_ans]) #ori\n",
    "#                    ,validation_data= (valid_emb, valid_fam_ans) \n",
    "                   ,validation_data= ([valid_emb_api,valid_emb], [valid_fam_ans]) #ori\n",
    "#                    ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans]+valid_Y2) #validY2\n",
    "                    ,max_queue_size=10  ,class_weight=fam_weights\n",
    "                    ,workers=10,use_multiprocessing=True   \n",
    "                   ,shuffle=True,verbose=1)\n",
    "model.save(model_save_path+\"_all.h5\")\n",
    "#1st:train 0_1_prediction=0.14XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json, model_from_yaml\n",
    "json_string = model.to_json()\n",
    "yaml_string = model.to_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('./model/LSTM_att/1stStage_44fam_0610.h5')\n",
    "# model.load_weights('./model/att_clf/1stStage_44fam_0611.h5')\n",
    "# score = model.evaluate([valid_emb_api,valid_emb], [valid_rep_ans]+valid_Y2)\n",
    "print(len(test_emb_api)) #改\n",
    "ans = model.predict([test_emb_api,test_emb]) #改\n",
    "y_true = test_fam_ans #改\n",
    "# ans = model.predict([valid_emb_api,valid_emb])\n",
    "len(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rep_ans[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.save_weights('./model/LSTM_att/test4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ = model#.layers[-2]\n",
    "layer_name = 'lambda_1' #lambda_1 multiply_1  #9~12\n",
    "intermediate_layer_model = Model(inputs=model_.inputs,\n",
    "                                 outputs=model_.layers[-3].output)\n",
    "intermediate_output = intermediate_layer_model.predict([valid_emb_api,valid_emb])\n",
    "intermediate_output[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(intermediate_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output[113].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intermediate_output[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_.summary() #multiply_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "# y_true = np.squeeze(test_fam_ans)\n",
    "# y_true = np.squeeze(valid_fam_ans1)\n",
    "# y_pred = np.squeeze(predict_fam)\n",
    "final_ans = []\n",
    "for sample in ans:\n",
    "    sample_ans = []\n",
    "    for value in sample:\n",
    "        if value < 0.2:\n",
    "            sample_ans.append(0)\n",
    "        else:\n",
    "            sample_ans.append(1)\n",
    "    final_ans.append(sample_ans)\n",
    "final_ans = np.array(final_ans)\n",
    "print(final_ans.shape , sum(final_ans[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_fam_ans\n",
    "print(y_true.shape , final_ans.shape)\n",
    "recall = recall_score(y_true=y_true, y_pred=final_ans, average='weighted')\n",
    "precision = precision_score(y_true=y_true, y_pred=final_ans, average='weighted')\n",
    "f1 = f1_score(y_true=y_true, y_pred=final_ans, average='weighted')\n",
    "recall ,precision, f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
