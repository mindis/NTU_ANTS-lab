{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisite packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil,pickle,tqdm,sys,random,re,string,pause, datetime,glob\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "# from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "from keras.metrics import *\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from collections import Counter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import unicodedata as udata\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "from keras import backend \n",
    "from imblearn.ensemble import *\n",
    "from imblearn.combine import *\n",
    "# from python.keras import backend \n",
    "# Embedding(10,20)\n",
    "from keras_transformer.extras import ReusableEmbedding, TiedOutputEmbedding\n",
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "from keras_transformer.bert import (\n",
    "    BatchGeneratorForBERT, masked_perplexity,\n",
    "    MaskedPenalizedSparseCategoricalCrossentropy)\n",
    "\n",
    "import keras_metrics as km\n",
    "from keras_trans_mask import RemoveMask, RestoreMask\n",
    "\n",
    "from keras_multi_head import *\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.manifold import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neede files in same dir.\n",
    "from models import transformer_bert_model\n",
    "from bpe import BPEEncoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/tree-rep-profiles_o2o/'\n",
    "train_pid_paths = pickle.load(open(root_dir + 'train_pid_paths.pkl','rb'))\n",
    "valid_pid_paths = pickle.load(open(root_dir + 'valid_pid_paths.pkl','rb'))\n",
    "test_pid_paths = pickle.load(open(root_dir + 'test_pid_paths.pkl','rb'))\n",
    "exp_pid_paths = pickle.load(open(root_dir + 'exp_pid_paths.pkl','rb'))\n",
    "# o2o_pid_paths = pickle.load(open(root_dir + 'o2o_pid_paths.pkl','rb'))\n",
    "\n",
    "train_emb_api,train_emb , train_rep_ans = pickle.load(open(root_dir + 'pids_train.pkl','rb'))\n",
    "valid_emb_api,valid_emb, valid_rep_ans = pickle.load(open(root_dir + 'pids_valid.pkl','rb'))\n",
    "test_emb_api,test_emb ,test_rep_ans = pickle.load(open(root_dir + 'pids_test.pkl','rb'))\n",
    "# o2o_api,o2o_emb,o2o_rep = pickle.load(open(root_dir + 'pids_o2o.pkl','rb'))\n",
    "exp_api,exp_emb = pickle.load(open(root_dir + 'pids_exp.pkl','rb'))\n",
    "emb_matrix = pickle.load(open('data/api_emb_matrix.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load trained NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_acc(y_true, y_pred):\n",
    "    return binary_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './model/o2o_stage_gru_selfatt/XXX.h5' #訓練好的模型\n",
    "model = load_model(model_save_path+'_all.h5',custom_objects={'bin_acc': bin_acc, 'SeqSelfAttention': SeqSelfAttention})\n",
    "# model = load_model(model_save_path+'_all.h5',custom_objects={'bin_acc': bin_acc,'RemoveMask':RemoveMask,'RestoreMask':RestoreMask})#, 'SeqSelfAttention': SeqSelfAttention}) #如果上面的load失敗的話，可以用此\n",
    "model.load_weights(model_save_path)\n",
    "model.summary()\n",
    "thr = 0.51 #0.482 #設定0004.於驗證資料集中所得到最佳的threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = Model(inputs=model.inputs,  outputs=model.layers[5].output) \n",
    "#.layers[5].中的數字可能要調整，讓emb_model最後一層剛好是最終的embedding相加(Add)\n",
    "emb_model.load_weights(model_save_path,by_name=True)\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拿出直到self attention weight輸出的layer，變成att_model。\n",
    "# 接下來就可以輸入某個execution profile的api function name ID跟對應的Sent2Vec vectors來用此model predict\n",
    "# 輸出的結果就會是該execution profile所對應的self attention layer z\n",
    "att_model = Model(inputs=model.inputs,  outputs=model.layers[-2].output) \n",
    "att_model.load_weights(model_save_path,by_name=True)\n",
    "att_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要取self-attention的z出來當profile representation而不是embedding的x' 要在這裡改:\n",
    "* 直接輸出self-attention那一層當成outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representative execution pattern vector\n",
    "* 一個profile所有important call invocation於向量空間中的表示法r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要記得先手動更改以下參數!! (依據不同dataset load近來的參數名稱會不同)\n",
    "def avg_train_vector(root_dir,thr,train_pid_paths=exp_pid_paths,train_emb_api=exp_api,\n",
    "                     train_emb=exp_emb,loner=False):\n",
    "    '''\n",
    "    train_pid_paths: 每一筆profile路徑位置\n",
    "    train_emb_api: api embedding vectors\n",
    "    train_emb: SentVec embedding vectors\n",
    "    loner: False=>train/dev/test Set ; True: Loner Set\n",
    "    '''\n",
    "    no_ind_rep = 0\n",
    "    no_ind_rep_li = []\n",
    "    tree_vec = []\n",
    "    empty_hash_li = []\n",
    "    problem_hash_li = []\n",
    "    un_aliase_path = './data/tree-rep-profiles_o2o/EXP_rev/'\n",
    "    if not os.path.exists(root_dir):\n",
    "        os.makedirs(root_dir,exist_ok=True)\n",
    "    for path,api,emb in tqdm(zip(train_pid_paths,train_emb_api,train_emb)): #tqdm\n",
    "        if loner:\n",
    "            if path not in (claim_path + unk_path):\n",
    "                continue\n",
    "            recent_fam = path.split('/')[-3] #fakeav_fakealeart\n",
    "            recent_hash = path.split('/')[-1].split('_')[0]\n",
    "            if '_' in recent_fam:\n",
    "                all_path_un_aliase = glob.glob(un_aliase_path+'*/*/'+current_hash+'*')\n",
    "                recent_fam = all_path_un_aliase[0].split('/')[4]\n",
    "                \n",
    "        else:\n",
    "            recent_fam = path.split('/')[-3].split('_')[0]\n",
    "        recent_tree = path.split('/')[-2]\n",
    "        hash_id = path.split('/')[-1][:5]\n",
    "        pid_id = path.split('/')[-1].split('_')[-1].split('.')[0][-3:]\n",
    "        with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines()\n",
    "        length = len(lines)\n",
    "        if loner:\n",
    "            if length < 2:\n",
    "                empty_hash_li.append(path)\n",
    "                continue\n",
    "        else:\n",
    "            if length < 11:\n",
    "                print('No common rep. Should not in a tree:',path)\n",
    "                problem_hash_li.append(path)\n",
    "                continue\n",
    "        try:\n",
    "            assert api[:length][-1] != 0\n",
    "            assert api[:length+1][-1] == 0\n",
    "        except AssertionError:\n",
    "            print('Mask assertion ERR:',path)\n",
    "            continue\n",
    "        api_e = np.expand_dims(api, axis=0)\n",
    "        emb_e = np.expand_dims(emb, axis=0)\n",
    "        final_emb_e = emb_model.predict([api_e,emb_e]) #(1,length,768)\n",
    "        final_emb = final_emb_e[0][:length] #(length,768)\n",
    "        byte_rep_e = model.predict([api_e,emb_e])\n",
    "        byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "        byte_rep_thr = []\n",
    "        for num in byte_rep:\n",
    "            if num[0] < thr:\n",
    "                byte_rep_thr.append(0)\n",
    "            else:\n",
    "                byte_rep_thr.append(1) # high than threshold, keep it\n",
    "        byte_rep_thr = np.array(byte_rep_thr)\n",
    "        byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "        mul_emb = np.multiply(final_emb,byte_rep_thr)\n",
    "        rep_emb = [] #超過threshold的emb才留下來\n",
    "        for emb in mul_emb:\n",
    "            if (sum(emb) <0) or (sum(emb)>0):\n",
    "                rep_emb.append(emb)\n",
    "        rep_emb_avg = np.average(rep_emb,axis=0) #把自己各api的embedding取平均，應該一定要有，不然就代表這個profile不該屬於這棵tree \n",
    "        try:\n",
    "            if not (rep_emb_avg <0 or rep_emb_avg>0):\n",
    "                no_ind_rep+=1\n",
    "                no_ind_rep_li.append(path)\n",
    "                continue #沒有individual REP\n",
    "        except ValueError: #正常profile\n",
    "            pass\n",
    "        rep_emb_avg = np.expand_dims(rep_emb_avg,axis=0)\n",
    "        pickle.dump(file=open(root_dir+recent_fam+'_'+recent_tree+'_'+hash_id+'_'+pid_id+'.pkl','wb'),obj=rep_emb_avg)\n",
    "    print('No ind REP#:',no_ind_rep)\n",
    "    if loner:\n",
    "        return no_ind_rep_li,empty_hash_li,problem_hash_li\n",
    "    return no_ind_rep_li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "產生train/valid/test的r，用以下，要更改root_dir、及上述function的輸入變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/valid/test要改三次跑三次\n",
    "root_dir = './data/tree-rep-profiles_o2o/trainingProfile_REP/' #最終輸出r的資料夾，會自動創立資料夾，存放pkl，記得最後的斜線\n",
    "no_ind_rep = avg_train_vector(root_dir=root_dir,thr=thr,loner=False)\n",
    "with open(root_dir + 'no_rep_file.txt','w') as fp:\n",
    "    for path in no_ind_rep:\n",
    "        fp.write(str(path.split('/')[-3:])+'\\n') #no rep的profile path紀錄檔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "產生loner的r，用以下，要更改root_dir(輸出r的資料夾)、及上述function的輸入變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loner\n",
    "loner_dir = './data/tree-rep-profiles_o2o/lonerProfile_REP/'\n",
    "no_ind_rep,empty_files,problem_files = avg_train_vector(root_dir=loner_dir,thr=0.482,loner=True)\n",
    "with open(loner_dir + 'no_rep_file.txt','w') as fp:\n",
    "    for path in no_ind_rep:\n",
    "        fp.write(str(path.split('/')[-1:])+'\\n') #-no rep的profile path紀錄檔\n",
    "with open(loner_dir + 'empty_file.txt','w') as fp:\n",
    "    for path in empty_files:\n",
    "        fp.write(str(path.split('/')[-1:])+'\\n') #空的或太短的profile\n",
    "with open(loner_dir + 'error_file.txt','w') as fp:\n",
    "    for path in problem_files:\n",
    "        fp.write(str(path.split('/')[-1:])+'\\n') #有問題的profile(assertion failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* testing set之BT與F家族歸類，建議直接改由testing set的r來直接與training set的r比較cosine similarity判斷是否歸類成功 (可參考legacy code改寫)\n",
    "* loner Set之F歸類，建議改由上述loner set所得的r直接與training set的r比較cosine similarity判斷是否歸類成功 (可參考legacy code改寫)\n",
    "* train set的r只拿有在DEV set family中的家族"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy code\n",
    "* 因為當時口試及論文時間太趕，再加上學姊當時所需要的數據與圖表較多，因此這部分的code較亂。建議改寫或重寫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing set profiles歸類(行為樹、惡意程式家族)\n",
    "* TY論文Sec 4.3實驗分析\n",
    "* 以process為單位，與training set process的r比較cosine similarity\n",
    "* 輸入: testing是profile(文字型)、training是r(vector型) [建議修改為都由vector型來比較]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試資料集之representative execution pattern準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inf_data(ind_rep_dir): #路徑要有最後的反斜線\n",
    "    ind_reps = next(os.walk(ind_rep_dir))[2]\n",
    "    ind_rep_tree = []\n",
    "    ind_rep_fam = []\n",
    "    for i,c in enumerate(ind_reps):\n",
    "        ind_rep = pickle.load(open(ind_rep_dir + c,'rb')) #每個tree的centroid pickle\n",
    "        if i==0:\n",
    "            ind_reps_matrix = ind_rep # 1st time\n",
    "        else:\n",
    "            ind_reps_matrix = np.concatenate((ind_reps_matrix,ind_rep),axis=0) #依順序合在一起 \n",
    "        ind_rep_tree.append(c.split('_')[1])\n",
    "        ind_rep_fam.append(c.split('_')[0].split('.')[-1])\n",
    "    assert ind_reps_matrix.shape[0] == len(ind_rep_tree) == len(ind_rep_fam)\n",
    "    return ind_reps_matrix,ind_rep_tree,ind_rep_fam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "與testing profile同樣都是最相近(最高分)的training profile有誰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_index(score):\n",
    "    '''\n",
    "    input: cosin sim 分數\n",
    "    output: 同分的index\n",
    "    '''\n",
    "    scores = score[0].argsort() #相似度小到大\n",
    "    scores = np.flip(scores) # 相似度大到小\n",
    "    tmp_score = np.around(np.max(score),decimals=4) # 取到小數點地五為四捨五入到第四位\n",
    "    em_id = []\n",
    "    for s in scores:\n",
    "        now_score = np.round(score[0][s],decimals=4) #現在這個pairwise 的similarity\n",
    "        if now_score ==tmp_score: #同分增額錄取\n",
    "            em_id.append(s)\n",
    "    return em_id,tmp_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 測試資料集BT與F歸類match rate\n",
    "* 論文中Figure 4.13的視覺化圖\n",
    "* 沒有r的profile分析\n",
    "* mismatch的profile分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  (實驗二的code ，找Family的時候，計算match的地方在哪裡。以及求profiles in training set的地方在哪裡)\n",
    "def inf_evaluate(inf_pid_paths,inf_emb_api,inf_emb,ind_reps_matrix,ind_rep_tree,ind_rep_fam,thr=thr):\n",
    "    em_up = 0\n",
    "    no_ind_rep = 0\n",
    "    bound_accuracy = []\n",
    "    wrong_tree = {}\n",
    "    wrong_fam = {}\n",
    "    REP_len = []\n",
    "    all_length = []\n",
    "    family_match = [] # 分對家族 1 否則-1 # 僅限有REP者\n",
    "    family_nmatch = [] # 分錯家族 2 否則-1\n",
    "    tree_match = [] # 分對BT 3 否則-1\n",
    "    tree_nmatch = [] # 分錯BT 4 否則-1\n",
    "    fam_li = [] #ground truth of fam\n",
    "    all_predict_fam = []\n",
    "    tree_li = [] #ground truth of tree\n",
    "    all_predict_tree = []\n",
    "    EM_sim_score = []\n",
    "    \n",
    "    family_match_all = [] # 分對家族 1 否則-1\n",
    "    family_nmatch_all = [] # 分錯家族 2 否則-1\n",
    "    tree_match_all = [] # 分對BT 3 否則-1\n",
    "    tree_nmatch_all = [] # 分錯BT 4 否則-1    \n",
    "    no_rep_list= []\n",
    "    for path,api,emb in tqdm(zip(inf_pid_paths,inf_emb_api,inf_emb)): #tqdm\n",
    "        recent_tree = path.split('/')[-2]\n",
    "        recent_fam = path.split('/')[-3].split('_')[0].split('.')[-1] #EX: shodi\n",
    "        with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines()\n",
    "        length = len(lines)\n",
    "        if length < 1:\n",
    "            print('Empty profile:',path)\n",
    "            continue\n",
    "        assert api[:length][-1] != 0 #確認profile跟所load 近來的vector是一致的\n",
    "        assert api[:length+1][-1] == 0\n",
    "        api_e = np.expand_dims(api, axis=0)\n",
    "        emb_e = np.expand_dims(emb, axis=0)\n",
    "        final_emb_e = emb_model.predict([api_e,emb_e]) #(1,length,768)\n",
    "        final_emb = final_emb_e[0][:length] #(length,768)\n",
    "        byte_rep_e = model.predict([api_e,emb_e])\n",
    "        byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "        byte_rep_thr = []\n",
    "        for num in byte_rep:\n",
    "            if num[0] < thr:\n",
    "                byte_rep_thr.append(0)\n",
    "            else:\n",
    "                byte_rep_thr.append(1) # high than threshold, keep it\n",
    "        byte_rep_thr = np.array(byte_rep_thr)\n",
    "        rep_length = sum(byte_rep_thr)\n",
    "        byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "        mul_emb = np.multiply(final_emb,byte_rep_thr)\n",
    "        rep_emb = [] #超過threshold的emb才留下來\n",
    "        for emb in mul_emb:\n",
    "            if (sum(emb) <0) or (sum(emb)>0):\n",
    "                rep_emb.append(emb)\n",
    "        rep_emb_avg = np.average(rep_emb,axis=0) #把自己各api的embedding取平均，應該一定要有，不然就代表這個profile不該屬於這棵tree \n",
    "        try:\n",
    "            if not (rep_emb_avg <0 or rep_emb_avg>0):\n",
    "                no_ind_rep+=1\n",
    "                no_rep_list.append(path)\n",
    "                family_match_all.append(-1) # 分對家族 1 否則-1\n",
    "                family_nmatch_all.append(2)  # 分錯家族 2 否則-1\n",
    "                tree_match_all.append(-1) # 分對BT 3 否則-1\n",
    "                tree_nmatch_all.append(4)\n",
    "                continue #沒有individual REP\n",
    "        except ValueError: #正常profile\n",
    "            pass\n",
    "        \n",
    "        REP_len.append(rep_length)\n",
    "        all_length.append(length)\n",
    "        \n",
    "        rep_emb_avg = np.expand_dims(rep_emb_avg,axis=0)\n",
    "        score = cosine_similarity(rep_emb_avg,ind_reps_matrix)\n",
    "        \n",
    "        scores = score[0].argsort() #相似度小到大\n",
    "        scores = np.flip(scores) # 相似度大到小\n",
    "        for idx in scores:\n",
    "            if ind_rep_tree[idx] == recent_tree:\n",
    "                bound_accuracy.append(score[0][idx]) #最差都是幾分的時候合併\n",
    "        \n",
    "        idx_list,max_score = EM_index(score) # exact match的id\n",
    "        EM_sim_score.append(max_score)\n",
    "        em_tree = []\n",
    "        for idx in idx_list:\n",
    "            em_tree.append(ind_rep_tree[idx])\n",
    "        em_tree = list(set(em_tree)) # exact match的tree name\n",
    "        all_predict_tree.append(em_tree)\n",
    "        tree_li.append(recent_tree)\n",
    "        if recent_tree in em_tree: #tree match\n",
    "            em_up = em_up + 1\n",
    "            tree_match.append(3) #畫圖\n",
    "            tree_nmatch.append(-1) # 畫圖\n",
    "            tree_match_all.append(3) # 分對BT 3 否則-1\n",
    "            tree_nmatch_all.append(-1)            \n",
    "        else: #tree not match\n",
    "            tree_nmatch.append(4)\n",
    "            tree_match.append(-1)\n",
    "            tree_match_all.append(-1) # 分對BT 3 否則-1\n",
    "            tree_nmatch_all.append(4)         \n",
    "            wrong_tree[path]=em_tree\n",
    "            \n",
    "        em_fam = []\n",
    "        for idx in idx_list:\n",
    "            em_fam.append(ind_rep_fam[idx])\n",
    "        em_fam = list(set(em_fam)) # exact match的tree name\n",
    "        all_predict_fam.append(em_fam)\n",
    "        fam_li.append(recent_fam)\n",
    "        if recent_fam in em_fam: # family match\n",
    "            family_match.append(1) #畫圖\n",
    "            family_nmatch.append(-1) # 畫圖\n",
    "            family_match_all.append(1) # 分對家族 1 否則-1\n",
    "            family_nmatch_all.append(-1)\n",
    "            \n",
    "        else: # family not match\n",
    "            family_nmatch.append(2) #畫圖\n",
    "            family_match.append(-1)      # 畫圖   \n",
    "            family_match_all.append(-1) # 分對家族 1 否則-1\n",
    "            family_nmatch_all.append(2)\n",
    "            wrong_fam[path] = em_fam\n",
    "    down = len(inf_pid_paths)\n",
    "#     print('ExactMatchAccuracyScore:',em_up/down,'EM#:/all#:/NoIndREP#:',em_up,down,no_ind_rep,\n",
    "#          'ExactMatchPrecision(w/o No ind REP):',em_up/(down-no_ind_rep)\n",
    "#          )\n",
    "    return (bound_accuracy,wrong_fam,wrong_tree,em_up/down,no_ind_rep,em_up/(down-no_ind_rep),sum(REP_len)/len(REP_len),sum(all_length)/len(all_length),\n",
    "           family_match_all,family_nmatch_all,tree_match_all,tree_nmatch_all,family_match,family_nmatch,tree_match,tree_nmatch,\n",
    "            fam_li,tree_li,all_predict_fam,all_predict_tree,EM_sim_score,no_rep_list) #真正正確的tree是在幾%相似度才有? 、{[正確 path]=predict list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_rep_dir = './data/tree-rep-profiles_o2o/profile_avg/'#training set的 r 目錄位置\n",
    "\n",
    "matrix,tree_ans,fam_ans = prepare_inf_data(ind_rep_dir)\n",
    "(temp1,test_wrong_fam,test_wrong_tree,EM_score,no_rep,EMw_score,avg_rep,avg_profile,\n",
    "FM_all,FN_all,TM_all,TN_all,FM,FN,TM,TN,\n",
    " fam_truth,tree_truth,fam_predict,tree_predict,EM_score_li,no_rep_path) = inf_evaluate(test_pid_paths,test_emb_api,test_emb\n",
    "                                                     ,ind_reps_matrix=matrix,ind_rep_tree=tree_ans,\n",
    "                                                                         ind_rep_fam=fam_ans,thr=thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TY論文Sec 4.3 mismatch Family的testing profiles分析\n",
    "* 原本所屬的fam name/所屬的tree ID/hash/pid/模型預測的families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mismatch_fam_pd = pd.DataFrame(columns=['ori_fam','ori_tree','sha256','pid','predict_fam'])\n",
    "for i , (path,pred) in enumerate(test_wrong_fam.items()):\n",
    "    fam = path.split('/')[4].split('_')[0].split('.')[-1]\n",
    "    tree = path.split('/')[5]\n",
    "    sha256 = path.split('/')[6].split('_')[0]\n",
    "    pid = path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    test_mismatch_fam_pd.loc[i] = [fam,tree,sha256,pid,pred]\n",
    "test_mismatch_fam_pd.to_excel('./data/tree-rep-profiles_o2o/test_mismatch_fam_sha256.xlsx') #輸出到excel的路徑\n",
    "test_mismatch_fam_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TY論文Sec 4.3 mismatch BT的testing profiles分析\n",
    "* 原本所屬的fam name/所屬的tree ID/hash/pid/模型預測的tree ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mismatch_tree_pd = pd.DataFrame(columns=['ori_fam','ori_tree','sha256','pid','predict_tree'])\n",
    "for i , (path,pred) in enumerate(test_wrong_tree.items()):\n",
    "    fam = path.split('/')[4].split('_')[0].split('.')[-1]\n",
    "    tree = path.split('/')[5]\n",
    "    sha256 = path.split('/')[6].split('_')[0]\n",
    "    pid = path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    test_mismatch_tree_pd.loc[i] = [fam,tree,sha256,pid,pred]\n",
    "test_mismatch_tree_pd.to_excel('./data/tree-rep-profiles_o2o/test_mismatch_tree_sha256.xlsx') #輸出excel到此路徑'\n",
    "test_mismatch_tree_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sec 4.3 no representative execution pattern vector的trainig profiles分析\n",
    "* family/treeID/hash/pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_rep_pd = pd.DataFrame(columns=['fam','tree','sha256','pid'])\n",
    "for i,path in enumerate(no_rep_path):\n",
    "    fam = path.split('/')[4].split('_')[0].split('.')[-1]\n",
    "    tree = path.split('/')[5]\n",
    "    sha256 = path.split('/')[6].split('_')[0]\n",
    "    pid = path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    test_no_rep_pd.loc[i] = [fam,tree,sha256,pid]\n",
    "test_no_rep_pd.to_excel('./data/tree-rep-profiles_o2o/test_no_rep_hash.xlsx') #輸出志excel    \n",
    "test_no_rep_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文中Figure 4.13的視覺化圖\n",
    "* 老師請你畫實驗二的family, BT的match/mismatch的code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的測試資料集都畫出來，包含沒有r的\n",
    "* 沒有r的直接預設BT跟F都分錯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all test set draw viz\n",
    "number_list = []\n",
    "fam_all_list = []\n",
    "for i,v in enumerate(test_pid_paths):\n",
    "    number_list.append(i+1)\n",
    "    fam_all_list.append(v.split('/')[-3].split('_')[0].split('.')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_df = pd.DataFrame(data={'true_fam':fam_all_list,'numberID':number_list,'FamMatch_all':FM_all,'FamNMatch_all':FN_all\n",
    "                             ,'TreeMatch_all':TM_all,'TreeNMatch_all':TN_all})\n",
    "draw_df.to_excel('data/tree-rep-profiles_o2o/EM_draw_allTest.xlsx',index=False) #輸出到excel，用建議圖表\n",
    "draw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只畫有取得r的testing profile來分析match/mismatch F or BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/o no REP draw\n",
    "number_list = [] \n",
    "fam_list = []\n",
    "for i,v in enumerate(fam_truth):\n",
    "    number_list.append(i+1)\n",
    "fam_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_df = pd.DataFrame(data={'true_fam':fam_truth,'numberID':number_list,'FamMatch':FM,'FamNMatch':FN\n",
    "                             ,'TreeMatch':TM,'TreeNMatch_all':TN})\n",
    "draw_df.to_excel('data/tree-rep-profiles_o2o/EM_draw_onlywREP.xlsx',index=False)\n",
    "draw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析在測試資料集中歸類的F&BT之precision/recall/F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_truth_li = []\n",
    "for fam in fam_truth:\n",
    "    fam_truth_li.append([fam])\n",
    "tree_truth_li = []\n",
    "for tree in tree_truth:\n",
    "    tree_truth_li.append([tree])\n",
    "assert len(fam_truth_li) == len(tree_truth_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_true = mlb.fit_transform(fam_truth_li)\n",
    "y_pred = mlb.transform(fam_predict)\n",
    "pre = precision_score(y_true,y_pred,average='micro')\n",
    "rec = recall_score(y_true,y_pred,average='micro')\n",
    "f1 = f1_score(y_true,y_pred,average='micro')\n",
    "print('Family precision/recall/f1',pre,rec,f1)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_pred = mlb.fit_transform(tree_predict)\n",
    "y_true = mlb.transform(tree_truth_li)\n",
    "pre = precision_score(y_true,y_pred,average='micro')\n",
    "rec = recall_score(y_true,y_pred,average='micro')\n",
    "f1 = f1_score(y_true,y_pred,average='micro')\n",
    "print('BT precision/recall/f1',pre,rec,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有r的testing profile預測了幾個F的統計變量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_predict_num = []\n",
    "for fam in fam_predict:\n",
    "    fam_predict_num.append(len(fam))\n",
    "basic_statistics(fam_predict_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存到excel畫分布圖\n",
    "kk = dict(Counter(fam_predict_num))\n",
    "argmax_fam_num_left = list(kk.keys())\n",
    "argmax_fam_num_right = list(kk.values())\n",
    "match_df = pd.DataFrame(data={'left':argmax_fam_num_left, 'right':argmax_fam_num_right})\n",
    "match_df.to_excel('data/tree-rep-profiles_o2o/argmax_match_fam_dist.xlsx',index=False) #excel輸出路徑\n",
    "match_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有r的testing profile系統是預測了幾個BT的統計變量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_predict_num = []\n",
    "for tree in tree_predict:\n",
    "    tree_predict_num.append(len(tree))\n",
    "basic_statistics(tree_predict_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argMax score是幾分的統計變量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_statistics(EM_score_li) #decimal=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 validation set family當中\n",
    "* training Set profiles有無r的統計分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有train rep的資料夾\n",
    "avg_dir = './data/tree-rep-profiles_o2o/profile_avg/' #僅存放包含於DEV set family中的training profiles之r\n",
    "files = next(os.walk(avg_dir))[2]\n",
    "avg_fam = []\n",
    "avg_tree = []\n",
    "print('在47個fam的train set中有#個profiles才有REP',len(files))\n",
    "for file in files:\n",
    "    fam = file.split('_')[0]\n",
    "    avg_fam.append(fam)\n",
    "    tree = fam +'_' +file.split('_')[1]\n",
    "    avg_tree.append(tree)\n",
    "avg_fam_nums = dict(Counter(avg_fam))\n",
    "avg_tree_nums = dict(Counter(avg_tree))\n",
    "\n",
    "print('在47個家族中有多少個fam完全沒有rep',len(set(train_only_fam) - set(avg_fam))) #在47個家族中有多少個fam完全沒有rep\n",
    "print('在47個家族中有多少個tree完全沒有rep',len(set(train_only_tree) - set(avg_tree)))\n",
    "set(train_only_fam) - set(avg_fam) , set(train_only_tree) - set(avg_tree) #哪個family沒有REP?, 哪個tree沒有REP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happen_no_fam_train_only = []\n",
    "for k,v in avg_fam_nums.items():\n",
    "    if train_only_fam_nums[k] == v:\n",
    "        continue\n",
    "    happen_no_fam_train_only.append(k)\n",
    "print('有發生NO ind REP的家族#:',len(set(happen_no_fam_train_only)))\n",
    "happen_no_tree_train_only = []\n",
    "for k,v in avg_tree_nums.items():\n",
    "    if train_only_tree_nums[k] == v:\n",
    "        continue\n",
    "    happen_no_tree_train_only.append(k)\n",
    "print('有發生NO ind REP的BT#:',len(set(happen_no_tree_train_only)))\n",
    "\n",
    "happen_no_tree_train_only #train有發生no r的BT ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* training Set每個family包含多少profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pid_fam_num = {}\n",
    "for path in train_pid_paths:\n",
    "    fam_name = path.split('/')[4].split('_')[0].split('.')[-1]\n",
    "    try:\n",
    "        train_pid_fam_num[fam_name] = train_pid_fam_num[fam_name] + 1\n",
    "    except KeyError:\n",
    "        train_pid_fam_num[fam_name] = 1\n",
    "train_pid_fam_num , sum(list(train_pid_fam_num.values())) # in DEV set, train set's profiles#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* training Set每個family有多少profiles有r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dir = './data/tree-rep-profiles_o2o/profile_avg/' # train set r dir.\n",
    "train_have_rep = next(os.walk(avg_dir))[2]\n",
    "train_pid_fam_haveREPnum = {}\n",
    "for pkl_name in train_have_rep:\n",
    "    fam = pkl_name.split('_')[0].split('.')[-1]\n",
    "    try:\n",
    "        train_pid_fam_haveREPnum[fam] = train_pid_fam_haveREPnum[fam] + 1\n",
    "    except KeyError:\n",
    "        train_pid_fam_haveREPnum[fam] = 1\n",
    "train_pid_fam_haveREPnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* training Set每個family有r的比例 (TY Thesis Fig4.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haveREP_rate = {}\n",
    "for fam,all_num in train_pid_fam_num.items():\n",
    "    try:\n",
    "        up = train_pid_fam_haveREPnum[fam]\n",
    "    except KeyError:\n",
    "        up=0\n",
    "    haveREP_rate[fam] =  up/all_num\n",
    "haveREP_pd = pd.DataFrame([haveREP_rate],index=['Family_Train_haveREP_ratio']).T\n",
    "haveREP_pd.to_excel('data/tree-rep-profiles_o2o/Family_Train_haveREP_pid_ratio.xlsx',index=True) #儲存至excel\n",
    "haveREP_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* training set各家族間有r的比例 (TY thesis Table4.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haverepnum = sum(list(train_pid_fam_haveREPnum.values()))\n",
    "cir_cookie = {}\n",
    "for k,v in train_pid_fam_haveREPnum.items():\n",
    "    cir_cookie[k] = v/haverepnum\n",
    "cir_cookie_pd = pd.DataFrame([cir_cookie],index=['Family_Train_inner_ratio']).T\n",
    "cir_cookie_pd.to_excel('data/tree-rep-profiles_o2o/Family_Train_allpid_ratio.xlsx',index=True) #輸出可以畫圓餅圖\n",
    "cir_cookie_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing set有無r之profile分布分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('testing set中沒有rep的profile個數',len((set(no_rep_path))))\n",
    "\n",
    "unk_ori_fam_num = []\n",
    "unk_fam_have_rep_num = []\n",
    "unk_ori_bt_num = []\n",
    "unk_bt_have_rep_num = []\n",
    "ori_fam = []\n",
    "ori_bt = []\n",
    "for path in no_rep_path:\n",
    "    fam = path.split('/')[4].split('_')[0]\n",
    "    tree = path.split('/')[5]\n",
    "    ori_fam.append(fam)\n",
    "    ori_bt.append(fam + '_'+tree)\n",
    "    fam_have_rep = glob.glob(avg_dir+fam+'*') #have\n",
    "    tree_have_rep = glob.glob(avg_dir+fam+'_'+tree+'*')  #have\n",
    "    fam_all_num = train_only_fam_nums[fam] #ori\n",
    "    tree_all_num = train_only_tree_nums[fam + '_'+tree] #ori\n",
    "    unk_ori_fam_num.append(fam_all_num)\n",
    "    unk_fam_have_rep_num.append(len(fam_have_rep))\n",
    "    unk_ori_bt_num.append(tree_all_num)\n",
    "    unk_bt_have_rep_num.append(len(tree_have_rep))\n",
    "\n",
    "kk = dict(Counter(ori_fam))\n",
    "fam_name = list(kk.keys())\n",
    "fam_name = [x.split('.')[1] for x in fam_name]\n",
    "fam_count = list(kk.values())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 發生在哪個family，該family發生了幾個profile no r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unk_fam_df = pd.DataFrame(data={'Testing no r family':fam_name, 'profiles_num':fam_count})\n",
    "unk_fam_df.to_excel('data/tree-rep-profiles_o2o/argmax_unk_fam_dist.xlsx',index=False)\n",
    "unk_fam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 發生no rep的每一筆testing profile是在哪個family、BT當中。該family原始的profile有幾個?取得r的有幾個?該BT原始的profile有幾個?成功取得r的profile有幾個?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_ana = pd.DataFrame(data={'Ori_Fam':ori_fam,'Ori_BT':ori_bt,'Ori_Fam_profileNum':unk_ori_fam_num,\n",
    "                             'TrainFam_Have_REP_num':unk_fam_have_rep_num,'Ori_BT_profileNum':unk_ori_bt_num,\n",
    "                             'TrainBT_Have_REP_num':unk_bt_have_rep_num})\n",
    "unk_ana.to_excel('data/tree-rep-profiles_o2o/unk_noREP_analyze.xlsx',index=False) #儲存到excel\n",
    "unk_ana #row的數量==len(no_rep_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* testing set中，可取得r的profile是來自train set中的哪個family、BT。該family在train set中含有多少個profile?可成功取得r的有幾個?該BT在trainSet中含有多少個profile?可成功取得r的有幾個profiles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_rep_path = (set(test_pid_paths) - set(no_rep_path))\n",
    "known_ori_fam_num = []\n",
    "known_fam_have_rep_num = []\n",
    "known_ori_bt_num = []\n",
    "known_bt_have_rep_num = []\n",
    "ori_fam = []\n",
    "ori_bt = []\n",
    "for path in have_rep_path:\n",
    "    fam = path.split('/')[4].split('_')[0]\n",
    "    tree = path.split('/')[5]\n",
    "    ori_fam.append(fam)\n",
    "    ori_bt.append(fam + '_'+tree)\n",
    "    fam_have_rep = glob.glob(avg_dir+fam+'*') #have\n",
    "    tree_have_rep = glob.glob(avg_dir+fam+'_'+tree+'*')  #have\n",
    "    fam_all_num = train_only_fam_nums[fam] #ori\n",
    "    tree_all_num = train_only_tree_nums[fam + '_'+tree] #ori\n",
    "    known_ori_fam_num.append(fam_all_num)\n",
    "    known_fam_have_rep_num.append(len(fam_have_rep))\n",
    "    known_ori_bt_num.append(tree_all_num)\n",
    "    known_bt_have_rep_num.append(len(tree_have_rep))\n",
    "\n",
    "known_ana = pd.DataFrame(data={'Ori_Fam':ori_fam,'Ori_BT':ori_bt,'Ori_Fam_profileNum':known_ori_fam_num,\n",
    "                             'TrainFam_Have_REP_num':known_fam_have_rep_num,'Ori_BT_profileNum':known_ori_bt_num,\n",
    "                             'TrainBT_Have_REP_num':known_bt_have_rep_num})\n",
    "known_ana.to_excel('data/tree-rep-profiles_o2o/known_wREP_analyze.xlsx',index=False) #輸出志excel路徑\n",
    "known_ana     #row數量，代表可取得r的profiles (in testing set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了人工去看為什麼會沒有r: 把沒有r的要分析家族profiles複製到另外一個資料夾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dest = './data/tree-rep-profiles_o2o/analyze_noREP/Loner/' #改\n",
    "analyze_fam = 'allaple'\n",
    "out_dir = analyze_dest+analyze_fam+'/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir,exist_ok=True)\n",
    "for path in tqdm(unk_path): #改 #unk_path #no_rep_path\n",
    "    if analyze_fam in path:\n",
    "        profile_name = path.split('/')[-1]\n",
    "        shutil.copy(path,out_dir+profile_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loner家族歸類實驗與分析\n",
    "* TY thesis Sec. 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_hash_pids(root_dir):\n",
    "    fam_dir = next(os.walk(root_dir))[1]\n",
    "    all_pids= []\n",
    "    all_pid_list = []\n",
    "    tree_count = 0\n",
    "    for fam in tqdm(fam_dir):\n",
    "        tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "        tree_count += len(tree_dir)\n",
    "        for tree in tree_dir:\n",
    "            in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            hash_list = [f.split('/')[-1].split('_')[0] for f in hl_list]\n",
    "            pid_list = [f.split('/')[-1] for f in hl_list]\n",
    "            all_pids.extend(hash_list)\n",
    "            all_pid_list.extend(pid_list)\n",
    "#             print(fam,len(hash_list)) #DEBUG\n",
    "    all_hash = set(all_pids)\n",
    "    print('Families#:',len(fam_dir),'Samples#:',len(all_hash),',Processes#:',len(set(all_pid_list)),',Trees#:',tree_count)\n",
    "    return all_hash, list(set(all_pid_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### effective match (topN) loner家族歸類\n",
    "* 門檻要測幾個就要重複跑幾次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN_index(score,sim_score=0.8): #改:多大相似度以上就拿進來 (effective match 門檻)\n",
    "    '''\n",
    "    input: cosin sim 分數\n",
    "    output: topN的index\n",
    "    '''\n",
    "    scores = score[0].argsort() #相似度小到大\n",
    "    scores = np.flip(scores) # 相似度大到小\n",
    "    tmp_score = np.around(np.max(score),decimals=4) # 取道小數點地五為四捨五入到第四位\n",
    "    topN_id = []\n",
    "    for idx in scores:\n",
    "        if score[0][idx] < sim_score:\n",
    "            break\n",
    "        topN_id.append(idx)\n",
    "    return topN_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loner_evaluate(inf_pid_paths,inf_emb_api,inf_emb,ind_reps_matrix, #ind_reps_matrix\n",
    "                   ind_rep_fam,thr=thr,debug=True): #ind_rep_fam\n",
    "    em_up = 0\n",
    "    no_ind_rep_hash = []\n",
    "    bound_accuracy = []\n",
    "    wrong_fam = {}\n",
    "    wrong_fam_em = {}\n",
    "    processed_hash = {}\n",
    "    processed_hash_topN = []\n",
    "    i=-1\n",
    "    topN_up = 0\n",
    "    empty_count = 0\n",
    "    \n",
    "    EM_path = []\n",
    "    topN_path = []\n",
    "    unk_path = []\n",
    "    claim_path = []\n",
    "    sample_topNpredict_num = {}\n",
    "    no_match_anyone = []\n",
    "    \n",
    "    #debug\n",
    "    em_fam_ori = {}\n",
    "    topN_fam_ori = {}\n",
    "#     mismatch_path = [] #改成用減的 (calim_path - EM - topN)\n",
    "    \n",
    "    for path,api,emb in tqdm(zip(inf_pid_paths,inf_emb_api,inf_emb)): #tqdm\n",
    "        i+=1\n",
    "        split_flag = False\n",
    "        recent_fam = path.split('/')[-3] #EX: fakeav_fakealert\n",
    "        if '_' in recent_fam:\n",
    "            split_flag = True\n",
    "        with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines()\n",
    "        length = len(lines)\n",
    "        if length < 2:\n",
    "#             print('Empty profile:',path)\n",
    "            empty_count += 1\n",
    "            continue\n",
    "        try:\n",
    "            assert api[:length][-1] != 0 #確認profile跟所load 近來的vector是一致的\n",
    "            assert api[:length+1][-1] == 0\n",
    "        except:\n",
    "#             print('===Assertion ERR:',i,path)\n",
    "            continue\n",
    "        recent_hash = path.split('/')[-1].split('_')[0]\n",
    "        if recent_hash in processed_hash.keys():\n",
    "            if processed_hash[recent_hash] == 1:\n",
    "                pass\n",
    "        else:\n",
    "            processed_hash[recent_hash] = 0        \n",
    "        \n",
    "        api_e = np.expand_dims(api, axis=0)\n",
    "        emb_e = np.expand_dims(emb, axis=0)\n",
    "        final_emb_e = emb_model.predict([api_e,emb_e]) #(1,length,768)\n",
    "        final_emb = final_emb_e[0][:length] #(length,768)\n",
    "        byte_rep_e = model.predict([api_e,emb_e])\n",
    "        byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "        byte_rep_thr = []\n",
    "        for num in byte_rep:\n",
    "            if num[0] < thr:\n",
    "                byte_rep_thr.append(0)\n",
    "            else:\n",
    "                byte_rep_thr.append(1) # high than threshold, keep it\n",
    "        byte_rep_thr = np.array(byte_rep_thr)\n",
    "        byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "        mul_emb = np.multiply(final_emb,byte_rep_thr)\n",
    "        rep_emb = [] #超過threshold的emb才留下來\n",
    "        for emb in mul_emb:\n",
    "            if (sum(emb) <0) or (sum(emb)>0):\n",
    "                rep_emb.append(emb)\n",
    "        rep_emb_avg = np.average(rep_emb,axis=0) #把自己各api的embedding取平均，應該一定要有，不然就代表這個profile不該屬於這棵tree \n",
    "        try:\n",
    "            if not (rep_emb_avg <0 or rep_emb_avg>0):\n",
    "                no_ind_rep_hash.append(recent_hash)\n",
    "                unk_path.append(path)\n",
    "                continue #沒有individual REP\n",
    "        except ValueError: #正常profile\n",
    "            pass\n",
    "        rep_emb_avg = np.expand_dims(rep_emb_avg,axis=0)\n",
    "        score = cosine_similarity(rep_emb_avg,ind_reps_matrix)\n",
    "        \n",
    "        scores = score[0].argsort() #相似度小到大\n",
    "        scores = np.flip(scores) # 相似度大到小\n",
    "        for idx in scores:\n",
    "            if ind_rep_fam[idx] == recent_fam:\n",
    "                bound_accuracy.append(score[0][idx]) #最差都是幾分的時候合併 #by_process\n",
    "        \n",
    "        idx_list,temp = EM_index(score) # exact match的id\n",
    "        idx_list_topN = topN_index(score)\n",
    "        em_fam = []\n",
    "        for idx in idx_list:\n",
    "            em_fam.append(ind_rep_fam[idx])\n",
    "        em_fam = list(set(em_fam)) # exact match的tree name\n",
    "        topN_fam = []\n",
    "        for idx in idx_list_topN:\n",
    "            topN_fam.append(ind_rep_fam[idx])\n",
    "        topN_fam = list(set(topN_fam))\n",
    "\n",
    "        if len(topN_fam)>0:\n",
    "            try:\n",
    "#                 print(sample_topNpredict_num[recent_hash],topN_fam,recent_hash)\n",
    "                sample_topNpredict_num[recent_hash].extend(topN_fam)\n",
    "            except KeyError:\n",
    "#                 print(topN_fam)\n",
    "                sample_topNpredict_num[recent_hash] = topN_fam\n",
    "        else:\n",
    "#                     if (topN_fam == []) or (topN_fam is None):\n",
    "            no_match_anyone.append(path)\n",
    "        \n",
    "        flag = False #預設沒分對類別\n",
    "        fams = []\n",
    "        for fam in em_fam:\n",
    "            if fam in recent_fam:# recent_fam: 答案\n",
    "                processed_hash[recent_hash] = 1\n",
    "                EM_path.append(path)\n",
    "                flag = True #分對類\n",
    "                fams.append(fam)\n",
    "                if split_flag and debug:\n",
    "                    em_fam_ori[path] = fams\n",
    "#                     print(path,fam,recent_fam) #profile path、predict、true\n",
    "                if not debug:\n",
    "                    break\n",
    "        if not flag: #依然沒分對類別\n",
    "            wrong_fam_em[path]=em_fam #by_process最接近的是誰\n",
    "        if recent_hash not in processed_hash_topN:\n",
    "            fams = []\n",
    "            for fam in topN_fam:\n",
    "                if fam in recent_fam:\n",
    "                    topN_up = topN_up + 1\n",
    "                    topN_path.append(path)\n",
    "                    flag = True\n",
    "                    fams.append(fam)\n",
    "                    if split_flag and debug:\n",
    "                        topN_fam_ori[path] = fams\n",
    "#                         print(path,fam,recent_fam)\n",
    "                    if not debug:\n",
    "                        break\n",
    "            processed_hash_topN.append(recent_hash)\n",
    "            if not flag:\n",
    "                wrong_fam[path]=topN_fam\n",
    "        claim_path.append(path)\n",
    "    em_up = sum(list(processed_hash.values()))\n",
    "    down = len(processed_hash)\n",
    "    no_ind_rep = len(list(set(no_ind_rep_hash) - set(processed_hash.keys())))#有問題\n",
    "#     print('ExactMatchAccuracyScore:',em_up/down,'EM_hash#:/all_hash#:/NoIndREP_sample#:',em_up,down,no_ind_rep,\n",
    "#          'ExactMatchPrecision(w/o No ind REP):',em_up/(down-no_ind_rep),\n",
    "#           'TopNAccScore:',topN_up/len(processed_hash_topN),'TopNAccScore(w/o No ind REP):',topN_up/(len(processed_hash_topN)-no_ind_rep),\n",
    "#           'Empty process#:',empty_count\n",
    "#          )\n",
    "    return (bound_accuracy,wrong_fam,processed_hash,topN_up/(len(processed_hash_topN)-no_ind_rep),\n",
    "           EM_path,topN_path,unk_path,claim_path,sample_topNpredict_num,no_match_anyone,\n",
    "            em_fam_ori,topN_fam_ori) #真正正確的tree是在幾%相似度才有? 、{[正確 path]=predict list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記得要改門檻是0.8/0.9 (上面function要改，下面變數名稱要改)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_rep_dir = './data/tree-rep-profiles_o2o/profile_avg/'#'train set r\n",
    "matrix,tree_ans,fam_ans = prepare_inf_data(ind_rep_dir)\n",
    "(temp1,loner_misFam_08,temp3,topN_score,\n",
    "EM_path,topN_path,unk_path,claim_path,\n",
    " sample_topNpredict_num,no_match_anyone_path,\n",
    "em_split_fam_ori,top08_split_fam_ori) = loner_evaluate(exp_pid_paths,exp_api,exp_emb\n",
    "                                                         ,ind_reps_matrix=matrix,ind_rep_fam=fam_ans,thr=thr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loner set no r 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hash_in_loner = list(set(temp3))\n",
    "print('loner dataset hash數量:',len(all_hash_in_loner),'processes數量:',len(claim_path)+len(unk_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_mismatch_hash = [x.split('/')[-1].split('_')[0] for x in claim_path]\n",
    "print('Samples# in topNMatch/ExactMatch/Mismatch:',len(set(match_mismatch_hash)),'pids#',len(match_mismatch_hash))\n",
    "unk_hash = list(set([x.split('/')[-1].split('_')[0] for x in unk_path]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_no_rep_sample = []\n",
    "is_child = []\n",
    "is_main = []\n",
    "for path in unk_path:\n",
    "    hash_id = path.split('/')[-1].split('_')[0]\n",
    "    pid = int(path.split('/')[-1].split('_')[1].split('.')[0])\n",
    "    corr_samples = [x for x in claim_path if hash_id in x]\n",
    "    if corr_samples == []:\n",
    "        pure_no_rep_sample.append(path)\n",
    "    else:\n",
    "        corr_samples_pid = [int(x.split('/')[-1].split('_')[1].split('.')[0]) for x in corr_samples]\n",
    "        if pid>min(corr_samples_pid):\n",
    "            is_child.append(path)\n",
    "        else:\n",
    "            is_main.append(path)\n",
    "pure_no_rep_sample_hash = list(set([x.split('/')[-1].split('_')[0] for x in pure_no_rep_sample]))\n",
    "print('某loner完全沒有任何一個process有REP的process數量:',len(pure_no_rep_sample),'Samples數量:',len(pure_no_rep_sample_hash))\n",
    "print('有其他process有REP，但有部分process沒REP且屬於child的pid數量是:',len(is_child),'沒REP是main pid的數量:',len(is_main))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sec 4.4 lonerSet no representative execution pattern vector的samples/profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loner_no_rep_pd = pd.DataFrame(columns=['fam','sha256','pid'])\n",
    "for i,path in enumerate(pure_no_rep_sample):\n",
    "    fam = path.split('/')[4].split('_')[0].split('.')[-1]\n",
    "#     tree = path.split('/')[5]\n",
    "    sha256 = path.split('/')[6].split('_')[0]\n",
    "    pid = path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    loner_no_rep_pd.loc[i] = [fam,sha256,pid]\n",
    "loner_no_rep_pd.to_excel('./data/tree-rep-profiles_o2o/loner_no_rep_hash.xlsx',index=False) #輸出志excel    \n",
    "loner_no_rep_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loner set 各家族所含的samples#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_path = []\n",
    "all_path.extend(claim_path)\n",
    "all_path.extend(unk_path)\n",
    "\n",
    "processed_sample = []\n",
    "fam_hash_num = {} #每個family在loner set當中有多少samples\n",
    "for path in all_path:\n",
    "    current_hash = path.split('/')[-1].split('_')[0]\n",
    "    current_fam = path.split('/')[4]\n",
    "    if current_hash in processed_sample:\n",
    "        continue\n",
    "    else:\n",
    "        processed_sample.append(current_hash)\n",
    "    try:\n",
    "        fam_hash_num[current_fam] = fam_hash_num[current_fam] + 1 \n",
    "    except KeyError:\n",
    "        fam_hash_num[current_fam] = 1\n",
    "fam_hash_num ,len(fam_hash_num) , sum(list(fam_hash_num.values())) #分布情形, family總數, samples總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將分布輸出到excel\n",
    "fam_unkhash_num_pd = pd.DataFrame([fam_hash_num],index=['fam_unkhash_num']).T\n",
    "fam_unkhash_num_pd.to_excel('data/tree-rep-profiles_o2o/fam_unkhash_num.xlsx',index=True)\n",
    "fam_unkhash_num_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loner set中各家族沒有r的profiles含蓋了多少個samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_sample = []\n",
    "fam_unkhash_num = {} #每個family在loner set當中有多少unk samples\n",
    "for path in pure_no_rep_sample:\n",
    "    current_hash = path.split('/')[-1].split('_')[0]\n",
    "    current_fam = path.split('/')[4]  \n",
    "    if current_hash in processed_sample:\n",
    "        continue\n",
    "    else:\n",
    "        processed_sample.append(current_hash)\n",
    "    try:\n",
    "        fam_unkhash_num[current_fam] = fam_unkhash_num[current_fam] + 1 \n",
    "    except KeyError:\n",
    "        fam_unkhash_num[current_fam] = 1\n",
    "fam_unkhash_num ,len(fam_unkhash_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將分布輸出到excel\n",
    "fam_unkhash_num_pd = pd.DataFrame([fam_unkhash_num],index=['fam_unkhash_num']).T\n",
    "fam_unkhash_num_pd.to_excel('data/tree-rep-profiles_o2o/fam_unkhash_num.xlsx',index=True)\n",
    "fam_unkhash_num_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用excel畫出每個家族的no r 比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw all family unk rate\n",
    "unk_rate = {}\n",
    "for fam,all_num in fam_hash_num.items():\n",
    "    try:\n",
    "        up = fam_unkhash_num[fam]\n",
    "    except KeyError:\n",
    "        up=0\n",
    "    unk_rate[fam] = up/all_num\n",
    "unk_pd = pd.DataFrame([unk_rate],index=['Unknown_rate']).T\n",
    "unk_pd.to_excel('data/tree-rep-profiles_o2o/FamilyUNK_rate_loner.xlsx',index=True) #路徑自行修改\n",
    "unk_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loner set中可成功取得r的family 分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "processed_sample = []\n",
    "fam_hash_woUNK_num = {} #每個family在loner set當中有多少samples是具有REP的\n",
    "for path in claim_path:\n",
    "    current_hash = path.split('/')[-1].split('_')[0]\n",
    "    current_fam = path.split('/')[4]\n",
    "    if current_hash in processed_sample:\n",
    "        continue\n",
    "    else:\n",
    "        processed_sample.append(current_hash)\n",
    "    try:\n",
    "        fam_hash_woUNK_num[current_fam] = fam_hash_woUNK_num[current_fam] + 1 \n",
    "    except KeyError:\n",
    "        fam_hash_woUNK_num[current_fam] = 1\n",
    "fam_hash_woUNK_num ,len(fam_hash_woUNK_num) , sum(list(fam_hash_woUNK_num.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存至excel\n",
    "fam_hash_woUNK_mis_num_pd = pd.DataFrame([fam_hash_woUNK_num],index=['fam_hash_haveREP_num']).T\n",
    "fam_hash_woUNK_mis_num_pd.to_excel('./data/tree-rep-profiles_o2o/loner_fam_dist.xlsx')\n",
    "fam_hash_woUNK_mis_num_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loner set effective match與mismatch分析\n",
    "* 不同相似度門檻要反覆做不同回合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_hash = [x.split('/')[-1].split('_')[0] for x in EM_path]\n",
    "topN_hash = [x.split('/')[-1].split('_')[0] for x in topN_path]\n",
    "mismatch_hash = list(set(match_mismatch_hash) - set(topN_hash) -set(EM_hash))\n",
    "print('mismatch sample#:',len(mismatch_hash))\n",
    "exp_dir = './data/tree-rep-profiles_o2o/EXP/' #loner Set目錄\n",
    "mismatch_path = []\n",
    "for hash_m in mismatch_hash:\n",
    "    sample_pids = glob.glob(exp_dir+'*/*/'+hash_m+'*')\n",
    "    mismatch_path.extend(sample_pids)\n",
    "print('mismatch process#:',len(mismatch_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每個family有多少個sample是分錯的\n",
    "* 注意變數名稱，不同門檻要記得更改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loner08_mismatch_fam_pd = pd.DataFrame(columns=['ori_fam','sha256'])\n",
    "processed_sample = []\n",
    "fam_hash_woUNK_mis_num = {} #每個family有多少個mismatch sample\n",
    "for i,path in enumerate(mismatch_path):\n",
    "    current_hash = path.split('/')[-1].split('_')[0]\n",
    "    current_fam = path.split('/')[4]\n",
    "#     pred = loner_misFam_09[path]\n",
    "    if current_hash in no_match_sample:\n",
    "        continue    \n",
    "\n",
    "    if current_hash in processed_sample:\n",
    "        continue\n",
    "    else:\n",
    "        processed_sample.append(current_hash)\n",
    "    try:\n",
    "        fam_hash_woUNK_mis_num[current_fam] = fam_hash_woUNK_mis_num[current_fam] + 1 \n",
    "    except KeyError:\n",
    "        fam_hash_woUNK_mis_num[current_fam] = 1\n",
    "    loner08_mismatch_fam_pd.loc[i] = [current_fam,current_hash]\n",
    "fam_hash_woUNK_mis_num ,len(fam_hash_woUNK_mis_num) , sum(list(fam_hash_woUNK_mis_num.values())) #家族分布, 發生的family, 總共包含多少個mismatch sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存至excel，注意檔名與路徑\n",
    "fam_hash_woUNK_mis_num_pd = pd.DataFrame([fam_hash_woUNK_mis_num],index=['fam_hash_woUNK_mis_num']).T\n",
    "fam_hash_woUNK_mis_num_pd.to_excel('data/tree-rep-profiles_o2o/fam_hash_woUNK_mis_num08.xlsx',index=True)\n",
    "fam_hash_woUNK_mis_num_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sec 4.4 mismatch Family的sample\n",
    "* 不同門檻記得要更改變數以及檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#儲存至execel\n",
    "loner08_mismatch_fam_pd.to_excel('./data/tree-rep-profiles_o2o/loner08_mismsatch_fam.xlsx',index=False)\n",
    "loner08_mismatch_fam_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loner set中各個家族effective match比例\n",
    "* 不同門檻要跑不同次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw all family unk rate\n",
    "match_rate = {}\n",
    "for fam,all_num in fam_hash_woUNK_num.items():\n",
    "    try:\n",
    "        up = fam_hash_woUNK_mis_num[fam]\n",
    "    except KeyError:\n",
    "        up=0\n",
    "    match_rate[fam] = 1 - up/all_num\n",
    "acc_pd = pd.DataFrame([match_rate],index=['Family_Match_rate']).T\n",
    "acc_pd.to_excel('data/tree-rep-profiles_o2o/FamilyMatch08_rate_loner.xlsx',index=True) #儲存至excel\n",
    "acc_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "undecided sample發生在哪些家族? 跟train set r的相似度都低於門檻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all predicted family below  threshold (sample)\n",
    "print(len(no_match_anyone_path)) #PID\n",
    "no_match_anyone_path_hash = list(set([x.split('/')[-1].split('_')[0] for x in no_match_anyone_path]))\n",
    "no_match_sample = list(set(no_match_anyone_path_hash) - set(sample_topNpredict_num.keys()))\n",
    "print('有多少個sample最近的family的相似度都低於topN thr:',len(no_match_sample))\n",
    "fam_no_match = []\n",
    "processed_sample = []\n",
    "\n",
    "for path in no_match_anyone_path:  \n",
    "    current_hash = path.split('/')[-1].split('_')[0]\n",
    "    if current_hash in processed_sample:\n",
    "        continue\n",
    "    else:\n",
    "        processed_sample.append(current_hash)  \n",
    "    current_fam = path.split('/')[4]  \n",
    "    if current_hash in no_match_sample:\n",
    "        fam_no_match.append(current_fam)\n",
    "print(len(fam_no_match))\n",
    "fam_no_match_stat = dict(Counter(fam_no_match))\n",
    "fam_no_match_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loner set每個sample預測了幾個F的統計資料\n",
    "* 不同相似度門檻要分開做不同次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_topNpredict_num = []\n",
    "for key, li in sample_topNpredict_num.items():\n",
    "    hash_topNpredict_num.append(len(set(li)))\n",
    "print('共有多少個samples至少預測一個family:',len(hash_topNpredict_num))\n",
    "basic_statistics(hash_topNpredict_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將分布存到excel畫圖\n",
    "sample_topNpredict_num_df = dict(Counter(hash_topNpredict_num))\n",
    "sample_topNpredict_num_df = pd.DataFrame([sample_topNpredict_num_df],index=['topN_fam_predict_num']).T\n",
    "sample_topNpredict_num_df.to_excel('data/tree-rep-profiles_o2o/topN08_predictFam_num_loner.xlsx',index=True)\n",
    "sample_topNpredict_num_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每個sample是預測了哪些family出來?\n",
    "* 以0.9 相似度門檻為例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_belongFam09_pd = pd.DataFrame(columns=['sha256','predict_fam'])\n",
    "all_values_bySample = []\n",
    "for i,(key, li) in enumerate(sample_topNpredict_num.items()):\n",
    "    value = list(set(li))\n",
    "    all_values_bySample.extend(value)\n",
    "    sample_belongFam09_pd.loc[i] = [key,value]\n",
    "sample_belongFam09_pd.to_excel('./data/tree-rep-profiles_o2o/sample_belongFam09.xlsx',index=False)\n",
    "sample_belongFam09_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的家族中每個家族共被系統預測了幾次 (sample很靠近的是來自哪些family)\n",
    "* 不同門檻要跑不同次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_loner_sampleNum_df = dict(Counter(all_values_bySample))\n",
    "fam_loner_sampleNum_df = pd.DataFrame([fam_loner_sampleNum_df],index=['loner_sampples_num']).T\n",
    "fam_loner_sampleNum_df.to_excel('./data/tree-rep-profiles_o2o/fam_loner_sampleNum09.xlsx') #儲存至excel\n",
    "fam_loner_sampleNum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representative execution pattern visualize\n",
    "* TY thesis Sec. 4.5\n",
    "* by umap 2D\n",
    "* tool: http://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 畫出某個family的BT分布\n",
    "* train & test\n",
    "* 如果想用在實驗二畫unknown sample應該怎麼做: 求出train的embedding vectors、求出test的embedding vector。並把vector中每個值用\\t分開。metadata的部分則可以放自己想要看的標籤。詳細可參考 http://projector.tensorflow.org/ 的load按鈕裡面的教學"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_name = '3.allaple'\n",
    "train_dir =  './data/tree-rep-profiles_o2o/profile_avg/'+fam_name+'*' #avg rep的位置\n",
    "train_avg_pickles = glob.glob(train_dir)\n",
    "# test_dir = './data/tree-rep-profiles_o2o/TEST/'+fam_name+'_0.8/'\n",
    "# thr =0.51 #0.482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_emb = []\n",
    "train_tree_name = ['Tree\\tType\\tID']\n",
    "for i,avg_emb_path in enumerate(train_avg_pickles):\n",
    "    tree_name = avg_emb_path.split('/')[-1].split('_')[1]\n",
    "    emb = pickle.load(open(avg_emb_path,'rb'))[0]\n",
    "    train_avg_emb.append(emb)\n",
    "    train_tree_name.append(tree_name+'\\t'+'Train'+'\\t'+str(i))\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_target_vector(target_pid_paths,target_emb_api,target_emb,thr):\n",
    "    '''\n",
    "    計算profile的r\n",
    "    '''\n",
    "    no_ind_rep = 0\n",
    "    tree_vec = []\n",
    "    target_avg_emb = []\n",
    "    target_tree_name = []\n",
    "    print(thr)\n",
    "    i=392 #接續上面i的ID\n",
    "\n",
    "    for path,api,emb in tqdm(zip(target_pid_paths,target_emb_api,target_emb)): #tqdm\n",
    "\n",
    "        recent_tree = path.split('/')[-2]\n",
    "\n",
    "        with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines()\n",
    "        length = len(lines)\n",
    "        if length < 2:\n",
    "            print('No common rep. Should not in a tree:',path)\n",
    "            continue\n",
    "        assert api[:length][-1] != 0\n",
    "        assert api[:length+1][-1] == 0\n",
    "        api_e = np.expand_dims(api, axis=0)\n",
    "        emb_e = np.expand_dims(emb, axis=0) \n",
    "        final_emb_e = emb_model.predict([api_e,emb_e]) #(1,length,768) ##api_e,emb_e\n",
    "        final_emb = final_emb_e[0][:length] #(length,768)\n",
    "        byte_rep_e = model.predict([api_e,emb_e]) ##api_e,emb_e\n",
    "        byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "        byte_rep_thr = []\n",
    "        for num in byte_rep:\n",
    "            if num[0] < thr:\n",
    "                byte_rep_thr.append(0)\n",
    "            else:\n",
    "                byte_rep_thr.append(1) # high than threshold, keep it\n",
    "        byte_rep_thr = np.array(byte_rep_thr)\n",
    "        byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "        mul_emb = np.multiply(final_emb,byte_rep_thr)\n",
    "        rep_emb = [] #超過threshold的emb才留下來\n",
    "        for emb in mul_emb:\n",
    "            if (sum(emb) <0) or (sum(emb)>0):\n",
    "                rep_emb.append(emb)\n",
    "        rep_emb_avg = np.average(rep_emb,axis=0) #把自己各api的embedding取平均，應該一定要有，不然就代表這個profile不該屬於這棵tree \n",
    "        try:\n",
    "            if not (rep_emb_avg <0 or rep_emb_avg>0):\n",
    "                no_ind_rep+=1\n",
    "                continue #沒有individual REP\n",
    "        except ValueError: #正常profile\n",
    "            pass\n",
    "        target_avg_emb.append(rep_emb_avg)\n",
    "        target_tree_name.append(recent_tree+'\\t'+'Test'+'\\t'+str(i))\n",
    "        i=i+1\n",
    "#         rep_emb_avg = np.expand_dims(rep_emb_avg,axis=0)\n",
    "#         pickle.dump(file=open(root_dir+recent_fam+'_'+recent_tree+'_'+hash_id+'_'+pid_id+'.pkl','wb'),obj=rep_emb_avg)\n",
    "    print('No ind REP#:',no_ind_rep)\n",
    "    return target_avg_emb,target_tree_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = []\n",
    "target_pid_paths = []\n",
    "for i,path in enumerate(test_pid_paths):\n",
    "    if fam_name in path:\n",
    "        target_id.append(i)\n",
    "        target_pid_paths.append(path)\n",
    "target_emb_api = test_emb_api[target_id]\n",
    "target_emb = test_emb[target_id]\n",
    "assert len(target_id) == len(target_emb_api) == len(target_emb) == len(target_pid_paths)\n",
    "print(target_emb.shape)\n",
    "\n",
    "target_avg_emb,target_tree_name = avg_target_vector(target_pid_paths,target_emb_api,target_emb,thr)\n",
    "assert len(target_avg_emb) == len(target_tree_name)\n",
    "\n",
    "all_avg_emb = train_avg_emb\n",
    "all_tree_name = train_tree_name\n",
    "print(len(all_avg_emb),len(all_tree_name))\n",
    "all_avg_emb.extend(target_avg_emb)\n",
    "all_tree_name.extend(target_tree_name)\n",
    "len(all_avg_emb),len(all_tree_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "輸出要視覺化的r vectors與metadata\n",
    "* 兩個檔案的輸出路徑記得修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_df = pd.DataFrame(np.array(all_avg_emb))\n",
    "all_avg_df.to_csv('./data/tree-rep-profiles_o2o/allaple_emb.tsv',sep='\\t',index=False,header=False)\n",
    "with open (\"./data/tree-rep-profiles_o2o/allaple_meta.tsv\",\"w\")as fp:\n",
    "    for line in all_tree_name:\n",
    "        fp.write(line+\"\\n\")\n",
    "all_avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 畫出train Set中有取得r的家族之所有profiles之r\n",
    "* train only、實驗三中的visualization的圖從一開始怎麼畫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir =  './data/tree-rep-profiles_o2o/profile_avg/'+'*' #train set r 的位置\n",
    "train_avg_pickles = glob.glob(train_dir)\n",
    "# thr = 0.51 #0.482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_emb = []\n",
    "train_tree_name = ['Family\\tTree\\tType']\n",
    "for i,avg_emb_path in enumerate(train_avg_pickles):\n",
    "    fam_name = avg_emb_path.split('/')[-1].split('_')[0]\n",
    "    tree_name = avg_emb_path.split('/')[-1].split('_')[1]\n",
    "    emb = pickle.load(open(avg_emb_path,'rb'))[0]\n",
    "    train_avg_emb.append(emb)\n",
    "    train_tree_name.append(fam_name+'\\t'+tree_name+'\\t'+'Train')\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "輸出train set中所有r的vectors及metadata labels\n",
    "* 輸出位置請自行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_avg_emb) + 1 == len(train_tree_name)\n",
    "fam_avg_df = pd.DataFrame(np.array(train_avg_emb))\n",
    "fam_avg_df.to_csv('./data/tree-rep-profiles_o2o/47fam_emb.tsv',sep='\\t',index=False,header=False)\n",
    "with open (\"./data/tree-rep-profiles_o2o/47fam_meta.tsv\",\"w\")as fp:\n",
    "    for line in train_tree_name:\n",
    "        fp.write(line+\"\\n\")\n",
    "fam_avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畫出loner set中某個家族所包含的profiles之r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_target = 'allaple'\n",
    "train_dir =  './data/tree-rep-profiles_o2o/profile_avg/'+'*' #avg rep的位置\n",
    "train_avg_pickles = glob.glob(train_dir)\n",
    "# thr = 0.51 #0.482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_emb = []\n",
    "train_tree_name = ['Family\\tTree\\tType']\n",
    "for i,avg_emb_path in enumerate(train_avg_pickles):\n",
    "    fam_name = avg_emb_path.split('/')[-1].split('_')[0].split('.')[1]\n",
    "    tree_name = fam_name+'_'+avg_emb_path.split('/')[-1].split('_')[1]\n",
    "    emb = pickle.load(open(avg_emb_path,'rb'))[0]\n",
    "    train_avg_emb.append(emb)\n",
    "    train_tree_name.append(fam_name+'\\t'+tree_name+'\\t'+'Train')\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_target_vector_fam(target_pid_paths,target_emb_api,target_emb,thr):\n",
    "    no_ind_rep = 0\n",
    "    tree_vec = []\n",
    "    target_avg_emb = []\n",
    "    target_fam_name = []\n",
    "    print(thr)\n",
    "    i=392 #\n",
    "    for path,api,emb in tqdm(zip(target_pid_paths,target_emb_api,target_emb)): #tqdm\n",
    "        recent_fam = path.split('/')[-3].split('_')[0]\n",
    "        recent_tree = recent_fam + '_' + path.split('/')[-2]\n",
    "#         hash_id = path.split('/')[-1][:5]\n",
    "#         pid_id = path.split('/')[-1].split('_')[-1].split('.')[0][-3:]\n",
    "        with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines()\n",
    "        length = len(lines)\n",
    "        if length < 2:\n",
    "            print('No common rep. Should not in a tree:',path)\n",
    "            continue\n",
    "        assert api[:length][-1] != 0\n",
    "        assert api[:length+1][-1] == 0\n",
    "        api_e = np.expand_dims(api, axis=0)\n",
    "        emb_e = np.expand_dims(emb, axis=0) \n",
    "        final_emb_e = emb_model.predict([api_e,emb_e]) #(1,length,768) ##api_e,emb_e\n",
    "        final_emb = final_emb_e[0][:length] #(length,768)\n",
    "        byte_rep_e = model.predict([api_e,emb_e]) ##api_e,emb_e\n",
    "        byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "        byte_rep_thr = []\n",
    "        for num in byte_rep:\n",
    "            if num[0] < thr:\n",
    "                byte_rep_thr.append(0)\n",
    "            else:\n",
    "                byte_rep_thr.append(1) # high than threshold, keep it\n",
    "        byte_rep_thr = np.array(byte_rep_thr)\n",
    "        byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "        mul_emb = np.multiply(final_emb,byte_rep_thr)\n",
    "        rep_emb = [] #超過threshold的emb才留下來\n",
    "        for emb in mul_emb:\n",
    "            if (sum(emb) <0) or (sum(emb)>0):\n",
    "                rep_emb.append(emb)\n",
    "        rep_emb_avg = np.average(rep_emb,axis=0) #把自己各api的embedding取平均，應該一定要有，不然就代表這個profile不該屬於這棵tree \n",
    "        try:\n",
    "            if not (rep_emb_avg <0 or rep_emb_avg>0):\n",
    "                no_ind_rep+=1\n",
    "                continue #沒有individual REP\n",
    "        except ValueError: #正常profile\n",
    "            pass\n",
    "        target_avg_emb.append(rep_emb_avg)\n",
    "        target_fam_name.append(recent_fam+'\\t'+recent_tree+'\\t'+'Loner')\n",
    "#         i=i+1\n",
    "#         rep_emb_avg = np.expand_dims(rep_emb_avg,axis=0)\n",
    "#         pickle.dump(file=open(root_dir+recent_fam+'_'+recent_tree+'_'+hash_id+'_'+pid_id+'.pkl','wb'),obj=rep_emb_avg)\n",
    "    print('No ind REP#:',no_ind_rep)\n",
    "    return target_avg_emb,target_fam_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = []\n",
    "target_pid_paths = []\n",
    "for i,path in enumerate(exp_pid_paths):\n",
    "    if (fam_target in path) and (path in topN_path): #topN_path要先跑完loner分析\n",
    "        target_id.append(i)\n",
    "        target_pid_paths.append(path)\n",
    "target_emb_api = exp_api[target_id]\n",
    "target_emb = exp_emb[target_id]\n",
    "assert len(target_id) == len(target_emb_api) == len(target_emb) == len(target_pid_paths)\n",
    "print(target_emb.shape)\n",
    "\n",
    "target_avg_emb,target_fam_name = avg_target_vector_fam(target_pid_paths,target_emb_api,target_emb,thr)\n",
    "assert len(target_avg_emb) == len(target_fam_name)\n",
    "\n",
    "all_avg_emb = []\n",
    "all_avg_emb.extend(train_avg_emb)\n",
    "all_fam_name = []\n",
    "all_fam_name.extend(train_tree_name)\n",
    "assert len(all_avg_emb)+1 == len(all_fam_name)\n",
    "all_avg_emb.extend(target_avg_emb)\n",
    "all_fam_name.extend(target_fam_name)\n",
    "assert len(all_avg_emb)+1 == len(all_fam_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "輸出所需之r vector與metadata label\n",
    "* 記得修改輸出的兩個路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_avg_df = pd.DataFrame(np.array(all_avg_emb))\n",
    "fam_avg_df.to_csv('./data/tree-rep-profiles_o2o/allapleLoner_fam_emb2.tsv',sep='\\t',index=False,header=False)\n",
    "with open (\"./data/tree-rep-profiles_o2o/allapleLoner_fam_emb_meta2.tsv\",\"w\")as fp:\n",
    "    for line in all_fam_name:\n",
    "        fp.write(line+\"\\n\")\n",
    "fam_avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建議使用tool當中的UMAP降維視覺化，設定neighbor為5，2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix Spplementary\n",
    "* 與任務主線無關，只是用來測試的小方法\n",
    "* Deprecated code. Need lots of modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN classification (loner family)\n",
    "* 依照loner所靠近的k個training r來投票決定歸屬哪一類 (然而因為通常都是一大群涵蓋近來，所以最多的不會是自己家族的，而且自己家族的樹量可能training太少所以無法投票成功)(data imbalance不可用)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對每一個family先進行編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loner_dir = './data/tree-rep-profiles_o2o/EXP/' # training Set r\n",
    "fam_exp = next(os.walk(loner_dir))[1]\n",
    "encode_fam = {}\n",
    "for i,v in enumerate(fam_exp):\n",
    "    encode_fam[v] = i\n",
    "pickle.dump(file=open('data/tree-rep-profiles_o2o/encode_fam.pkl','wb'),obj=encode_fam) #備份家族編碼順序\n",
    "encode_fam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 training r 轉換為kNN可用的訓練資料集向量格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data prepare\n",
    "ind_rep_dir = './data/tree-rep-profiles_o2o/profile_avg/' #要有最後的反斜線\n",
    "ind_reps = next(os.walk(ind_rep_dir))[2]\n",
    "kNN_train_Y = []\n",
    "for i,c in enumerate(ind_reps):\n",
    "    ind_rep = pickle.load(open(ind_rep_dir + c,'rb')) #每個tree的centroid pickle\n",
    "    fam_name = c.split('_')[0].split('.')[-1]\n",
    "    for j,v in enumerate(list(encode_fam.keys())):\n",
    "        if fam_name in v:\n",
    "            kNN_train_Y.append(j)\n",
    "            break\n",
    "    if i==0:\n",
    "        kNN_train_X = ind_rep # 1st time\n",
    "    else:\n",
    "        kNN_train_X = np.concatenate((kNN_train_X,ind_rep),axis=0) #依順序合在一起 \n",
    "assert kNN_train_X.shape[0] == len(kNN_train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "轉換驗證資料集為knn可用的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(inf_pid_paths,inf_emb_api,inf_emb,encode_fam=encode_fam,loner_dir=False):\n",
    "    em_up = 0\n",
    "    no_ind_rep = 0\n",
    "    inf_X = []\n",
    "    inf_Y = []\n",
    "    for path,api,emb in tqdm(zip(inf_pid_paths,inf_emb_api,inf_emb)):\n",
    "        if loner_dir:\n",
    "            fam_name = path.split('/')[-3]\n",
    "        else:\n",
    "            fam_name = path.split('/')[-3].split('_')[0].split('.')[-1]\n",
    "        with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines()\n",
    "        length = len(lines)\n",
    "        if length < 1:\n",
    "            print('Empty profile:',path)\n",
    "            continue\n",
    "        try:\n",
    "            assert api[:length][-1] != 0 #確認profile跟所load 近來的vector是一致的\n",
    "            assert api[:length+1][-1] == 0\n",
    "        except:\n",
    "            print('===Assertion ERR:',i,path)\n",
    "            continue\n",
    "        api_e = np.expand_dims(api, axis=0)\n",
    "        emb_e = np.expand_dims(emb, axis=0)\n",
    "        final_emb_e = emb_model.predict([api_e,emb_e]) #(1,length,768)\n",
    "        final_emb = final_emb_e[0][:length] #(length,768)\n",
    "        byte_rep_e = model.predict([api_e,emb_e])\n",
    "        byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "        byte_rep_thr = []\n",
    "        for num in byte_rep:\n",
    "            if num[0] < thr:\n",
    "                byte_rep_thr.append(0)\n",
    "            else:\n",
    "                byte_rep_thr.append(1) # high than threshold, keep it\n",
    "        byte_rep_thr = np.array(byte_rep_thr)\n",
    "        byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "        mul_emb = np.multiply(final_emb,byte_rep_thr)\n",
    "        rep_emb = [] #超過threshold的emb才留下來\n",
    "        for emb in mul_emb:\n",
    "            if (sum(emb) <0) or (sum(emb)>0):\n",
    "                rep_emb.append(emb)\n",
    "        rep_emb_avg = np.average(rep_emb,axis=0)\n",
    "        try:\n",
    "            if not (rep_emb_avg <0 or rep_emb_avg>0):\n",
    "#                 no_ind_rep_hash.append(recent_hash)\n",
    "                continue #沒有individual REP\n",
    "        except ValueError: #正常profile\n",
    "            pass\n",
    "        inf_X.append(rep_emb_avg)\n",
    "        for j,v in enumerate(list(encode_fam.keys())):\n",
    "            if fam_name in v:\n",
    "                inf_Y.append(j)\n",
    "                break        \n",
    "    assert len(inf_X) == len(inf_Y)\n",
    "    return np.array(inf_X),inf_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X , valid_Y = data_prepare(valid_pid_paths,valid_emb_api,valid_emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用驗證資料集決定k要選多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [3,5,7,9,11,13,15,17,19,21,23,25,27,29,31]:\n",
    "    neigh = KNeighborsClassifier(n_neighbors=i,n_jobs=30,weights='distance') #,weights='distance'\n",
    "    neigh.fit(kNN_train_X, kNN_train_Y) \n",
    "    score = neigh.score(valid_X,valid_Y)\n",
    "    print('k=',i,',acc=',score) #選acc最高的k做為最終要使用的k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "檢驗loner set最終的acc效能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_X , exp_Y = data_prepare(exp_pid_paths,exp_api,exp_emb) #轉換為knn格式向量\n",
    "neigh = KNeighborsClassifier(n_neighbors=1311,n_jobs=30) #,weights='distance'\n",
    "neigh.fit(kNN_train_X, kNN_train_Y) \n",
    "score = neigh.score(exp_X,exp_Y)\n",
    "ans = neigh.predict(exp_X)\n",
    "print('最終分類準確率:',score)\n",
    "ans #每一個loner predict的家族編碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BT centroid分類法\n",
    "* 算出train set每個BT中所有profiles r的centroid\n",
    "* 看loner與哪個centroid最靠近，在來看是否為原所屬家族的BT，判斷是否歸類成功\n",
    "* 目前是用euclidean dist.比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先計算每個train BT的centroid並存到root_dir中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_name = None\n",
    "tree_name = None\n",
    "tree_vec = []\n",
    "root_dir = './data/tree-rep-profiles_o2o/centroids/' \n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir,exist_ok=True)\n",
    "for path,api,emb in tqdm(zip(train_pid_paths,train_emb_api,train_emb)):\n",
    "    recent_fam = path.split('/')[-3].split('_')[0]\n",
    "    recent_tree = path.split('/')[-2]\n",
    "    with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "        lines = f.read()\n",
    "    lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "    lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "    lines = re.sub('=\\\\n','',lines)\n",
    "    lines = re.sub('y\\\\n','',lines)\n",
    "    lines = lines.splitlines()\n",
    "    length = len(lines)\n",
    "    if length < 11:\n",
    "        print('No common rep. Should not in a tree:',path)\n",
    "        continue\n",
    "#     print(len(lines))\n",
    "    assert api[:length][-1] != 0\n",
    "    assert api[:length+1][-1] == 0\n",
    "    api_e = np.expand_dims(api, axis=0)\n",
    "    emb_e = np.expand_dims(emb, axis=0)\n",
    "    final_emb_e = emb_model.predict([api_e,emb_e]) #(1,length,768)\n",
    "    final_emb = final_emb_e[0][:length] #(length,768)\n",
    "    byte_rep_e = model.predict([api_e,emb_e])\n",
    "    byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "    byte_rep_thr = []\n",
    "    for num in byte_rep:\n",
    "        if num[0] < thr:\n",
    "            byte_rep_thr.append(0)\n",
    "        else:\n",
    "            byte_rep_thr.append(1) # high than threshold, keep it\n",
    "    byte_rep_thr = np.array(byte_rep_thr)\n",
    "    byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "    mul_emb = np.multiply(final_emb,byte_rep_thr)\n",
    "    rep_emb = [] #超過threshold的emb才留下來\n",
    "    for emb in mul_emb:\n",
    "        if (sum(emb) <0) or (sum(emb)>0):\n",
    "            rep_emb.append(emb)\n",
    "    rep_emb_avg = np.average(rep_emb,axis=0) #把自己各api的embedding取平均，應該一定要有，不然就代表這個profile不該屬於這棵tree\n",
    "    if recent_fam==fam_name and recent_tree==tree_name:\n",
    "        tree_vec.append(rep_emb_avg) #如果同tree同fam就把rep_emb加進去list\n",
    "    else:\n",
    "        if tree_vec:\n",
    "            fam_tree_c = np.average(tree_vec,axis=0) # 求出整個tree的centroid\n",
    "            pickle.dump(file=open(root_dir+fam_name+'_'+tree_name+'.pkl','wb'),obj=fam_tree_c)\n",
    "            tree_vec = []\n",
    "        fam_name = recent_fam\n",
    "        tree_name = recent_tree\n",
    "        if not tree_vec: # if tree vector list is empty\n",
    "            tree_vec.append(rep_emb_avg)\n",
    "#     break\n",
    "#     print(path,api,emb)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training set 歸類\n",
    "* calculate euclidean distance\n",
    "* check if match right tree/fam\n",
    "* top1/top3/top5\n",
    "\n",
    "建議做:\n",
    "\n",
    "* 同sample不同pid的歸類，同分的歸類 (增額錄取)\n",
    "* topN dis<0.2以下的歸類 (topN effective match)\n",
    "* family改成是最近的3/5個family (同family就往下推移)\n",
    "* 把training沒有此tree的DEV pid拿掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_dir = './data/tree-rep-profiles_o2o/centroids/' #要有最後的反斜線。訓練資料集各BT的centroid目錄\n",
    "def inf_evaluate(inf_pid_paths,inf_emb_api,inf_emb,centroid_dir=centroid_dir):\n",
    "    centroids = next(os.walk(centroid_dir))[2]\n",
    "    centroid_matrix = None\n",
    "    c_fam = []\n",
    "    c_tree = []\n",
    "    err_centroid = []\n",
    "    top1_fam_up = 0\n",
    "    top1_tree_up = 0\n",
    "    top3_fam_up = 0\n",
    "    top3_tree_up = 0\n",
    "    top5_fam_up = 0\n",
    "    top5_tree_up = 0\n",
    "    wrong_fam = {}\n",
    "    wrong_tree = {}\n",
    "    no_ind_rep = 0\n",
    "    for i,c in enumerate(centroids):\n",
    "        tree_rep = pickle.load(open(centroid_dir + c,'rb')) #每個tree的centroid pickle\n",
    "        tree_rep = np.expand_dims(tree_rep,axis=0)\n",
    "        try:\n",
    "            chk = sum(sum(tree_rep))\n",
    "        except TypeError:\n",
    "            err_centroid.append(c)\n",
    "            continue\n",
    "        if (not chk < 0) and (not chk>0):\n",
    "            err_centroid.append(c)\n",
    "            continue\n",
    "        if i >0:\n",
    "            centroid_matrix = np.concatenate((centroid_matrix,tree_rep),axis=0) #依順序合在一起       \n",
    "        if i==0:\n",
    "            centroid_matrix = tree_rep # 1st time\n",
    "        c_fam.append(c.split('_')[0]) # centroid的fam name\n",
    "        c_tree.append(c.split('_')[-1].split('.')[0]) # centroid的tree name\n",
    "    assert centroid_matrix.shape[0] == len(c_fam) == len(c_tree)\n",
    "    assert len(inf_pid_paths) == len(inf_emb_api) == len(inf_emb)\n",
    "    for path,api,emb in tqdm(zip(inf_pid_paths,inf_emb_api,inf_emb)):\n",
    "        recent_fam = path.split('/')[-3].split('_')[0]\n",
    "        recent_tree = path.split('/')[-2]\n",
    "        with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines()\n",
    "        length = len(lines)\n",
    "        if length < 1:\n",
    "            print('Empty profile:',path)\n",
    "            continue\n",
    "        assert api[:length][-1] != 0 #確認profile跟所load 近來的vector是一致的\n",
    "        assert api[:length+1][-1] == 0\n",
    "        api_e = np.expand_dims(api, axis=0)\n",
    "        emb_e = np.expand_dims(emb, axis=0)\n",
    "        final_emb_e = emb_model.predict([api_e,emb_e])\n",
    "        final_emb = final_emb_e[0][:length] #(length,768)\n",
    "        byte_rep_e = model.predict([api_e,emb_e])\n",
    "        byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "        byte_rep_thr = []\n",
    "        for num in byte_rep:\n",
    "            if num[0] < thr:\n",
    "                byte_rep_thr.append(0)\n",
    "            else:\n",
    "                byte_rep_thr.append(1) # high than threshold, keep it\n",
    "        byte_rep_thr = np.array(byte_rep_thr)\n",
    "        byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "        mul_emb = np.multiply(final_emb,byte_rep_thr) #把embediing跟超過threshold的byterep相乘  \n",
    "        rep_emb = [] #超過threshold的emb才留下來\n",
    "        for emb in mul_emb:\n",
    "            if sum(emb) !=0: #跟0鄉城都會=0\n",
    "                rep_emb.append(emb)       \n",
    "        rep_emb_avg = np.average(rep_emb,axis=0)\n",
    "        rep_emb_avg = np.expand_dims(rep_emb_avg,axis=0)\n",
    "        try:\n",
    "            if not (rep_emb_avg <0 or rep_emb_avg>0):\n",
    "                no_ind_rep+=1\n",
    "                continue #沒有individual REP\n",
    "        except ValueError: #正常profile\n",
    "            pass\n",
    "        dist = euclidean_distances(rep_emb_avg,centroid_matrix) #取出與各rep距離，求min的idx對答案\n",
    "        min_dist = np.min(dist) #top1\n",
    "        min_idx = np.argmin(dist) #top1\n",
    "        dist5 = dist[0].argsort()[:5] #top3\n",
    "        if c_fam[min_idx] == recent_fam:\n",
    "            top1_fam_up = top1_fam_up+1\n",
    "        else:\n",
    "            wrong_fam[path] = c_fam[min_idx]\n",
    "        if (c_fam[min_idx] == recent_fam) and (c_tree[min_idx] == recent_tree):\n",
    "            top1_tree_up = top1_tree_up+1\n",
    "        else:\n",
    "            wrong_tree[path] = c_fam[min_idx]+'_'+ c_tree[min_idx]\n",
    "        if recent_fam in [c_fam[dist5[0]],c_fam[dist5[1]],c_fam[dist5[2]]]:\n",
    "            top3_fam_up = top3_fam_up+1\n",
    "        if (recent_tree in [c_tree[dist5[0]],c_tree[dist5[1]],c_tree[dist5[2]]]) and (recent_fam in[c_fam[dist5[0]],c_fam[dist5[1]],c_fam[dist5[2]]]):\n",
    "            top3_tree_up = top3_tree_up+1\n",
    "        if recent_fam in [c_fam[dist5[0]],c_fam[dist5[1]],c_fam[dist5[2]],c_fam[dist5[3]],c_fam[dist5[4]]]:\n",
    "            top5_fam_up = top5_fam_up+1\n",
    "        if (recent_fam in [c_fam[dist5[0]],c_fam[dist5[1]],c_fam[dist5[2]],c_fam[dist5[3]],c_fam[dist5[4]]]) and (recent_tree in [c_tree[dist5[0]],c_tree[dist5[1]],c_tree[dist5[2]],c_tree[dist5[3]],c_tree[dist5[4]]]):\n",
    "            top5_tree_up = top5_tree_up + 1\n",
    "    down = len(inf_pid_paths)\n",
    "    print('Top1 Fam score:',top1_fam_up/down,'Top1 Tree score:',top1_tree_up/down,\n",
    "         'Top3 Fam score:',top3_fam_up/down,'Top3 Tree score:',top3_tree_up/down,\n",
    "          'Top5 Fam score:',top5_fam_up/down,'Top5 Tree score:', top5_tree_up/down,\n",
    "          'No ind rep#:',no_ind_rep\n",
    "         )\n",
    "    return wrong_fam,wrong_tree\n",
    "#         for c in centroids:\n",
    "#             tree_rep = pickle.load(open(centroid_dir + c,'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = inf_evaluate(test_pid_paths,test_emb_api,test_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loner set 家族歸類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_dir = './data/tree-rep-profiles_o2o/centroids/' #要有最後的反斜線\n",
    "def loner_evaluate(lon_pid_paths,lon_emb_api,lon_emb,centroid_dir=centroid_dir):\n",
    "    centroids = next(os.walk(centroid_dir))[2]\n",
    "    centroid_matrix = None\n",
    "    c_fam = []\n",
    "    c_tree = []\n",
    "    err_centroid = []\n",
    "    top1_fam_up = 0\n",
    "#     top1_tree_up = 0\n",
    "    top3_fam_up = 0\n",
    "#     top3_tree_up = 0\n",
    "    top5_fam_up = 0\n",
    "#     top5_tree_up = 0\n",
    "    wrong_fam = {}\n",
    "#     wrong_tree = {}\n",
    "    no_ind_rep = 0\n",
    "    for i,c in enumerate(centroids):\n",
    "        tree_rep = pickle.load(open(centroid_dir + c,'rb')) #每個tree的centroid pickle\n",
    "        tree_rep = np.expand_dims(tree_rep,axis=0)\n",
    "        try:\n",
    "            chk = sum(sum(tree_rep))\n",
    "        except TypeError:\n",
    "            err_centroid.append(c)\n",
    "            continue\n",
    "        if (not chk < 0) and (not chk>0):\n",
    "            err_centroid.append(c)\n",
    "            continue\n",
    "        if i >0:\n",
    "            centroid_matrix = np.concatenate((centroid_matrix,tree_rep),axis=0) #依順序合在一起       \n",
    "        if i==0:\n",
    "            centroid_matrix = tree_rep # 1st time\n",
    "        c_fam.append(c.split('_')[0]) # centroid的fam name\n",
    "#         c_tree.append(c.split('_')[-1].split('.')[0]) # centroid的tree name\n",
    "    assert centroid_matrix.shape[0] == len(c_fam) #== len(c_tree)\n",
    "    assert len(lon_pid_paths) == len(lon_emb_api) == len(lon_emb)\n",
    "    for path,api,emb in tqdm(zip(lon_pid_paths,lon_emb_api,lon_emb)):\n",
    "        recent_fam = path.split('/')[-3].split('_')[0]\n",
    "#         recent_tree = path.split('/')[-2]\n",
    "        with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines()\n",
    "        length = len(lines)\n",
    "        if length < 1:\n",
    "            print('Empty profile:',path)\n",
    "            continue\n",
    "        assert api[:length][-1] != 0 #確認profile跟所load 近來的vector是一致的\n",
    "        assert api[:length+1][-1] == 0\n",
    "        api_e = np.expand_dims(api, axis=0)\n",
    "        emb_e = np.expand_dims(emb, axis=0)\n",
    "        final_emb_e = emb_model.predict([api_e,emb_e])\n",
    "        final_emb = final_emb_e[0][:length] #(length,768)\n",
    "        byte_rep_e = model.predict([api_e,emb_e])\n",
    "        byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "        byte_rep_thr = []\n",
    "        for num in byte_rep:\n",
    "            if num[0] < thr:\n",
    "                byte_rep_thr.append(0)\n",
    "            else:\n",
    "                byte_rep_thr.append(1) # high than threshold, keep it\n",
    "        byte_rep_thr = np.array(byte_rep_thr)\n",
    "        byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "        mul_emb = np.multiply(final_emb,byte_rep_thr) #把embediing跟超過threshold的byterep相乘  \n",
    "        rep_emb = [] #超過threshold的emb才留下來\n",
    "        for emb in mul_emb:\n",
    "            if sum(emb) !=0: #跟0鄉城都會=0\n",
    "                rep_emb.append(emb)       \n",
    "        rep_emb_avg = np.average(rep_emb,axis=0)\n",
    "        rep_emb_avg = np.expand_dims(rep_emb_avg,axis=0)\n",
    "        try:\n",
    "            if not (rep_emb_avg <0 or rep_emb_avg>0):#沒有individual REP\n",
    "                no_ind_rep+=1\n",
    "                continue \n",
    "        except ValueError: #正常profile\n",
    "            pass\n",
    "        dist = euclidean_distances(rep_emb_avg,centroid_matrix) #取出與各rep距離，求min的idx對答案\n",
    "        min_dist = np.min(dist) #top1\n",
    "        min_idx = np.argmin(dist) #top1\n",
    "        dist5 = dist[0].argsort()[:5] #top3\n",
    "        if c_fam[min_idx] == recent_fam:\n",
    "            top1_fam_up = top1_fam_up+1\n",
    "        else:\n",
    "            wrong_fam[path] = c_fam[min_idx]\n",
    "#         if (c_fam[min_idx] == recent_fam) and (c_tree[min_idx] == recent_tree):\n",
    "#             top1_tree_up = top1_tree_up+1\n",
    "#         else:\n",
    "#             wrong_tree[path] = c_fam[min_idx]+'_'+ c_tree[min_idx]\n",
    "        if recent_fam in [c_fam[dist5[0]],c_fam[dist5[1]],c_fam[dist5[2]]]:\n",
    "            top3_fam_up = top3_fam_up+1\n",
    "#         if (recent_tree in [c_tree[dist5[0]],c_tree[dist5[1]],c_tree[dist5[2]]]) and (recent_fam in[c_fam[dist5[0]],c_fam[dist5[1]],c_fam[dist5[2]]]):\n",
    "#             top3_tree_up = top3_tree_up+1\n",
    "        if recent_fam in [c_fam[dist5[0]],c_fam[dist5[1]],c_fam[dist5[2]],c_fam[dist5[3]],c_fam[dist5[4]]]:\n",
    "            top5_fam_up = top5_fam_up+1\n",
    "#         if (recent_fam in [c_fam[dist5[0]],c_fam[dist5[1]],c_fam[dist5[2]],c_fam[dist5[3]],c_fam[dist5[4]]]) and (recent_tree in [c_tree[dist5[0]],c_tree[dist5[1]],c_tree[dist5[2]],c_tree[dist5[3]],c_tree[dist5[4]]]):\n",
    "#             top5_tree_up = top5_tree_up + 1\n",
    "    down = len(lon_pid_paths)\n",
    "    print('Top1 Fam score:',top1_fam_up/down,#'Top1 Tree score:',top1_tree_up/down,\n",
    "         'Top3 Fam score:',top3_fam_up/down,#'Top3 Tree score:',top3_tree_up/down,\n",
    "          'Top5 Fam score:',top5_fam_up/down,#'Top5 Tree score:', top5_tree_up/down,\n",
    "          'No ind rep#:',no_ind_rep\n",
    "         )\n",
    "    return wrong_fam\n",
    "#         for c in centroids:\n",
    "#             tree_rep = pickle.load(open(centroid_dir + c,'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = loner_evaluate(exp_pid_paths,exp_api,exp_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
