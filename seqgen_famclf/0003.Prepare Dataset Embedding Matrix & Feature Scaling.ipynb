{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,glob\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "from keras.utils import *\n",
    "from keras.utils.generic_utils import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.preprocessing.image import *\n",
    "from multiprocessing import *\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.manifold import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn import preprocessing\n",
    "import sent2vec\n",
    "import re\n",
    "import string\n",
    "import unicodedata as udata\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load trained Sent2Vec model & predefined api_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('model/o2o_o2m_Sent2Vec_lower_woParam_0616_768.bin') # sent2vec model\n",
    "dil= r\"[,.;\\-+^()/@#?!&$:{}\\\\*%~\\'\\\"\\=\\_]+\\ *\" #等號、底線被保留，注意要跟先前一致\n",
    "# all_df = pd.read_csv('./data/tree-rep-profiles-partial/process2family_df.csv')\n",
    "api_li = ['LoadLibrary',\n",
    "'CreateProcess',\n",
    "'OpenProcess',\n",
    "'ExitProcess',\n",
    "'TerminateProcess',\n",
    "'WinExec',\n",
    "'CreateRemoteThread',\n",
    "'CreateThread',\n",
    "'CopyFile',\n",
    "'CreateFile',\n",
    "'DeleteFile',\n",
    "'RegSetValue',\n",
    "'RegCreateKey',\n",
    "'RegDeleteKey',\n",
    "'RegDeleteValue',\n",
    "'RegQueryValue',\n",
    "'RegEnumValue',\n",
    "'WinHttpConnect',\n",
    "'WinHttpOpen',\n",
    "'WinHttpOpenRequest',\n",
    "'WinHttpReadData',\n",
    "'WinHttpSendRequest',\n",
    "# 'WinHttpWriteData', #少了\n",
    "'InternetOpen',\n",
    "'InternetConnect',\n",
    "'HttpSendRequest',\n",
    "'GetUrlCacheEntryInfo']\n",
    "api_li = [x.lower() for x in api_li] #lowrer case?\n",
    "len(api_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dict = {}\n",
    "for i,v in enumerate(api_li):\n",
    "    encode_dict[v] = i+1\n",
    "pickle.dump(file=open('data/tree-rep-profiles_o2o/encode_dict.pkl','wb'),obj=encode_dict) #儲存每個API function name的編碼\n",
    "encode_dict\n",
    "# encode_dict = pickle.load(open('data/tree-rep-profiles-partial/encode_dict.pkl','rb'))\n",
    "# encode_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data vectors\n",
    "* 獲取每一個資料集(train/valid/test/loner Set)execution profiles的api function name 整數對照encoding (train_api/valid_api/test_api/exp_api)\n",
    "* 整個API invocation calls的Sen2Vec embedding vectors (train_emb/valid_emb/test_emb/exp_emb)\n",
    "* 每一筆profile對應的路徑位置(依照順序): train_pid_paths/valid_pid_paths/test_pid_paths/exp_pid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sent2vec_emb(root_dir,max_length=226,model=model,dim=768,encode_dict=encode_dict,dil=dil,train_only=False):\n",
    "    # dim = 768\n",
    "    '''\n",
    "    Input: \n",
    "    root_dir=>資料集目錄\n",
    "    max_length=>資料集中execution profile所含API invocation calls最大長度\n",
    "    model=> Sent2Vec trained model\n",
    "    dim=>想要embed成的vector維度大小\n",
    "    '''\n",
    "\n",
    "    dev_pids_path = [] #儲存profile順序?\n",
    "    \n",
    "    sent_pad = [0]*dim\n",
    "    # encode_dict = encode_dict_lower\n",
    "    all_profiles = []\n",
    "    all_api_name = []\n",
    "    all_fam_ans = []\n",
    "    all_byterep=[]\n",
    "    hl_rep = []\n",
    "    sen2vec_length_normalize = []\n",
    "    sen2vec_dim_normalize = []\n",
    "    loner = False\n",
    "    if root_dir.split('/')[-2] in 'EXP2': #請自行修改loner的資料夾名稱!!或是如果是再跑loner的要把這個flag改成True\n",
    "        loner = True\n",
    "#     for rasmma in rasmma_dir:\n",
    "    fam_dir = next(os.walk(root_dir))[1]\n",
    "    for fam in tqdm(fam_dir):\n",
    "        tree_dir = next(os.walk(root_dir +fam))[1]\n",
    "        for tree in tree_dir:\n",
    "#             if train_only:\n",
    "#                 dest_dir = test_dir + fam + '/' + tree #r檢查此dir是否存在於TEST當中\n",
    "#                 if not os.path.exists(dest_dir):\n",
    "#                     continue\n",
    "            in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            for hl_f in hl_list:\n",
    "                profile_emb = [] #整個api invocation call\n",
    "                func_name_emb = [] #僅有api function name\n",
    "                if not loner:\n",
    "                    byterep = hl_f.replace('.profile','_byterep.pickle') #拿出hl_f所對應的byterep\n",
    "                    rep = pickle.load(open(byterep,'rb')) #list type\n",
    "                    if len(rep) < max_length: #Y1\n",
    "                        for _ in range(max_length-len(rep)):\n",
    "                            rep.append(0)\n",
    "                with open(hl_f,encoding='ISO 8859-1') as f: #X2\n",
    "                    lines = f.read()\n",
    "                lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "                lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "                lines = re.sub('=\\\\n','',lines)\n",
    "                lines = re.sub('y\\\\n','',lines)\n",
    "                lines = lines.splitlines()\n",
    "                if len(lines) > max_length:\n",
    "                    print(\"ERR length too long:\",hl_f,'=>',len(lines))\n",
    "                    continue\n",
    "                dev_pids_path.append(hl_f)\n",
    "                for line in lines:\n",
    "                    temp = re.sub(dil,\" \",line.lower()) # lower? 跟先前一致\n",
    "                    temp = temp.split(\" \")\n",
    "                    temp = list(filter(None, temp))\n",
    "                    temp = ' '.join(temp)\n",
    "                    func_name = temp.split(' ')[0] # X1\n",
    "                    if func_name not in api_li:\n",
    "                        print('=ERROR:=',hl_f,'=>',temp)\n",
    "                    func_name_emb.append(encode_dict[func_name]) # encode証術數數字\n",
    "                    emb = model.embed_sentence(temp)\n",
    "                    emb = emb[0]\n",
    "                    profile_emb.append(emb) #放入embdding，句子=>profile\n",
    "                    sen2vec_dim_normalize.append(emb) #dim norm用\n",
    "                sen2vec_length_normalize.append(len(lines))\n",
    "                if len(lines) < max_length:\n",
    "                    for _ in range(max_length-len(lines)):\n",
    "                        profile_emb.append(sent_pad) #sent2vec vectors padding\n",
    "                if not loner:\n",
    "                    hl_rep.append(rep) #Y1\n",
    "                all_profiles.append(profile_emb) #X2\n",
    "                all_api_name.append(func_name_emb) #X1\n",
    "    #             all_byterep.extend(rep_list)\n",
    "    all_api_name = pad_sequences(all_api_name,maxlen=max_length,padding='post',value=0) #X1 padding for API function name\n",
    "\n",
    "    if not loner:\n",
    "        return np.array(all_api_name) , np.array(all_profiles) , np.array(hl_rep),np.array(sen2vec_dim_normalize) ,sen2vec_length_normalize, dev_pids_path\n",
    "    else:\n",
    "        return np.array(all_api_name) , np.array(all_profiles) ,np.array(sen2vec_dim_normalize) ,sen2vec_length_normalize,dev_pids_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './data/tree-rep-profiles_o2o/normal/' # Train Set位置\n",
    "train_api,train_emb, train_rep, train_normalize , train_len_norm,train_pid_paths =   Sent2vec_emb(train_dir)\n",
    "print(train_api.shape,train_emb.shape,train_rep.shape)\n",
    "valid_dir = './data/tree-rep-profiles_o2o/DEV/' # Valid Set位置\n",
    "valid_api,valid_emb, valid_rep, valid_normalize , valid_len_norm,valid_pid_paths =   Sent2vec_emb(valid_dir)\n",
    "print(valid_api.shape,valid_emb.shape,valid_rep.shape)\n",
    "test_dir = './data/tree-rep-profiles_o2o/TEST/' # Test Set 位置\n",
    "test_api,test_emb, test_rep, test_normalize , test_len_norm,test_pid_paths =   Sent2vec_emb(test_dir)\n",
    "print(test_api.shape,test_emb.shape,test_rep.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記得要先把上面function中前面的loner flag改成True\n",
    "exp_dir = './data/tree-rep-profiles_o2o/EXP2/' #loner Set dir位置\n",
    "exp_api,exp_emb, exp_normalize , exp_len_norm,exp_pid_paths =   Sent2vec_emb(exp_dir)\n",
    "print(exp_api.shape,exp_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "* 僅針對Sent2Vec vectors\n",
    "* Z-score normalization by each dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算每個dimension的平均值跟標準差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = np.concatenate([train_normalize,valid_normalize,test_normalize])\n",
    "mean = np.mean(alls,axis=0)\n",
    "std = np.std(alls,axis=0)\n",
    "# mean.shape , std.shape\n",
    "# 建議把mean跟std也存成pickle，方便以後可以沿用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使每個資料集的平均值=0，變異數=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dim(emb,length,mean=mean,std=std):\n",
    "    sent_emb = (emb[:length,:] - mean)/std\n",
    "    final_emb = np.concatenate([sent_emb, emb[length:,:]],axis=0)\n",
    "    return final_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb_norm = []\n",
    "valid_emb_norm = []\n",
    "train_only_emb_norm = [] #pahse2\n",
    "exp_emb_norm = []\n",
    "o2o_emb_norm = [] #phase1 for encoder\n",
    "for emb,length in zip(train_emb,train_len_norm):\n",
    "    emb_norm = normalize_dim(emb,length)\n",
    "    train_only_emb_norm.append(emb_norm)\n",
    "for emb,length in zip(valid_emb,valid_len_norm):\n",
    "    emb_norm = normalize_dim(emb,length)\n",
    "    valid_emb_norm.append(emb_norm)\n",
    "for emb,length in zip(test_emb,test_len_norm):\n",
    "    emb_norm = normalize_dim(emb,length)\n",
    "    test_emb_norm.append(emb_norm)\n",
    "for emb,length in zip(exp_emb,exp_len_norm):\n",
    "    emb_norm = normalize_dim(emb,length)\n",
    "    exp_emb_norm.append(emb_norm)\n",
    "\n",
    "    \n",
    "train_only_emb = np.array(train_only_emb_norm)\n",
    "valid_emb = np.array(valid_emb_norm)\n",
    "test_emb = np.array(test_emb_norm)\n",
    "exp_emb = np.array(exp_emb_norm)\n",
    "\n",
    "print(train_only_emb.shape,valid_emb.shape,test_emb.shape,exp_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把最後每個資料集所得到的embedding vector以及每個profile的路徑位置存成pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/tree-rep-profiles_o2o/'\n",
    "pickle.dump(file=open(root_dir + 'pids_train.pkl','wb')\n",
    "            ,obj=(train_api , train_emb , train_rep),protocol=4)\n",
    "pickle.dump(file=open(root_dir + 'pids_valid.pkl','wb')\n",
    "            ,obj=(valid_api,valid_emb,valid_rep),protocol=4)\n",
    "pickle.dump(file=open(root_dir + 'pids_test.pkl','wb')\n",
    "            ,obj=(test_api,test_emb,test_rep),protocol=4)\n",
    "pickle.dump(file=open(root_dir + 'pids_exp.pkl','wb')\n",
    "            ,obj=(exp_api,exp_emb),protocol=4)\n",
    "\n",
    "\n",
    "pickle.dump(file=open(root_dir + 'train_pid_paths.pkl','wb'),obj=train_pid_paths)\n",
    "pickle.dump(file=open(root_dir + 'valid_pid_paths.pkl','wb'),obj=valid_pid_paths)\n",
    "pickle.dump(file=open(root_dir + 'test_pid_paths.pkl','wb'),obj=test_pid_paths)\n",
    "pickle.dump(file=open(root_dir + 'exp_pid_paths.pkl','wb'),obj=exp_pid_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix Spplementary\n",
    "* 與任務主線無關，只是測試用的小工具\n",
    "* Deprecated code. Need lots of modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若不想要所有家族都拿進去train，只想拿有在dev/test Set中的家族來訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Encoder: train_only=True\n",
    "train_dir = './data/tree-rep-profiles_o2o/normal/'\n",
    "# test_dir = '' #測試資料集目錄需指定\n",
    "train_only_api,train_only_emb, train_only_rep, train_only_normalize , train_only_len_norm,train_only_pid_paths =   Sent2vec_emb(train_dir,train_only=True)\n",
    "print(train_only_api.shape,train_only_emb.shape,train_only_rep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "針對過小過短的行為樹也進行編碼轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 會噴ERR是正常的\n",
    "small_short_dir = './data/tree-rep-profiles_o2o/small_short/'\n",
    "small_short_api,small_short_emb, small_short_rep, small_short_normalize , small_short_len_norm,small_short_pid_paths =   Sent2vec_emb(small_short_dir)\n",
    "print(small_short_api.shape,small_short_emb.shape,small_short_rep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若想合併短跟小的行為樹到原本做出來的train data當中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put small_short into train data?\n",
    "o2o_api = np.concatenate((train_api,small_short_api),axis=0)\n",
    "o2o_emb = np.concatenate((train_emb,small_short_emb),axis=0)\n",
    "o2o_rep = np.concatenate((train_rep,small_short_rep),axis=0)\n",
    "o2o_normalize = np.concatenate((train_normalize,small_short_normalize),axis=0)\n",
    "o2o_len_norm = train_len_norm\n",
    "o2o_len_norm.extend(small_short_len_norm)\n",
    "o2o_pid_paths = train_pid_paths\n",
    "o2o_pid_paths.extend(small_short_pid_paths)\n",
    "assert len(o2o_api) == len(o2o_emb) == len(o2o_rep) == len(o2o_pid_paths) == len(o2o_len_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training o2o normalize & store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb,length in zip(o2o_emb,o2o_len_norm):\n",
    "    emb_norm = normalize_dim(emb,length)\n",
    "    o2o_emb_norm.append(emb_norm)\n",
    "o2o_emb = np.array(o2o_emb_norm)\n",
    "\n",
    "pickle.dump(file=open(root_dir + 'pids_o2o.pkl','wb')\n",
    "            ,obj=(o2o_api,o2o_emb,o2o_rep),protocol=4)\n",
    "pickle.dump(file=open(root_dir + 'o2o_pid_paths.pkl','wb'),obj=o2o_pid_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
