{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys,random, pickle\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "from keras import regularizers, metrics\n",
    "from keras.models import Model\n",
    "# noinspection PyPep8Naming\n",
    "from keras import backend as K\n",
    "from keras import optimizers, callbacks, losses\n",
    "from keras.layers import * #Input, Softmax, Embedding, Add, Lambda, Dense, Multiply, Bidirectional, CuDNNGRU, CuDNNLSTM,LSTM,BatchNormalization,Concatenate\n",
    "from keras.activations import hard_sigmoid, sigmoid\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from keras_transformer.extras import ReusableEmbedding, TiedOutputEmbedding\n",
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "from keras_transformer.bert import (\n",
    "    BatchGeneratorForBERT, masked_perplexity,\n",
    "    MaskedPenalizedSparseCategoricalCrossentropy)\n",
    "\n",
    "import keras_metrics as km\n",
    "from keras_trans_mask import RemoveMask, RestoreMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import transformer_bert_model\n",
    "from bpe import BPEEncoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test of sent2vec vector: (272, 213, 700) (272, 213) (272, 87) (272, 213, 1)\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_emb_api, train_fam_ans, train_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/TRAIN_vec.pkl','rb'))\n",
    "valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/DEV_vec.pkl','rb'))\n",
    "test_emb, test_emb_api,test_fam_ans,test_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/TEST_vec.pkl','rb'))\n",
    "# print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "# print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "train_rep_ans = np.expand_dims(train_rep_ans,axis=-1)\n",
    "valid_rep_ans = np.expand_dims(valid_rep_ans,axis=-1)\n",
    "test_rep_ans = np.expand_dims(test_rep_ans,axis=-1)\n",
    "print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_fam_ans.shape,test_rep_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, X2 ,X3,X4):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], X2[randomize],X3[randomize],X4[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train of sent2vec vector: (11925, 213, 700) (11925, 213) (11925, 87) (11925, 213, 1)\n",
      "valid of sent2vec vector: (336, 213, 700) (336, 213) (336, 87) (336, 213, 1)\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_emb_api, train_fam_ans, train_rep_ans = _shuffle(train_emb, train_emb_api, train_fam_ans, train_rep_ans)\n",
    "valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans = _shuffle(valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans)\n",
    "# test_emb, test_emb_api,test_fam_ans,test_rep_ans  = _shuffle(test_emb,test_emb_api,test_fam_ans,test_rep_ans)\n",
    "print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "# print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_fam_ans.shape,test_rep_ans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n",
      "(336, 213, 700)\n"
     ]
    }
   ],
   "source": [
    "# scale = 'no'\n",
    "\n",
    "def scaling(trainX,validX,testX,scale='min_max'):\n",
    "#     if scale == 'min_max':\n",
    "    max_value = max([np.max(trainX) , np.max(validX),np.max(testX)])\n",
    "    min_value = min([np.min(trainX),np.min(validX),np.min(testX)])\n",
    "\n",
    "    trainX = (trainX - min_value) / (max_value - min_value)\n",
    "    validX = (validX - min_value) / (max_value - min_value )\n",
    "    testX = (testX - min_value) / (max_value - min_value )\n",
    "    print(np.max(trainX),np.max(validX))\n",
    "    return trainX,validX,testX , max_value , min_value\n",
    "\n",
    "train_emb,valid_emb,test_emb , max_value,min_value = scaling(train_emb,valid_emb,test_emb)   \n",
    "print(valid_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many dense output: (MML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def change2MML(np_array):\\n    temp = np.expand_dims(np_array,axis=-1)\\n    temp = np.swapaxes(temp,0,1)\\n    return temp\\ntrain_fam_ans = change2MML(train_fam_ans)\\nvalid_fam_ans = change2MML(valid_fam_ans)\\n# test_fam_ans = change2MML(test_fam_ans)\\nprint(valid_fam_ans.shape)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def change2MML(np_array):\n",
    "    temp = np.expand_dims(np_array,axis=-1)\n",
    "    temp = np.swapaxes(temp,0,1)\n",
    "    return temp\n",
    "train_fam_ans = change2MML(train_fam_ans)\n",
    "valid_fam_ans = change2MML(valid_fam_ans)\n",
    "# test_fam_ans = change2MML(test_fam_ans)\n",
    "print(valid_fam_ans.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = [[1,0],[0,0],[1,1]]\n",
    "# temp = np.array(temp)\n",
    "# temp = np.expand_dims(temp,axis=-1)\n",
    "# print(temp.shape)\n",
    "# # temp = temp.reshape(2,3,1)\n",
    "# # temp = np.swapaxes(temp,0,1)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize = np.arange(len(temp))\n",
    "# np.random.shuffle(randomize)\n",
    "# temp[randomize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型參數\n",
    "l2_reg_penalty = 1e-4#1e-4\n",
    "embedding_dropout = 0.6\n",
    "transformer_dropout = 0.1\n",
    "use_universal_transformer = True #true=>ACT\n",
    "\n",
    "transformer_depth = 2\n",
    "vocabulary_size = 26 #api name種類\n",
    "max_seq_length = test_emb.shape[1] # profile最大長度\n",
    "word_embedding_size = test_emb.shape[2]#被除數，跟Sen2Vec最終維度相同\n",
    "num_heads = 2#除數，要整除\n",
    "fam_num = test_fam_ans.shape[1]#test_fam_ans.shape[1] MML\n",
    "batch_size = 256 #128\n",
    "\n",
    "CONFIDENCE_PENALTY = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids1 = Input(shape=(max_seq_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\n",
    "sent_ids = Masking(mask_value=0)(sent_ids1)\n",
    "#shape=(max_seq_length,),batch_shape=(batch_size,max_seq_length)\n",
    "sentemb1 = Input(shape=(max_seq_length,word_embedding_size),name='sent_emb') #輸入Sent2Vec的embeeding (長度,維度大小)\n",
    "sentemb = Masking(mask_value=0)(sentemb1)\n",
    "#shape=(max_seq_length,word_embedding_size),,batch_shape=(batch_size,max_seq_length,word_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model function\n",
    "l2_regularizer = (regularizers.l2(l2_reg_penalty) if l2_reg_penalty else None)\n",
    "\n",
    "add_segment_layer = Add(name='add_segment')\n",
    "\n",
    "multiply_embedding_layer = Multiply(name='mul_emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding function\n",
    "embedding_layer = ReusableEmbedding( # 希望可以跟model一起學的embedding\n",
    "        vocabulary_size, word_embedding_size, #emb size\n",
    "        input_length=max_seq_length,\n",
    "        name='bpe_embeddings',\n",
    "        # Regularization is based on paper \"A Comparative Study on\n",
    "        # Regularization Strategies for Embedding-based Neural Networks\"\n",
    "        # https://arxiv.org/pdf/1508.03721.pdf\n",
    "        embeddings_regularizer=l2_regularizer,trainable=True)\n",
    "# 一個sentence一個ID\n",
    "# segment_embedding_layer = Embedding(vocabulary_size ,word_embedding_size,weights=[emb_matrix]\n",
    "#                                    input_length=max_seq_length , name='segment_embeddings')  #利用pre-train sent2vec vector，跟前面的加在一起\n",
    "\n",
    "coordinate_embedding_layer = TransformerCoordinateEmbedding(\n",
    "        transformer_depth , name='coordinate_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "next_step_input_emb, embedding_matrix = embedding_layer(sent_ids)\n",
    "# segment_embeddings = segment_embedding_layer(sent_ids) # 需增加pretrained Sent2Vec的embedding matrix\n",
    "segment_embeddings = sentemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = TiedOutputEmbedding(\n",
    "        projection_regularizer=l2_regularizer,\n",
    "        projection_dropout=embedding_dropout,\n",
    "        name='word_prediction_logits')\n",
    "\n",
    "output_softmax_layer = Softmax(name='fam_predictions') # 2nf stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_input1 = RemoveMask()(segment_embeddings)\n",
    "next_step_input = coordinate_embedding_layer(next_step_input1, step=0) #next_step_input_emb\n",
    "next_step_input= RestoreMask()([next_step_input,next_step_input1])\n",
    "next_step_input = add_segment_layer([next_step_input, next_step_input_emb]) #segment_embeddings\n",
    "# next_step_input = Concatenate()([next_step_input,next_step_input_emb])\n",
    "for i in range(transformer_depth):\n",
    "    next_step_input1 = RemoveMask()(next_step_input)\n",
    "    next_step_input = (\n",
    "        TransformerBlock(\n",
    "            name='transformer' + str(i), num_heads=num_heads,\n",
    "            residual_dropout=transformer_dropout,\n",
    "            attention_dropout=transformer_dropout,\n",
    "            use_masking=False,  # Allow bi-directional attention\n",
    "            vanilla_wiring=True)\n",
    "        (next_step_input1))\n",
    "    next_step_input = RestoreMask()([next_step_input,next_step_input1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_layer = TransformerACT(\n",
    "#             name='adaptive_computation_time')\n",
    "\n",
    "# transformer_block = TransformerBlock(\n",
    "#             name='transformer', num_heads=num_heads,\n",
    "#             residual_dropout=transformer_dropout,\n",
    "#             attention_dropout=transformer_dropout,\n",
    "#             # Allow bi-directional attention\n",
    "#             use_masking=False)\n",
    "\n",
    "# act_output = next_step_input\n",
    "\n",
    "# for i in range(transformer_depth):\n",
    "#             next_step_input = coordinate_embedding_layer(\n",
    "#                 next_step_input, step=i)\n",
    "#             next_step_input = add_segment_layer(\n",
    "#                 [next_step_input, segment_embeddings])\n",
    "#             next_step_input1 = RemoveMask()(next_step_input)\n",
    "#             next_step_input2 = transformer_block(next_step_input1)\n",
    "# #             next_step_input = RestoreMask()([next_step_input2,next_step_input1])\n",
    "#             next_step_input, act_output = act_layer(next_step_input2)\n",
    "#             next_step_input = RestoreMask()([next_step_input,next_step_input1])\n",
    "\n",
    "# act_layer.finalize()\n",
    "# next_step_input = act_output\n",
    "# next_step_input = RestoreMask()([next_step_input,act_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"out={}\\nfor i in range(fam_num):\\n    out[i] = Dense(1,activation='sigmoid',name='family'+str(i))(family_prediction)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trm_output = next_step_input #output_layer([next_step_input, embedding_matrix]) ?\n",
    "family_input = output_layer([next_step_input1, embedding_matrix]) #embedding_matrix，\n",
    "family_input1 = RestoreMask()([family_input,next_step_input1]) #MASK\n",
    "next_step_input = BatchNormalization()(next_step_input)\n",
    "class_prediction = (\n",
    "        Dense(1, name='0_1_prediction', activation=hard_sigmoid) #sigmoid?hard_sigmoid\n",
    "        (next_step_input)) #輸出重要性，如果要用softmax就會變成Dense(2)\n",
    "\n",
    "family_input = multiply_embedding_layer([class_prediction,next_step_input]) # 2nd stage next_step_input/family_input\n",
    "# family_input = BatchNormalization()(family_input)\n",
    "family_prediction = Bidirectional(LSTM(fam_num,kernel_regularizer=l2_regularizer,\n",
    "                                            recurrent_regularizer=l2_regularizer,return_sequences=True,\n",
    "                                           dropout=transformer_dropout, #stateful=True,\n",
    "                                            recurrent_dropout=transformer_dropout,name='fam_feature'))(family_input)\n",
    "family_prediction = Concatenate()([family_prediction,family_input])\n",
    "family_prediction = BatchNormalization()(family_prediction)\n",
    "family_prediction = (\n",
    "        Bidirectional(GRU(fam_num,name='fam_clf'))(family_prediction) #int((word_embedding_size+vocabulary_size)/4)\n",
    ") # 2nd stage，變成bidirectional加dense?，變成兩層?加L2\n",
    "family_prediction = Dense(fam_num,activation='sigmoid',name='family')(family_prediction)\n",
    "#MML\n",
    "'''out={}\n",
    "for i in range(fam_num):\n",
    "    out[i] = Dense(1,activation='sigmoid',name='family'+str(i))(family_prediction)'''\n",
    "# family_prediction = output_softmax_layer(family_prediction) # 2nd stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_emb (InputLayer)           (None, 10, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 10, 100)      0           sent_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "remove_mask_1 (RemoveMask)      (None, 10, 100)      0           masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sent_ids (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coordinate_embedding (Transform (None, 10, 100)      1200        remove_mask_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 10)           0           sent_ids[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "restore_mask_1 (RestoreMask)    (None, 10, 100)      0           coordinate_embedding[0][0]       \n",
      "                                                                 remove_mask_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bpe_embeddings (ReusableEmbeddi [(None, 10, 100), (2 2600        masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_segment (Add)               (None, 10, 100)      0           restore_mask_1[0][0]             \n",
      "                                                                 bpe_embeddings[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "remove_mask_2 (RemoveMask)      (None, 10, 100)      0           add_segment[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_self_attention (Mu (None, 10, 100)      40000       remove_mask_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_dropout (Dropout)  (None, 10, 100)      0           transformer0_self_attention[0][0]\n",
      "                                                                 transformer0_transition[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_add (Add)          (None, 10, 100)      0           remove_mask_2[0][0]              \n",
      "                                                                 transformer0_dropout[0][0]       \n",
      "                                                                 transformer0_normalization1[0][0]\n",
      "                                                                 transformer0_dropout[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_normalization1 (La (None, 10, 100)      200         transformer0_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "transformer0_transition (Transf (None, 10, 100)      80500       transformer0_normalization1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer0_normalization2 (La (None, 10, 100)      200         transformer0_add[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "restore_mask_2 (RestoreMask)    (None, 10, 100)      0           transformer0_normalization2[0][0]\n",
      "                                                                 remove_mask_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "remove_mask_3 (RemoveMask)      (None, 10, 100)      0           restore_mask_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "transformer1_self_attention (Mu (None, 10, 100)      40000       remove_mask_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "transformer1_dropout (Dropout)  (None, 10, 100)      0           transformer1_self_attention[0][0]\n",
      "                                                                 transformer1_transition[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "transformer1_add (Add)          (None, 10, 100)      0           remove_mask_3[0][0]              \n",
      "                                                                 transformer1_dropout[0][0]       \n",
      "                                                                 transformer1_normalization1[0][0]\n",
      "                                                                 transformer1_dropout[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "transformer1_normalization1 (La (None, 10, 100)      200         transformer1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "transformer1_transition (Transf (None, 10, 100)      80500       transformer1_normalization1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer1_normalization2 (La (None, 10, 100)      200         transformer1_add[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "restore_mask_3 (RestoreMask)    (None, 10, 100)      0           transformer1_normalization2[0][0]\n",
      "                                                                 remove_mask_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 10, 100)      400         restore_mask_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "0_1_prediction (Dense)          (None, 10, 1)        101         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mul_emb (Multiply)              (None, 10, 100)      0           0_1_prediction[0][0]             \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 10, 174)      130848      mul_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 274)      0           bidirectional_1[0][0]            \n",
      "                                                                 mul_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10, 274)      1096        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 174)          188964      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "family (Dense)                  (None, 87)           15225       bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 582,234\n",
      "Trainable params: 578,886\n",
      "Non-trainable params: 3,348\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = Model(inputs = [sent_ids,sentemb],outputs=[class_prediction]) #1st stage\n",
    "#MML\n",
    "'''outs = []\n",
    "for i in range(fam_num):\n",
    "    outs.append(out[i])\n",
    "model = Model(inputs = [sent_ids1,sentemb1],outputs=[class_prediction]+outs) # 1st stage fine-tune'''\n",
    "# model = Model(inputs = [sent_ids1,sentemb1],outputs=[class_prediction,family_prediction])\n",
    "model = Model(inputs = [sent_ids1,sentemb1],outputs=[family_prediction]) #2nd stage\n",
    "model = multi_gpu_model(model , gpus=3) #如果不能儲存multiGPU model，只能single GPU，那LSTM就改成stateful=True\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original model archietecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    single_model = model.layers[-3]\n",
    "    single_model.summary()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MTL:\n",
    "https://datascience.stackexchange.com/questions/28827/multi-task-learning-architecture-for-multi-label-classification\n",
    "* customized loss\n",
    "* f1_loss\n",
    "* feature scaling\n",
    "* concate api vector\n",
    "* imbalance learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics & loss & save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def full_multi_label_metric(y_true, y_pred):\n",
    "    comp = K.equal(y_true, K.round(y_pred))\n",
    "    return K.cast(K.all(comp, axis=-1), K.floatx())\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "# from sklearn.metrics import f1_score\n",
    "# def f1_sk(y_true,y_pred):\n",
    "#     score = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "#     return score\n",
    "# model.load_weights('./model/trm_encoder/1stStage_87fam_0524.h5')\n",
    "# 訓練參數\n",
    "los = [losses.binary_crossentropy,f1_loss] # 1st stage. \n",
    "#SINGLE\n",
    "los = [binary_focal_loss(gamma=2., alpha=.25)]\n",
    "#MML\n",
    "'''los = []\n",
    "for i in range(fam_num):\n",
    "    los.append(binary_focal_loss(alpha=.25, gamma=2))\n",
    "los = [losses.binary_crossentropy] + los'''\n",
    "\n",
    "\n",
    "metric = {'RasMMA': 'acc','family': f1} # 1st stage. km.f1_score()\n",
    "#SINGLE\n",
    "metric = {'family': f1_metric}\n",
    "#MML\n",
    "'''metrics = []\n",
    "for i in range(fam_num+1):\n",
    "    metrics.append('acc')\n",
    "# metrics = {}\n",
    "# metrics['RasMMA'] = 'acc'\n",
    "# for i in range(fam_num):\n",
    "#     metrics['fam'+str(i)]='acc'\n",
    "metric = metrics'''\n",
    "\n",
    "\n",
    "loss_weight = [0.01,1] #stage1 0.95,0.05  #1st stage # 2nd stage [0.01,0.99]\n",
    "#SINGLE\n",
    "loss_weight = [1]\n",
    "#MML\n",
    "'''loss_weight = []\n",
    "for i in range(fam_num):\n",
    "    loss_weight.append(0.95)\n",
    "loss_weight = [0.05] + loss_weight'''\n",
    "\n",
    "learning_rate = 2e-4 # 2nd stage: 1e-4 @1st:2e-4\n",
    "batch_size = batch_size #32 #128\n",
    "\n",
    "num_epochs = 1000\n",
    "patien = 50\n",
    "\n",
    "model_save_path = './model/trm_encoder/1stStage_87fam_0524.h5'\n",
    "tensorboard_log_path = './logs/'+ model_save_path.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print valid loss metric only: https://stackoverflow.com/questions/47774931/keras-print-metrics-only-for-validation-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(\n",
    "            lr=learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1., amsgrad=False)\n",
    "\n",
    "lr_scheduler = callbacks.LearningRateScheduler(\n",
    "        CosineLRSchedule(lr_high=learning_rate, lr_low=1e-8,\n",
    "                         initial_period=num_epochs),\n",
    "        verbose=1)\n",
    "\n",
    "model.compile(\n",
    "            optimizer,\n",
    "            loss=los,\n",
    "            metrics=metric ,loss_weights=loss_weight)#{'word_predictions': masked_perplexity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best, early stopping, 2 models ens weight:(best=0.8,last=0.2)\n",
    "history = History()\n",
    "stop_nan = callbacks.TerminateOnNaN()\n",
    "model_callbacks = [\n",
    "        callbacks.ModelCheckpoint(\n",
    "            model_save_path,\n",
    "            monitor='val_loss',mode='min' ,save_best_only=True, verbose=1,save_weights_only=True),\n",
    "            EarlyStopping(patience=patien,monitor='val_loss',verbose=1,mode='min'),\n",
    "        lr_scheduler, history,stop_nan\n",
    "    ]\n",
    "model_callbacks.append(callbacks.TensorBoard(tensorboard_log_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_batch(batch_size, X_train1, X_train2 , Y_train1, Y_train2):\n",
    "    '''\n",
    "    X_train1 = sent_ids: shape為(N, max_seq_length)\n",
    "    X_train2 = sentemb: shape為(N,max_seq_length, word_embedding_size)\n",
    "    Y_train1 = class_prediction: shape為(N, max_seq_length, 1)\n",
    "    Y_train2 = family_prediction(stage2): shape為(N, fam_num)\n",
    "    '''\n",
    "    idx = np.arange(len(X_train1))\n",
    "    np.random.shuffle(idx)\n",
    "#     if Y_train2 == None:\n",
    "#         print('Input shape of X1,X2,Y1,Y2:',X_train1.shape,X_train2.shape,Y_train1.shape)\n",
    "#         while True:\n",
    "#             for i in idx:\n",
    "#                 train_X1 = X_train1[idx[i]:idx[i]+batch_size]\n",
    "#                 train_X2 = X_train1[idx[i]:idx[i]+batch_size]\n",
    "#                 train_Y1 = X_train1[idx[i]:idx[i]+batch_size]\n",
    "#                 yield train_X1,train_X2,train_Y1\n",
    "#                 if i == idx[-1]:\n",
    "#                     idx = np.arange(len(X_train1))\n",
    "#                     np.random.shuffle(idx)\n",
    "#                     break\n",
    "#     print('Input shape of X1,X2,Y1,Y2:',X_train1.shape,X_train2.shape,Y_train1.shape,Y_train2.shape) #(all data,profile length,api )\n",
    "\n",
    "    while True:\n",
    "        for i in idx:\n",
    "            train_X1 = X_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_X2 = X_train2[idx[i]:idx[i]+batch_size]\n",
    "            train_Y1 = Y_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_Y2 = Y_train2[idx[i]:idx[i]+batch_size]\n",
    "#             yield ([train_X1,train_X2],[train_Y1,train_Y2]) #ori\n",
    "            yield ([train_X1,train_X2],[train_Y2])\n",
    "            if i == idx[-1]:\n",
    "                idx = np.arange(len(X_train1))\n",
    "                np.random.shuffle(idx)\n",
    "                break\n",
    "            \n",
    "#     data_size = X_train.shape[0]\n",
    "#     ep = data_size / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def training_batch(batch_size, X_train1, X_train2 , Y_train1, Y_train2):\\n\\n    idx = np.arange(len(X_train1))\\n    np.random.shuffle(idx)\\n    \\n    fam_dict = {}\\n    for i in range(fam_num):\\n        fam_dict[i] = Y_train2[i]\\n    while True:\\n        for i in idx:\\n            train_X1 = X_train1[idx[i]:idx[i]+batch_size]\\n            train_X2 = X_train2[idx[i]:idx[i]+batch_size]\\n            train_Y1 = Y_train1[idx[i]:idx[i]+batch_size]\\n            train_Y2 = []\\n            for famid in range(fam_num):\\n                temp = fam_dict[famid][idx[i]:idx[i]+batch_size]\\n                train_Y2.append(temp)\\n#             train_Y2 = Y_train2[idx[i]:idx[i]+batch_size]\\n            trainY = [train_Y1]+train_Y2\\n#             print(len(trainY))\\n            yield ([train_X1,train_X2],trainY)\\n            if i == idx[-1]:\\n                idx = np.arange(len(X_train1))\\n                np.random.shuffle(idx)\\n                break\\n\\n# valid_fam_ans.shape\\nvalid_Y2 = []\\nfor i in range(fam_num):\\n    valid_Y2.append(valid_fam_ans[i])'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MTM\n",
    "'''def training_batch(batch_size, X_train1, X_train2 , Y_train1, Y_train2):\n",
    "\n",
    "    idx = np.arange(len(X_train1))\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    fam_dict = {}\n",
    "    for i in range(fam_num):\n",
    "        fam_dict[i] = Y_train2[i]\n",
    "    while True:\n",
    "        for i in idx:\n",
    "            train_X1 = X_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_X2 = X_train2[idx[i]:idx[i]+batch_size]\n",
    "            train_Y1 = Y_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_Y2 = []\n",
    "            for famid in range(fam_num):\n",
    "                temp = fam_dict[famid][idx[i]:idx[i]+batch_size]\n",
    "                train_Y2.append(temp)\n",
    "#             train_Y2 = Y_train2[idx[i]:idx[i]+batch_size]\n",
    "            trainY = [train_Y1]+train_Y2\n",
    "#             print(len(trainY))\n",
    "            yield ([train_X1,train_X2],trainY)\n",
    "            if i == idx[-1]:\n",
    "                idx = np.arange(len(X_train1))\n",
    "                np.random.shuffle(idx)\n",
    "                break\n",
    "\n",
    "# valid_fam_ans.shape\n",
    "valid_Y2 = []\n",
    "for i in range(fam_num):\n",
    "    valid_Y2.append(valid_fam_ans[i])'''\n",
    "# valid_fam_ans = valid_Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/.local/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0002.\n",
      " 4/47 [=>............................] - ETA: 3:40 - loss: 2589.8497 - f1_metric: 0.0319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/leoqaz12/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-09cadbc4e375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0;34m,\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m  \u001b[0;31m#,class_weight='auto'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0;34m,\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                    ,shuffle=True,verbose=1)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_all.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#1st:train 0_1_prediction=0.14XX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(generator=training_batch(batch_size=batch_size,X_train1=train_emb_api,X_train2=train_emb ,\n",
    "                                             Y_train1=train_rep_ans,Y_train2=train_fam_ans) #Y_train2\n",
    "                    , steps_per_epoch=int(np.ceil(len(train_emb_api)/batch_size)) ,\n",
    "                    epochs=num_epochs,callbacks=model_callbacks\n",
    "#                    ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans,valid_fam_ans]) #ori\n",
    "                   ,validation_data= ([valid_emb_api,valid_emb], [valid_fam_ans]) #ori\n",
    "#                    ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans]+valid_Y2) #validY2\n",
    "                    ,max_queue_size=10  #,class_weight='auto'\n",
    "                    ,workers=10,use_multiprocessing=True   \n",
    "                   ,shuffle=True,verbose=1)\n",
    "model.save(model_save_path+\"_all.h5\")\n",
    "#1st:train 0_1_prediction=0.14XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "import matplotlib as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = num_epochs\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model/trm_encoder/1stStage_87fam_0524.h5')\n",
    "# score = model.evaluate([valid_emb_api,valid_emb], [valid_rep_ans]+valid_Y2)\n",
    "print(len(test_emb_api))\n",
    "# ans = model.predict([test_emb_api,test_emb])\n",
    "ans = model.predict([valid_emb_api,valid_emb])\n",
    "len(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fam = np.array(ans[1:])\n",
    "predict_fam = np.swapaxes(predict_fam,0,1)\n",
    "valid_fam_ans1 = np.swapaxes(valid_fam_ans,0,1)\n",
    "predict_fam.shape , valid_fam_ans1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# y_true = np.squeeze(test_fam_ans)\n",
    "y_true = np.squeeze(valid_fam_ans1)\n",
    "y_pred = np.squeeze(predict_fam)\n",
    "final_ans = []\n",
    "for sample in y_pred:\n",
    "    sample_ans = []\n",
    "    for value in sample:\n",
    "        if value < 0.25:\n",
    "            sample_ans.append(0)\n",
    "        else:\n",
    "            sample_ans.append(1)\n",
    "    final_ans.append(sample_ans)\n",
    "final_ans = np.array(final_ans)\n",
    "print(final_ans.shape , sum(final_ans[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_fam_ans = np.swapaxes(test_fam_ans,0,1)\n",
    "# test_fam_ans.shape\n",
    "# y_true = np.array([[1,0,0,0], [1,1,0,0], [1,1,1,1]])\n",
    "# y_pred = np.array([[1,0,0,0], [1,1,1,0], [1,1,1,1]])\n",
    "print(y_true.shape , final_ans.shape)\n",
    "recall = recall_score(y_true=y_true, y_pred=final_ans, average='micro')\n",
    "precision = precision_score(y_true=y_true, y_pred=final_ans, average='micro')\n",
    "f1 = f1_score(y_true=y_true, y_pred=final_ans, average='micro')\n",
    "recall ,precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[0] , final_ans[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
