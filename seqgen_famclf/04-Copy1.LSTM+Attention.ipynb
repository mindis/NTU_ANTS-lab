{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,shutil,pickle,tqdm,sys,random,re,string,pause, datetime,glob\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "# from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "\n",
    "# from collections import Counter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import unicodedata as udata\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "from keras import backend \n",
    "from imblearn.ensemble import *\n",
    "from imblearn.combine import *\n",
    "# from python.keras import backend \n",
    "# Embedding(10,20)\n",
    "from keras_transformer.extras import ReusableEmbedding, TiedOutputEmbedding\n",
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "from keras_transformer.bert import (\n",
    "    BatchGeneratorForBERT, masked_perplexity,\n",
    "    MaskedPenalizedSparseCategoricalCrossentropy)\n",
    "\n",
    "import keras_metrics as km\n",
    "from keras_trans_mask import RemoveMask, RestoreMask\n",
    "\n",
    "from keras_multi_head import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import transformer_bert_model\n",
    "from bpe import BPEEncoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test of sent2vec vector: (424, 213, 768) (424, 213) (424, 44) (424, 213, 1)\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_emb_api, train_fam_ans, train_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/TRAIN_vec.pkl','rb'))\n",
    "valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/DEV_vec.pkl','rb'))\n",
    "test_emb, test_emb_api,test_fam_ans,test_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/TEST_vec.pkl','rb'))\n",
    "# print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "# print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "train_rep_ans = np.expand_dims(train_rep_ans,axis=-1)\n",
    "valid_rep_ans = np.expand_dims(valid_rep_ans,axis=-1)\n",
    "test_rep_ans = np.expand_dims(test_rep_ans,axis=-1)\n",
    "print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_fam_ans.shape,test_rep_ans.shape)\n",
    "emb_matrix = pickle.load(open('data/tree-rep-profiles-partial/api_emb_matrix.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train only fam hash unique\n",
    "unique , indx = np.unique(train_emb, axis=0, return_index=True)\n",
    "emb_api = train_emb_api[indx]\n",
    "fam = train_fam_ans[indx]\n",
    "print(unique.shape,emb_api.shape,fam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, X2 ,X3,X4):\n",
    "#     X3 = np.take(train_fam_ans,[0],axis=-1) #只train第幾個familiy\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], X2[randomize],X3[randomize],X4[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train of sent2vec vector: (11141, 213, 768) (11141, 213) (11141, 44) (11141, 213, 1)\n",
      "valid of sent2vec vector: (437, 213, 768) (437, 213) (437, 44) (437, 213, 1)\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_emb_api, train_fam_ans, train_rep_ans = _shuffle(train_emb, train_emb_api, train_fam_ans, train_rep_ans)\n",
    "valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans = _shuffle(valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans)\n",
    "\n",
    "# test_emb, test_emb_api,test_fam_ans,test_rep_ans  = _shuffle(test_emb,test_emb_api,test_fam_ans,test_rep_ans)\n",
    "\n",
    "print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "\n",
    "# print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_fam_ans.shape,test_rep_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale = 'no'\n",
    "\n",
    "# dim-wise scaling\n",
    "def scaling(trainX,validX,testX,scale='min_max'):\n",
    "#     if scale == 'min_max':\n",
    "    max_value = max([np.max(trainX) , np.max(validX),np.max(testX)])\n",
    "    min_value = min([np.min(trainX),np.min(validX),np.min(testX)])\n",
    "\n",
    "    trainX = (trainX - min_value) / (max_value - min_value)\n",
    "    validX = (validX - min_value) / (max_value - min_value )\n",
    "    testX = (testX - min_value) / (max_value - min_value )\n",
    "    print(np.max(trainX),np.max(validX))\n",
    "    return trainX,validX,testX , max_value , min_value\n",
    "def scaling(trainX,validX,testX,scale='mean_dim',):\n",
    "#     if scale == 'min_max':\n",
    "    alls = np.concatenate((trainX,validX,testX),axis=0)\n",
    "    mean = np.mean(alls,axis=-1)\n",
    "    mean = np.mean(mean,axis=0)\n",
    "    mean = np.expand_dims(mean,axis=-1)\n",
    "    mean = np.repeat(mean,trainX.shape[2],axis=-1)\n",
    "    mean = np.expand_dims(mean,axis=0)\n",
    "    mean_train = np.repeat(mean,trainX.shape[0],axis=0)\n",
    "    mean_valid = np.repeat(mean,validX.shape[0],axis=0)\n",
    "    mean_test = np.repeat(mean,testX.shape[0],axis=0)\n",
    "    std = np.std(alls,axis=-1)\n",
    "    std = np.std(std,axis=0)\n",
    "    std = np.expand_dims(std,axis=-1)\n",
    "    std = np.repeat(std,validX.shape[2],axis=-1)\n",
    "    std_train = np.repeat(std,trainX.shape[0],axis=0)\n",
    "    std_valid = np.repeat(std,validX.shape[0],axis=0)\n",
    "    std_test = np.repeat(std,testX.shape[0],axis=0)\n",
    "#     min_value = min([np.min(trainX),np.min(validX),np.min(testX)])\n",
    "\n",
    "    trainX = (trainX - mean) / (std + 1e-10)\n",
    "    validX = (validX - mean) / (std + 1e-10)\n",
    "    testX = (testX - mean) / (std + 1e-10)\n",
    "#     print(np.max(trainX),np.max(validX))\n",
    "    return trainX,validX,testX , mean , std\n",
    "\n",
    "# train_emb,valid_emb,test_emb , max_value,min_value = scaling(train_emb,valid_emb,test_emb)   \n",
    "# print(valid_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33855778, -0.32893312, -0.72717506, ...,  0.91245544,\n",
       "         1.18298519, -0.31610405],\n",
       "       [-0.48375982, -0.76325315, -1.20088184, ...,  0.87430948,\n",
       "         1.2489109 , -0.08723355],\n",
       "       [ 0.0535125 , -0.68292403, -0.69628733, ...,  1.14116526,\n",
       "         0.79122907, -0.22612473],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kk = np.mean(train_emb,axis=-1)\n",
    "# kk = np.mean(kk,axis=0)\n",
    "# kk = np.expand_dims(kk,axis=0)\n",
    "# kk = np.repeat(kk,100,axis=0)\n",
    "# kk = np.expand_dims(kk,axis=-1)\n",
    "# kk = np.repeat(kk,768,axis=-1)\n",
    "# kk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kkk = (emb_matrix - kk)/kk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kk = np.std(emb_matrix,axis=-1)\n",
    "# kk = np.expand_dims(kk,axis=-1)\n",
    "# kk = np.repeat(kk,768,axis=-1)\n",
    "# kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc = SMOTEENN()\n",
    "# N,t,d = train_emb.shape\n",
    "# train_emb_ = train_emb.reshape(N,t*d)\n",
    "# train_fam_ans_ = train_fam_ans.reshape(N,)\n",
    "# train_emb_ , train_fam_ans_  = bc.fit_resample(train_emb_, train_fam_ans_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emb = train_emb_.reshape(-1,t,d)\n",
    "# train_fam_ans = train_fam_ans_.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4.963642428285524,\n",
       " 1: 3.4256445672191527,\n",
       " 2: 11.222473604826547,\n",
       " 3: 17.163783160322954,\n",
       " 4: 50.6156462585034,\n",
       " 5: 63.59401709401709,\n",
       " 6: 49.60333333333333,\n",
       " 7: 105.53900709219859,\n",
       " 8: 56.36742424242424,\n",
       " 9: 54.70955882352941,\n",
       " 10: 148.81,\n",
       " 11: 73.66831683168317,\n",
       " 12: 73.66831683168317,\n",
       " 13: 256.5689655172414,\n",
       " 14: 195.80263157894737,\n",
       " 15: 25.39419795221843,\n",
       " 16: 201.09459459459458,\n",
       " 17: 212.58571428571432,\n",
       " 18: 153.41237113402062,\n",
       " 19: 228.93846153846152,\n",
       " 20: 195.80263157894737,\n",
       " 21: 144.47572815533982,\n",
       " 22: 33.7437641723356,\n",
       " 23: 215.66666666666669,\n",
       " 24: 165.34444444444443,\n",
       " 25: 130.53508771929825,\n",
       " 26: 165.34444444444443,\n",
       " 27: 338.20454545454544,\n",
       " 28: 222.1044776119403,\n",
       " 29: 256.5689655172414,\n",
       " 30: 64.98253275109171,\n",
       " 31: 354.3095238095238,\n",
       " 32: 190.7820512820513,\n",
       " 33: 163.52747252747253,\n",
       " 34: 98.54966887417218,\n",
       " 35: 94.78343949044586,\n",
       " 36: 362.9512195121951,\n",
       " 37: 150.31313131313132,\n",
       " 38: 744.05,\n",
       " 39: 620.0416666666666,\n",
       " 40: 391.60526315789474,\n",
       " 41: 316.6170212765958,\n",
       " 42: 496.03333333333336,\n",
       " 43: 132.86607142857142}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = sum(train_fam_ans) / sum(sum(train_fam_ans))\n",
    "fam_weights={}\n",
    "for i in range(len(class_weights)):\n",
    "    fam_weights[i] = 1/class_weights[i]\n",
    "fam_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2998,\n",
       " 1: 4344,\n",
       " 2: 1326,\n",
       " 3: 867,\n",
       " 4: 294,\n",
       " 5: 234,\n",
       " 6: 300,\n",
       " 7: 141,\n",
       " 8: 264,\n",
       " 9: 272,\n",
       " 10: 100,\n",
       " 11: 202,\n",
       " 12: 202,\n",
       " 13: 58,\n",
       " 14: 76,\n",
       " 15: 586,\n",
       " 16: 74,\n",
       " 17: 70,\n",
       " 18: 97,\n",
       " 19: 65,\n",
       " 20: 76,\n",
       " 21: 103,\n",
       " 22: 441,\n",
       " 23: 69,\n",
       " 24: 90,\n",
       " 25: 114,\n",
       " 26: 90,\n",
       " 27: 44,\n",
       " 28: 67,\n",
       " 29: 58,\n",
       " 30: 229,\n",
       " 31: 42,\n",
       " 32: 78,\n",
       " 33: 91,\n",
       " 34: 151,\n",
       " 35: 157,\n",
       " 36: 41,\n",
       " 37: 99,\n",
       " 38: 20,\n",
       " 39: 24,\n",
       " 40: 38,\n",
       " 41: 47,\n",
       " 42: 30,\n",
       " 43: 112}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fam = sum(train_fam_ans)\n",
    "for i in range(len(all_fam)):\n",
    "    fam_weights[i] = all_fam[i]\n",
    "fam_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.3664174982464374,\n",
       " 1: 1.0,\n",
       " 2: 2.1821960061631724,\n",
       " 3: 2.6070792001284384,\n",
       " 4: 3.688538409570299,\n",
       " 5: 3.916797061551279,\n",
       " 6: 3.6683357022527794,\n",
       " 7: 4.423358286530812,\n",
       " 8: 3.7961690737626643,\n",
       " 9: 3.766316110612983,\n",
       " 10: 4.766947990920889,\n",
       " 11: 4.063850479507775,\n",
       " 12: 4.063850479507775,\n",
       " 13: 5.311675166362561,\n",
       " 14: 5.041384836622649,\n",
       " 15: 2.9987983873319677,\n",
       " 16: 5.068053083704811,\n",
       " 17: 5.123622934859622,\n",
       " 18: 4.7974071984055975,\n",
       " 19: 5.1977309070133435,\n",
       " 20: 5.041384836622649,\n",
       " 21: 4.737389188679344,\n",
       " 22: 3.2830733014621343,\n",
       " 23: 5.138011672311721,\n",
       " 24: 4.872308506578715,\n",
       " 25: 4.635919728514485,\n",
       " 26: 4.872308506578715,\n",
       " 27: 5.587928542990719,\n",
       " 28: 5.167425557518015,\n",
       " 29: 5.311675166362561,\n",
       " 30: 3.938396173354741,\n",
       " 31: 5.634448558625612,\n",
       " 32: 5.015409350219389,\n",
       " 33: 4.8612586703921306,\n",
       " 34: 4.354838340094056,\n",
       " 35: 4.315872371560673,\n",
       " 36: 5.658546110204672,\n",
       " 37: 4.7769983267743905,\n",
       " 38: 6.376385903354989,\n",
       " 39: 6.194064346561035,\n",
       " 40: 5.734532017182595,\n",
       " 41: 5.521970575198922,\n",
       " 42: 5.970920795246825,\n",
       " 43: 4.653619305613886}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def create_class_weight(labels_dict,mu=0.79):\n",
    "    total = np.sum(np.array(list(labels_dict.values())))\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "\n",
    "    return class_weight\n",
    "fam_weights = create_class_weight(fam_weights)\n",
    "fam_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# temp = list(fam_weights.values())\n",
    "# max_value = np.max(temp)\n",
    "# for i in range(len(fam_weights)):\n",
    "#     fam_weights[i] = fam_weights[i]/max_value\n",
    "# fam_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 768 #被除數\n",
    "num_heads = 4#除數，要整除\n",
    "max_length = 213 # max sequence length\n",
    "fam_num = train_fam_ans.shape[1]\n",
    "vocabulary_size = 26\n",
    "transformer_depth = 1\n",
    "transformer_dropout = 0.1\n",
    "l2_reg_penalty = 1e-5#1e-4\n",
    "dp_rate = 0.2\n",
    "traina = True\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "constrain = keras.constraints.MinMaxNorm(min_value=0.0, max_value=0.0, rate=1.0, axis=0)\n",
    "init = keras.initializers.Ones()\n",
    "coordinate_embedding_layer = TransformerCoordinateEmbedding(\n",
    "        transformer_depth , name='coordinate_embedding')\n",
    "act_layer = TransformerACT(\n",
    "            name='adaptive_computation_time')\n",
    "\n",
    "transformer_block = TransformerBlock(\n",
    "            name='transformer', num_heads=num_heads,\n",
    "            residual_dropout=transformer_dropout,\n",
    "            attention_dropout=transformer_dropout,\n",
    "            # Allow bi-directional attention\n",
    "            use_masking=False)\n",
    "add_segment_layer = Add(name='add_segment')\n",
    "l2_regularizer = (regularizers.l2(l2_reg_penalty) if l2_reg_penalty else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\n",
    "sentemb = Masking(mask_value=0)(sentemb1)\n",
    "#shape=(max_length,emb_dim),,batch_shape=(batch_size,max_length,emb_dim)\n",
    "sent_ids1 = Input(shape=(max_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\n",
    "sent_ids = Masking(mask_value=0)(sent_ids1)\n",
    "#shape=(max_length,),batch_shape=(batch_size,max_length)\n",
    "api_emb = Embedding(vocabulary_size+1, emb_dim,weights=[emb_matrix],input_length=max_length\n",
    "                    ,trainable=True,name='api_emb')(sent_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segment_embeddings = Add()([sentemb,api_emb])\n",
    "next_step_input1 = RemoveMask()(segment_embeddings)\n",
    "next_step_input = coordinate_embedding_layer(next_step_input1, step=0,trainable=traina) #next_step_input_emb\n",
    "next_step_input= RestoreMask()([next_step_input,segment_embeddings])\n",
    "# next_step_input = add_segment_layer([next_step_input, api_emb]) \n",
    "\n",
    "att_layer = MultiHeadAttention(\n",
    "    head_num=num_heads, trainable=traina,\n",
    "    name='Multi-Head')(next_step_input)\n",
    "next_step_input = BatchNormalization()(att_layer)\n",
    "att_in = Dense(64,kernel_initializer=keras.initializers.lecun_normal(),activation='selu',\n",
    "               name='attention_in_64',trainable=True,kernel_regularizer=l2_regularizer)(next_step_input)\n",
    "rep_prediction = (\n",
    "        Dense(1, name='0_1_predict', activation=hard_sigmoid)\n",
    "    (att_in))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "mul = multiply([segment_embeddings,rep_prediction],name='mul')\n",
    "\n",
    "bn = BatchNormalization()\n",
    "dp = Dropout(dp_rate)\n",
    "dense1 = Dense(8,kernel_initializer=keras.initializers.lecun_normal(),activation='selu',\n",
    "              kernel_regularizer=l2_regularizer,name='dense_8')\n",
    "gru = GRU(64, dropout=dp_rate, recurrent_dropout=dp_rate,name='gru_64')\n",
    "alls = []\n",
    "for i in range(fam_num):\n",
    "    alls.append(dense1(bn(gru(mul))))\n",
    "out = Concatenate()(alls)\n",
    "out = Dense(44,activation='sigmoid',name='family')(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_ids (InputLayer)           (None, 213)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sent_emb (InputLayer)           (None, 213, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 213)          0           sent_ids[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 213, 768)     0           sent_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "api_emb (Embedding)             (None, 213, 768)     20736       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 213, 768)     0           masking_1[0][0]                  \n",
      "                                                                 api_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "remove_mask_1 (RemoveMask)      (None, 213, 768)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "coordinate_embedding (Transform (None, 213, 768)     164352      remove_mask_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "restore_mask_1 (RestoreMask)    (None, 213, 768)     0           coordinate_embedding[0][0]       \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Multi-Head (MultiHeadAttention) (None, 213, 768)     2362368     restore_mask_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 213, 768)     3072        Multi-Head[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_in_64 (Dense)         (None, 213, 64)      49216       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "0_1_predict (Dense)             (None, 213, 1)       65          attention_in_64[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mul (Multiply)                  (None, 213, 768)     0           add_1[0][0]                      \n",
      "                                                                 0_1_predict[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 32)           76896       mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "                                                                 mul[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32)           128         gru[0][0]                        \n",
      "                                                                 gru[1][0]                        \n",
      "                                                                 gru[2][0]                        \n",
      "                                                                 gru[3][0]                        \n",
      "                                                                 gru[4][0]                        \n",
      "                                                                 gru[5][0]                        \n",
      "                                                                 gru[6][0]                        \n",
      "                                                                 gru[7][0]                        \n",
      "                                                                 gru[8][0]                        \n",
      "                                                                 gru[9][0]                        \n",
      "                                                                 gru[10][0]                       \n",
      "                                                                 gru[11][0]                       \n",
      "                                                                 gru[12][0]                       \n",
      "                                                                 gru[13][0]                       \n",
      "                                                                 gru[14][0]                       \n",
      "                                                                 gru[15][0]                       \n",
      "                                                                 gru[16][0]                       \n",
      "                                                                 gru[17][0]                       \n",
      "                                                                 gru[18][0]                       \n",
      "                                                                 gru[19][0]                       \n",
      "                                                                 gru[20][0]                       \n",
      "                                                                 gru[21][0]                       \n",
      "                                                                 gru[22][0]                       \n",
      "                                                                 gru[23][0]                       \n",
      "                                                                 gru[24][0]                       \n",
      "                                                                 gru[25][0]                       \n",
      "                                                                 gru[26][0]                       \n",
      "                                                                 gru[27][0]                       \n",
      "                                                                 gru[28][0]                       \n",
      "                                                                 gru[29][0]                       \n",
      "                                                                 gru[30][0]                       \n",
      "                                                                 gru[31][0]                       \n",
      "                                                                 gru[32][0]                       \n",
      "                                                                 gru[33][0]                       \n",
      "                                                                 gru[34][0]                       \n",
      "                                                                 gru[35][0]                       \n",
      "                                                                 gru[36][0]                       \n",
      "                                                                 gru[37][0]                       \n",
      "                                                                 gru[38][0]                       \n",
      "                                                                 gru[39][0]                       \n",
      "                                                                 gru[40][0]                       \n",
      "                                                                 gru[41][0]                       \n",
      "                                                                 gru[42][0]                       \n",
      "                                                                 gru[43][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            132         batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_2[1][0]      \n",
      "                                                                 batch_normalization_2[2][0]      \n",
      "                                                                 batch_normalization_2[3][0]      \n",
      "                                                                 batch_normalization_2[4][0]      \n",
      "                                                                 batch_normalization_2[5][0]      \n",
      "                                                                 batch_normalization_2[6][0]      \n",
      "                                                                 batch_normalization_2[7][0]      \n",
      "                                                                 batch_normalization_2[8][0]      \n",
      "                                                                 batch_normalization_2[9][0]      \n",
      "                                                                 batch_normalization_2[10][0]     \n",
      "                                                                 batch_normalization_2[11][0]     \n",
      "                                                                 batch_normalization_2[12][0]     \n",
      "                                                                 batch_normalization_2[13][0]     \n",
      "                                                                 batch_normalization_2[14][0]     \n",
      "                                                                 batch_normalization_2[15][0]     \n",
      "                                                                 batch_normalization_2[16][0]     \n",
      "                                                                 batch_normalization_2[17][0]     \n",
      "                                                                 batch_normalization_2[18][0]     \n",
      "                                                                 batch_normalization_2[19][0]     \n",
      "                                                                 batch_normalization_2[20][0]     \n",
      "                                                                 batch_normalization_2[21][0]     \n",
      "                                                                 batch_normalization_2[22][0]     \n",
      "                                                                 batch_normalization_2[23][0]     \n",
      "                                                                 batch_normalization_2[24][0]     \n",
      "                                                                 batch_normalization_2[25][0]     \n",
      "                                                                 batch_normalization_2[26][0]     \n",
      "                                                                 batch_normalization_2[27][0]     \n",
      "                                                                 batch_normalization_2[28][0]     \n",
      "                                                                 batch_normalization_2[29][0]     \n",
      "                                                                 batch_normalization_2[30][0]     \n",
      "                                                                 batch_normalization_2[31][0]     \n",
      "                                                                 batch_normalization_2[32][0]     \n",
      "                                                                 batch_normalization_2[33][0]     \n",
      "                                                                 batch_normalization_2[34][0]     \n",
      "                                                                 batch_normalization_2[35][0]     \n",
      "                                                                 batch_normalization_2[36][0]     \n",
      "                                                                 batch_normalization_2[37][0]     \n",
      "                                                                 batch_normalization_2[38][0]     \n",
      "                                                                 batch_normalization_2[39][0]     \n",
      "                                                                 batch_normalization_2[40][0]     \n",
      "                                                                 batch_normalization_2[41][0]     \n",
      "                                                                 batch_normalization_2[42][0]     \n",
      "                                                                 batch_normalization_2[43][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 176)          0           dense[0][0]                      \n",
      "                                                                 dense[1][0]                      \n",
      "                                                                 dense[2][0]                      \n",
      "                                                                 dense[3][0]                      \n",
      "                                                                 dense[4][0]                      \n",
      "                                                                 dense[5][0]                      \n",
      "                                                                 dense[6][0]                      \n",
      "                                                                 dense[7][0]                      \n",
      "                                                                 dense[8][0]                      \n",
      "                                                                 dense[9][0]                      \n",
      "                                                                 dense[10][0]                     \n",
      "                                                                 dense[11][0]                     \n",
      "                                                                 dense[12][0]                     \n",
      "                                                                 dense[13][0]                     \n",
      "                                                                 dense[14][0]                     \n",
      "                                                                 dense[15][0]                     \n",
      "                                                                 dense[16][0]                     \n",
      "                                                                 dense[17][0]                     \n",
      "                                                                 dense[18][0]                     \n",
      "                                                                 dense[19][0]                     \n",
      "                                                                 dense[20][0]                     \n",
      "                                                                 dense[21][0]                     \n",
      "                                                                 dense[22][0]                     \n",
      "                                                                 dense[23][0]                     \n",
      "                                                                 dense[24][0]                     \n",
      "                                                                 dense[25][0]                     \n",
      "                                                                 dense[26][0]                     \n",
      "                                                                 dense[27][0]                     \n",
      "                                                                 dense[28][0]                     \n",
      "                                                                 dense[29][0]                     \n",
      "                                                                 dense[30][0]                     \n",
      "                                                                 dense[31][0]                     \n",
      "                                                                 dense[32][0]                     \n",
      "                                                                 dense[33][0]                     \n",
      "                                                                 dense[34][0]                     \n",
      "                                                                 dense[35][0]                     \n",
      "                                                                 dense[36][0]                     \n",
      "                                                                 dense[37][0]                     \n",
      "                                                                 dense[38][0]                     \n",
      "                                                                 dense[39][0]                     \n",
      "                                                                 dense[40][0]                     \n",
      "                                                                 dense[41][0]                     \n",
      "                                                                 dense[42][0]                     \n",
      "                                                                 dense[43][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "family (Dense)                  (None, 44)           7788        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,684,753\n",
      "Trainable params: 2,683,153\n",
      "Non-trainable params: 1,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[sent_ids1,sentemb1], outputs=[out]) #out\n",
    "model.load_weights('./model/att_clf/2ndStage_44fam_0611_copy1.h5',by_name=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multi_gpu_model(model , gpus=2)\n",
    "\n",
    "# model.load_weights('./model/LSTM_att/1stStage_44fam_0607.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nsentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\\nsentemb = Masking(mask_value=0)(sentemb1)\\nsent_ids1 = Input(shape=(max_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\\nsent_ids = Masking(mask_value=0)(sent_ids1)\\napi_emb = Embedding(vocabulary_size+1, emb_dim,weights=[emb_matrix],input_length=max_length,trainable=True,name='api_emb')(sent_ids)\\n\\nfinal_emb = Add()([sentemb,api_emb])\\n\\ntimesteps,state_h,state_c = LSTM(int(emb_dim/2),return_sequences=True,return_state=True,name='lstm1')(final_emb) #final_emb\\nstate = Concatenate()([state_h,state_c])\\nfc = Dense(max_length,activation='sigmoid',bias_constraint=None,kernel_initializer=init,name='attention')(state)\\nfc = Lambda(lambda x: keras.backend.expand_dims(x,axis=-1),name='RasMMA')(fc)\\nfc = Lambda(lambda x: keras.backend.repeat_elements(x,int(emb_dim/2),axis=-1))(fc)\\n# fc = keras.backend.repeat_elements(fc,256,axis=-1)\\n# fc = keras.backend.expand_dims(fc,axis=-1)\\nmul = Multiply()([fc,timesteps])\\n# mul = BatchNormalization()(mul)\\nalls = []\\ngru = (GRU(int(emb_dim/4))) #/8\\n# gru = GRU(1)\\nbn = BatchNormalization()\\ndp = Dropout(0.01)\\n\\ndense = Dense(1,activation='sigmoid')\\nfor i in range(fam_num):\\n#     alls.append(dense(bn(gru(mul))))\\n    alls.append(dense(dp(bn(gru(mul)))))\\n#     alls.append(gru(mul))\\nout = Concatenate(name='family')(alls)\\n# out = Dense(44,activation='sigmoid')(out)\\nmodel_old = Model(inputs=[sent_ids1,sentemb1], outputs=[out]) #out\\nmodel_old = multi_gpu_model(model_old , gpus=3)\\nmodel_old.load_weights('./model/LSTM_att/1stStage_44fam_0607.h5')\\nmodel_old.summary()\\n\\n# model = load_model('./model/LSTM_att/1stStage_44fam_0607.h5_all.h5')\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "sentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\n",
    "sentemb = Masking(mask_value=0)(sentemb1)\n",
    "sent_ids1 = Input(shape=(max_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\n",
    "sent_ids = Masking(mask_value=0)(sent_ids1)\n",
    "api_emb = Embedding(vocabulary_size+1, emb_dim,weights=[emb_matrix],input_length=max_length,trainable=True,name='api_emb')(sent_ids)\n",
    "\n",
    "final_emb = Add()([sentemb,api_emb])\n",
    "\n",
    "timesteps,state_h,state_c = LSTM(int(emb_dim/2),return_sequences=True,return_state=True,name='lstm1')(final_emb) #final_emb\n",
    "state = Concatenate()([state_h,state_c])\n",
    "fc = Dense(max_length,activation='sigmoid',bias_constraint=None,kernel_initializer=init,name='attention')(state)\n",
    "fc = Lambda(lambda x: keras.backend.expand_dims(x,axis=-1),name='RasMMA')(fc)\n",
    "fc = Lambda(lambda x: keras.backend.repeat_elements(x,int(emb_dim/2),axis=-1))(fc)\n",
    "# fc = keras.backend.repeat_elements(fc,256,axis=-1)\n",
    "# fc = keras.backend.expand_dims(fc,axis=-1)\n",
    "mul = Multiply()([fc,timesteps])\n",
    "# mul = BatchNormalization()(mul)\n",
    "alls = []\n",
    "gru = (GRU(int(emb_dim/4))) #/8\n",
    "# gru = GRU(1)\n",
    "bn = BatchNormalization()\n",
    "dp = Dropout(0.01)\n",
    "\n",
    "dense = Dense(1,activation='sigmoid')\n",
    "for i in range(fam_num):\n",
    "#     alls.append(dense(bn(gru(mul))))\n",
    "    alls.append(dense(dp(bn(gru(mul)))))\n",
    "#     alls.append(gru(mul))\n",
    "out = Concatenate(name='family')(alls)\n",
    "# out = Dense(44,activation='sigmoid')(out)\n",
    "model_old = Model(inputs=[sent_ids1,sentemb1], outputs=[out]) #out\n",
    "model_old = multi_gpu_model(model_old , gpus=3)\n",
    "model_old.load_weights('./model/LSTM_att/1stStage_44fam_0607.h5')\n",
    "model_old.summary()\n",
    "\n",
    "# model = load_model('./model/LSTM_att/1stStage_44fam_0607.h5_all.h5')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_old.layers[-2].save_weights('./model/LSTM_att/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_multi_label_metric(y_true, y_pred):\n",
    "    comp = K.equal(y_true, K.round(y_pred))\n",
    "    return K.cast(K.all(comp, axis=-1), K.floatx())\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def custom_acc1(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred,k=3)\n",
    "from keras.metrics import binary_accuracy\n",
    "def bin_acc(y_true, y_pred):\n",
    "    return binary_accuracy(y_true, y_pred)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    return tf.keras.metrics.Precision(y_true,y_pred)[1]\n",
    "def recall(y_true, y_pred):\n",
    "    return tf.keras.metrics.Recall(y_true,y_pred)[1]\n",
    "# from sklearn.metrics import f1_score\n",
    "# def f1_sk(y_true,y_pred):\n",
    "#     score = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "#     return score\n",
    "\n",
    "# 訓練參數\n",
    "los = [losses.binary_crossentropy,binary_focal_loss(alpha=.25, gamma=2)] # 1st stage.  f1_loss\n",
    "#SINGLE\n",
    "los = [binary_focal_loss(alpha=.25, gamma=2)]\n",
    "\n",
    "# los = [losses.binary_crossentropy]\n",
    "# MML\n",
    "'''los = []\n",
    "for i in range(fam_num):\n",
    "    los.append(binary_focal_loss(alpha=.25, gamma=2))\n",
    "los = [losses.binary_crossentropy] + los'''\n",
    "\n",
    "\n",
    "metric = {'RasMMA': 'acc','family': f1} # 1st stage. km.f1_score()\n",
    "#SINGLE\n",
    "metric = [f1_metric,bin_acc]\n",
    "# metric = [km.f1_score(),bin_acc,km.binary_f1_score()]\n",
    "# metric = {'RasMMA': 'acc'}\n",
    "# metric = [bin_acc]\n",
    "#MML\n",
    "'''metrics = []\n",
    "for i in range(fam_num+1):\n",
    "    metrics.append('acc')\n",
    "# metrics = {}\n",
    "# metrics['RasMMA'] = 'acc'\n",
    "# for i in range(fam_num):\n",
    "#     metrics['fam'+str(i)]='acc'\n",
    "metric = metrics'''\n",
    "\n",
    "\n",
    "loss_weight = [1,1] #stage1 0.95,0.05  #1st stage # 2nd stage [0.01,0.99]\n",
    "#SINGLE\n",
    "loss_weight = [1]\n",
    "#MML\n",
    "'''loss_weight = []\n",
    "for i in range(fam_num):\n",
    "    loss_weight.append(0.95)\n",
    "loss_weight = [0.05] + loss_weight'''\n",
    "\n",
    "learning_rate = 5e-4#2e-4 # 2nd stage: 1e-4 @1st:2e-4 0.002\n",
    "# batch_size = 128 #32 #128\n",
    "\n",
    "num_epochs = 1000\n",
    "patien = 50\n",
    "\n",
    "model_save_path = './model/att_clf/2ndStage_44fam_0611_copy1.h5'\n",
    "tensorboard_log_path = './logs/'+ model_save_path.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "optimizer = optimizers.Adam(\n",
    "            lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False) #clipnorm=1. , clipvalue=1.\n",
    "optimizer = keras.optimizers.Nadam(lr=learning_rate, clipvalue=1.)\n",
    "# tf.keras.optimizers.Nadam\n",
    "lr_scheduler1 = callbacks.LearningRateScheduler(\n",
    "        CosineLRSchedule(lr_high=0.0006, lr_low=1e-8, #learning_rate\n",
    "                         initial_period=num_epochs),\n",
    "        verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=int(patien/3),\n",
    "                                      min_lr=1e-8,mode='min')\n",
    "\n",
    "model.compile(\n",
    "            optimizer,\n",
    "            loss=los,\n",
    "            metrics=metric ,loss_weights=loss_weight)#{'word_predictions': masked_perplexity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best, early stopping, 2 models ens weight:(best=0.8,last=0.2)\n",
    "history = History()\n",
    "stop_nan = callbacks.TerminateOnNaN()\n",
    "model_callbacks = [\n",
    "        callbacks.ModelCheckpoint(\n",
    "            model_save_path,\n",
    "            monitor='val_f1_metric',mode='max' ,save_best_only=True, verbose=1,save_weights_only=True),\n",
    "            EarlyStopping(patience=patien,monitor='val_loss',verbose=1,mode='min'),\n",
    "        lr_scheduler, lr_scheduler1,history,stop_nan\n",
    "    ]\n",
    "model_callbacks.append(callbacks.TensorBoard(tensorboard_log_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_batch(batch_size, X_train1, X_train2 , Y_train1, Y_train2):\n",
    "    '''\n",
    "    X_train1 = sent_ids: shape為(N, max_seq_length)\n",
    "    X_train2 = sentemb: shape為(N,max_seq_length, word_embedding_size)\n",
    "    Y_train1 = class_prediction: shape為(N, max_seq_length, 1)\n",
    "    Y_train2 = family_prediction(stage2): shape為(N, fam_num)\n",
    "    '''\n",
    "    idx = np.arange(len(X_train1))\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    while True:\n",
    "        for i in idx:\n",
    "            train_X1 = X_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_X2 = X_train2[idx[i]:idx[i]+batch_size]\n",
    "            train_Y1 = Y_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_Y2 = Y_train2[idx[i]:idx[i]+batch_size]\n",
    "#             yield(train_X2,train_Y2)\n",
    "#             yield ([train_X1,train_X2],[train_Y1,train_Y2]) #ori\n",
    "            yield ([train_X1,train_X2],[train_Y2])\n",
    "            if i == idx[-1]:\n",
    "                idx = np.arange(len(X_train1))\n",
    "                np.random.shuffle(idx)\n",
    "                break\n",
    "            \n",
    "#     data_size = X_train.shape[0]\n",
    "#     ep = data_size / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/.local/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0007.\n",
      "175/175 [==============================] - 2404s 14s/step - loss: 311.8083 - f1_metric: 0.0690 - bin_acc: 0.8919 - val_loss: 35.6385 - val_f1_metric: 0.2064 - val_bin_acc: 0.9671\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.20640, saving model to ./model/att_clf/2ndStage_44fam_0611_copy1.h5\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0006999982728453244.\n",
      "175/175 [==============================] - 2311s 13s/step - loss: 55.8455 - f1_metric: 0.2737 - bin_acc: 0.9738 - val_loss: 38.4309 - val_f1_metric: 0.3494 - val_bin_acc: 0.9662\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.20640 to 0.34939, saving model to ./model/att_clf/2ndStage_44fam_0611_copy1.h5\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0006999930913983436.\n",
      "175/175 [==============================] - 2284s 13s/step - loss: 50.5315 - f1_metric: 0.3515 - bin_acc: 0.9758 - val_loss: 34.6726 - val_f1_metric: 0.3057 - val_bin_acc: 0.9692\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.34939\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0006999844557101969.\n",
      "175/175 [==============================] - 2418s 14s/step - loss: 51.5608 - f1_metric: 0.3477 - bin_acc: 0.9757 - val_loss: 11.2134 - val_f1_metric: 0.2896 - val_bin_acc: 0.9684\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.34939\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0006999723658661146.\n",
      "175/175 [==============================] - 2302s 13s/step - loss: 51.1091 - f1_metric: 0.3561 - bin_acc: 0.9761 - val_loss: 32.2630 - val_f1_metric: 0.2673 - val_bin_acc: 0.9697\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.34939\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.0006999568219854188.\n",
      "175/175 [==============================] - 2288s 13s/step - loss: 51.1638 - f1_metric: 0.3668 - bin_acc: 0.9762 - val_loss: 69.5470 - val_f1_metric: 0.1370 - val_bin_acc: 0.9416\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.34939\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.0006999378242215212.\n",
      "175/175 [==============================] - 2306s 13s/step - loss: 52.0663 - f1_metric: 0.3841 - bin_acc: 0.9763 - val_loss: 35.2164 - val_f1_metric: 0.2271 - val_bin_acc: 0.9692\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.34939\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.0006999153727619223.\n",
      "175/175 [==============================] - 2309s 13s/step - loss: 48.8171 - f1_metric: 0.3979 - bin_acc: 0.9767 - val_loss: 37.2321 - val_f1_metric: 0.3307 - val_bin_acc: 0.9682\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.34939\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.0006998894678282086.\n",
      "175/175 [==============================] - 2355s 13s/step - loss: 48.6214 - f1_metric: 0.3886 - bin_acc: 0.9761 - val_loss: 32.2290 - val_f1_metric: 0.2572 - val_bin_acc: 0.9688\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.34939\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0006998601096760515.\n",
      "175/175 [==============================] - 2331s 13s/step - loss: 48.7280 - f1_metric: 0.3913 - bin_acc: 0.9763 - val_loss: 34.9229 - val_f1_metric: 0.2473 - val_bin_acc: 0.9679\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.34939\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.0006998272985952042.\n",
      "175/175 [==============================] - 2317s 13s/step - loss: 47.0031 - f1_metric: 0.3913 - bin_acc: 0.9765 - val_loss: 31.2406 - val_f1_metric: 0.3409 - val_bin_acc: 0.9709\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.34939\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.0006997910349094987.\n",
      "175/175 [==============================] - 2312s 13s/step - loss: 46.8020 - f1_metric: 0.4014 - bin_acc: 0.9768 - val_loss: 34.7688 - val_f1_metric: 0.3470 - val_bin_acc: 0.9683\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.34939\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.000699751318976843.\n",
      "175/175 [==============================] - 2333s 13s/step - loss: 47.3502 - f1_metric: 0.4008 - bin_acc: 0.9767 - val_loss: 33.4887 - val_f1_metric: 0.3216 - val_bin_acc: 0.9690\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.34939\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.0006997081511892172.\n",
      "175/175 [==============================] - 2284s 13s/step - loss: 49.7991 - f1_metric: 0.3725 - bin_acc: 0.9762 - val_loss: 40.6298 - val_f1_metric: 0.3075 - val_bin_acc: 0.9640\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.34939\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.0006996615319726703.\n",
      "175/175 [==============================] - 2330s 13s/step - loss: 48.3056 - f1_metric: 0.3857 - bin_acc: 0.9764 - val_loss: 33.2219 - val_f1_metric: 0.2945 - val_bin_acc: 0.9696\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.34939\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.0006996114617873147.\n",
      "175/175 [==============================] - 2354s 13s/step - loss: 45.5674 - f1_metric: 0.3874 - bin_acc: 0.9764 - val_loss: 37.3123 - val_f1_metric: 0.3035 - val_bin_acc: 0.9669\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.34939\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.0006995579411273232.\n",
      "175/175 [==============================] - 2283s 13s/step - loss: 47.6989 - f1_metric: 0.3930 - bin_acc: 0.9766 - val_loss: 34.5073 - val_f1_metric: 0.2292 - val_bin_acc: 0.9683\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.34939\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.0006995009705209228.\n",
      "175/175 [==============================] - 2335s 13s/step - loss: 44.5769 - f1_metric: 0.4179 - bin_acc: 0.9775 - val_loss: 30.8370 - val_f1_metric: 0.2981 - val_bin_acc: 0.9705\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.34939\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.0006994405505303908.\n",
      "175/175 [==============================] - 2306s 13s/step - loss: 48.7374 - f1_metric: 0.3831 - bin_acc: 0.9764 - val_loss: 33.2709 - val_f1_metric: 0.2640 - val_bin_acc: 0.9696\n",
      "\n",
      "Epoch 00019: val_f1_metric did not improve from 0.34939\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.0006993766817520478.\n",
      "175/175 [==============================] - 2326s 13s/step - loss: 46.1914 - f1_metric: 0.4024 - bin_acc: 0.9765 - val_loss: 37.1643 - val_f1_metric: 0.2972 - val_bin_acc: 0.9687\n",
      "\n",
      "Epoch 00020: val_f1_metric did not improve from 0.34939\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.0006993093648162529.\n",
      "175/175 [==============================] - 2362s 13s/step - loss: 46.6576 - f1_metric: 0.3922 - bin_acc: 0.9768 - val_loss: 37.5755 - val_f1_metric: 0.3199 - val_bin_acc: 0.9678\n",
      "\n",
      "Epoch 00021: val_f1_metric did not improve from 0.34939\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.0006992386003873973.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-1:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "H = model.fit_generator(\n",
    "    generator=training_batch(batch_size=batch_size,X_train1=train_emb_api,X_train2=train_emb ,\n",
    "                                             Y_train1=train_rep_ans,Y_train2=train_fam_ans) #Y_train2\n",
    "#                     generator=training_batch(batch_size=batch_size,X_train1=valid_emb_api,X_train2=valid_emb ,\n",
    "#                                              Y_train1=train_rep_ans,Y_train2=train_fam_ans)\n",
    "                        , steps_per_epoch=int(np.ceil(len(train_emb_api)/batch_size)) ,\n",
    "                    epochs=num_epochs,callbacks=model_callbacks\n",
    "#                    ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans,valid_fam_ans]) #ori\n",
    "#                    ,validation_data= (valid_emb, valid_fam_ans) \n",
    "                   ,validation_data= ([valid_emb_api,valid_emb], [valid_fam_ans]) #ori\n",
    "#                    ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans]+valid_Y2) #validY2\n",
    "                    ,max_queue_size=10  ,class_weight=fam_weights\n",
    "                    ,workers=10,use_multiprocessing=True   \n",
    "                   ,shuffle=True,verbose=1)\n",
    "model.save(model_save_path+\"_all.h5\")\n",
    "#1st:train 0_1_prediction=0.14XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json, model_from_yaml\n",
    "json_string = model.to_json()\n",
    "yaml_string = model.to_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights('./model/LSTM_att/1stStage_44fam_0610.h5')\n",
    "# model.load_weights('./model/att_clf/1stStage_44fam_0611_copy1.h5')\n",
    "# score = model.evaluate([valid_emb_api,valid_emb], [valid_rep_ans]+valid_Y2)\n",
    "print(len(test_emb_api)) #改\n",
    "ans = model.predict([test_emb_api[:64],test_emb[:64]]) #改\n",
    "y_true = test_fam_ans #改\n",
    "# ans = model.predict([valid_emb_api,valid_emb])\n",
    "len(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rep_ans[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.save_weights('./model/LSTM_att/test4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ = model#.layers[-2]\n",
    "layer_name = 'lambda_1' #lambda_1 multiply_1  #9~12\n",
    "intermediate_layer_model = Model(inputs=model_.inputs,\n",
    "                                 outputs=model_.layers[-3].output)\n",
    "intermediate_output = intermediate_layer_model.predict([valid_emb_api,valid_emb])\n",
    "intermediate_output[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(intermediate_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output[113].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intermediate_output[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary() #multiply_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "# y_true = np.squeeze(test_fam_ans)\n",
    "# y_true = np.squeeze(valid_fam_ans1)\n",
    "# y_pred = np.squeeze(predict_fam)\n",
    "final_ans = []\n",
    "for sample in ans:\n",
    "    sample_ans = []\n",
    "    for value in sample:\n",
    "        if value < 0.2:\n",
    "            sample_ans.append(0)\n",
    "        else:\n",
    "            sample_ans.append(1)\n",
    "    final_ans.append(sample_ans)\n",
    "final_ans = np.array(final_ans)\n",
    "print(final_ans.shape , sum(final_ans[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_fam_ans\n",
    "print(y_true.shape , final_ans.shape)\n",
    "recall = recall_score(y_true=y_true, y_pred=final_ans, average='weighted')\n",
    "precision = precision_score(y_true=y_true, y_pred=final_ans, average='weighted')\n",
    "f1 = f1_score(y_true=y_true, y_pred=final_ans, average='weighted')\n",
    "recall ,precision, f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
