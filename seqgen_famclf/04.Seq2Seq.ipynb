{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,shutil,pickle,tqdm,sys,random,re,string,pause, datetime\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "# from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "\n",
    "# from collections import Counter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import unicodedata as udata\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "from keras import backend \n",
    "# from python.keras import backend \n",
    "# Embedding(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test of sent2vec vector: (272, 213, 700) (272, 213) (272, 87) (272, 213, 1)\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_emb_api, train_fam_ans, train_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/TRAIN_vec.pkl','rb'))\n",
    "valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/DEV_vec.pkl','rb'))\n",
    "test_emb, test_emb_api,test_fam_ans,test_rep_ans = pickle.load(open('data/tree-rep-profiles-partial/TEST_vec.pkl','rb'))\n",
    "# print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "# print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "train_rep_ans = np.expand_dims(train_rep_ans,axis=-1)\n",
    "valid_rep_ans = np.expand_dims(valid_rep_ans,axis=-1)\n",
    "test_rep_ans = np.expand_dims(test_rep_ans,axis=-1)\n",
    "print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_fam_ans.shape,test_rep_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, X2 ,X3,X4):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], X2[randomize],X3[randomize],X4[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train of sent2vec vector: (11925, 213, 700) (11925, 213) (11925, 87) (11925, 213, 1)\n",
      "valid of sent2vec vector: (336, 213, 700) (336, 213) (336, 87) (336, 213, 1)\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_emb_api, train_fam_ans, train_rep_ans = _shuffle(train_emb, train_emb_api, train_fam_ans, train_rep_ans)\n",
    "valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans = _shuffle(valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans)\n",
    "# test_emb, test_emb_api,test_fam_ans,test_rep_ans  = _shuffle(test_emb,test_emb_api,test_fam_ans,test_rep_ans)\n",
    "print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "# print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_fam_ans.shape,test_rep_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n",
      "(336, 213, 700)\n"
     ]
    }
   ],
   "source": [
    "# scale = 'no'\n",
    "\n",
    "def scaling(trainX,validX,testX,scale='min_max'):\n",
    "#     if scale == 'min_max':\n",
    "    max_value = max([np.max(trainX) , np.max(validX),np.max(testX)])\n",
    "    min_value = min([np.min(trainX),np.min(validX),np.min(testX)])\n",
    "\n",
    "    trainX = (trainX - min_value) / (max_value - min_value)\n",
    "    validX = (validX - min_value) / (max_value - min_value )\n",
    "    testX = (testX - min_value) / (max_value - min_value )\n",
    "    print(np.max(trainX),np.max(validX))\n",
    "    return trainX,validX,testX , max_value , min_value\n",
    "\n",
    "train_emb,valid_emb,test_emb , max_value,min_value = scaling(train_emb,valid_emb,test_emb)   \n",
    "print(valid_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "input: 213 output_length: 213\n"
     ]
    }
   ],
   "source": [
    "#parameter\n",
    "# opt=Adam(decay=1e-20,amsgrad=False)\n",
    "opt=Nadam()\n",
    "batchSize=64#256\n",
    "patien=15\n",
    "epoch=300\n",
    "hidden_dims=350\n",
    "io_dim=700\n",
    "input_lengths=train_emb.shape[1] #profile_Q3\n",
    "output_lengths= train_rep_ans.shape[1]#rep_max size\n",
    "depths=2\n",
    "dp = 0.05\n",
    "saveP = 'model/NotScaleAttentionSeq2Seq_Param_'+str(io_dim)+'_'+str(input_lengths)+'_'+str(output_lengths)+'_'+str(batchSize)+'_'+str(hidden_dims)+'_'+str(depths)+'_'+str(dp)+'.h5'\n",
    "logD = './logs/NotScaleAS2S_Param/'+str(batchSize)+'_'+str(hidden_dims)+'_'+str(depths)\n",
    "history = History()\n",
    "print(\"input:\",train_emb.shape[1],'output_length:',train_rep_ans.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型參數\n",
    "l2_reg_penalty = 1e-4#1e-4\n",
    "embedding_dropout = 0.6\n",
    "transformer_dropout = 0.1\n",
    "use_universal_transformer = True #true=>ACT\n",
    "\n",
    "transformer_depth = 2\n",
    "vocabulary_size = 26 #api name種類\n",
    "max_seq_length = test_emb.shape[1] # profile最大長度\n",
    "word_embedding_size = test_emb.shape[2]#被除數，跟Sen2Vec最終維度相同\n",
    "num_heads = 2#除數，要整除\n",
    "fam_num = test_fam_ans.shape[1]#test_fam_ans.shape[1] MML\n",
    "batch_size = 256 #128\n",
    "\n",
    "CONFIDENCE_PENALTY = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* REF1: https://github.com/andreyzharkov/keras-monotonic-attention\n",
    "* REF2: https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39\n",
    "    * https://github.com/thushv89/attention_keras\n",
    "* REF3: https://medium.com/@jbetker/implementing-seq2seq-with-attention-in-keras-63565c8e498c\n",
    "    * https://github.com/neonbjb/ml-notebooks/tree/master/keras-seq2seq-with-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from attention_decoder import AttentionDecoder\n",
    "from keras.models import Model\n",
    "from attention_decoder import AttentionDecoder\n",
    "from position_embedding import PositionEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids1 = Input(shape=(max_seq_length,), dtype='int64', name='sent_ids') # 輸入的api funvtion name ID\n",
    "# sent_ids = Masking(mask_value=0)(sent_ids1)\n",
    "sentemb1 = Input(shape=(max_seq_length,word_embedding_size),name='sent_emb') #輸入Sent2Vec的embeeding (長度,維度大小)\n",
    "# sentemb = Masking(mask_value=0)(sentemb1)\n",
    "ans_input = Input(shape=(max_seq_length,), dtype='int64', name='ans_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = Embedding(vocabulary_size, vocabulary_size, trainable=True)(sent_ids1) # weights=[np.eye(n_labels)]\n",
    "# pos_emb = PositionEmbedding(max_time=1000, n_waves=max_seq_length/2, d_model=40)(embedded)\n",
    "nnet = concatenate([embedded, sentemb1], axis=-1)\n",
    "# nnet = Concatenate()([sent_ids, pos_emb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attention_decoder = AttentionDecoder(fam_num, 1,\n",
    "                                     embedding_dim=700,\n",
    "                                     is_monotonic=False,\n",
    "                                     normalize_energy=False)\n",
    "output = attention_decoder([nnet, ans_input])\n",
    "family_input = multiply_embedding_layer([output,nnet])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_ids (InputLayer)           (None, 213)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 213, 26)      676         sent_ids[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sent_emb (InputLayer)           (None, 213, 700)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 213, 726)     0           embedding_1[0][0]                \n",
      "                                                                 sent_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "ans_ids (InputLayer)            (None, 213)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "AttentionDecoder (AttentionDeco (None, 213, 1)       530735      concatenate_1[0][0]              \n",
      "                                                                 ans_ids[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 531,411\n",
      "Trainable params: 531,411\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[sent_ids1, sentemb1,ans_input], outputs=[output])\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multi_gpu_model(model,gpus=3)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Nadam(),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 11925 samples, validate on 336 samples\n",
      "Epoch 1/50\n",
      "11925/11925 [==============================] - 61s 5ms/step - loss: 13.1514 - acc: 0.1751 - val_loss: 11.8307 - val_acc: 0.2579\n",
      "Epoch 2/50\n",
      "11925/11925 [==============================] - 58s 5ms/step - loss: 13.1514 - acc: 0.1751 - val_loss: 11.8307 - val_acc: 0.2579\n",
      "Epoch 3/50\n",
      "11925/11925 [==============================] - 58s 5ms/step - loss: 13.1514 - acc: 0.1751 - val_loss: 11.8307 - val_acc: 0.2579\n",
      "Epoch 4/50\n",
      "11925/11925 [==============================] - 58s 5ms/step - loss: 13.1514 - acc: 0.1751 - val_loss: 11.8307 - val_acc: 0.2579\n",
      "Epoch 5/50\n",
      "11925/11925 [==============================] - 58s 5ms/step - loss: 13.1514 - acc: 0.1751 - val_loss: 11.8307 - val_acc: 0.2579\n",
      "Epoch 6/50\n",
      "11925/11925 [==============================] - 58s 5ms/step - loss: 13.1514 - acc: 0.1751 - val_loss: 11.8307 - val_acc: 0.2579\n",
      "Epoch 7/50\n",
      "11925/11925 [==============================] - 58s 5ms/step - loss: 13.1514 - acc: 0.1751 - val_loss: 11.8307 - val_acc: 0.2579\n",
      "Epoch 8/50\n",
      "11925/11925 [==============================] - 58s 5ms/step - loss: 13.1514 - acc: 0.1751 - val_loss: 11.8307 - val_acc: 0.2579\n",
      "Epoch 9/50\n",
      " 1152/11925 [=>............................] - ETA: 50s - loss: 12.9416 - acc: 0.1882"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d6013b98a423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit([train_emb_api, train_emb,np.squeeze(train_rep_ans, axis=-1)], train_rep_ans,\n\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           validation_data=([valid_emb_api, valid_emb,np.squeeze(valid_rep_ans, axis=-1)], valid_rep_ans),batch_size=128)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 array_vals.append(\n\u001b[1;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2655\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([train_emb_api, train_emb,np.squeeze(train_rep_ans, axis=-1)], train_rep_ans,\n",
    "          epochs=50, shuffle=True,\n",
    "          validation_data=([valid_emb_api, valid_emb,np.squeeze(valid_rep_ans, axis=-1)], valid_rep_ans),batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 213, 700)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 213, 350)     4905600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recurrent_sequential_2 (Recurre (None, 213, 1)       1477003     bidirectional_1[0][0]            \n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "__________________________________________________________________________________________________\n",
      "multiply_67 (Multiply)          (None, 213, 700)     0           input_1[0][0]                    \n",
      "                                                                 recurrent_sequential_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 213, 700)     2800        multiply_67[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 213, 174)     548448      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 213, 874)     0           bidirectional_2[0][0]            \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 213, 874)     3496        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 174)          502164      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "family (Dense)                  (None, 87)           15225       bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 7,454,736\n",
      "Trainable params: 7,451,588\n",
      "Non-trainable params: 3,148\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = AttentionSeq2Seq(input_dim=io_dim, input_length=input_lengths, hidden_dim=hidden_dims, \n",
    "                         output_length=output_lengths, output_dim=1, depth=depths, dropout=dp)\n",
    "# model = multi_gpu_model(model,gpus=3)\n",
    "\n",
    "emb = model.input\n",
    "byte = model.output\n",
    "\n",
    "l2_regularizer = (regularizers.l2(l2_reg_penalty) if l2_reg_penalty else None)\n",
    "\n",
    "family_input = Multiply()([emb,byte])\n",
    "family_input = BatchNormalization()(family_input)\n",
    "family_prediction = Bidirectional(LSTM(fam_num,kernel_regularizer=l2_regularizer,\n",
    "                                            recurrent_regularizer=l2_regularizer,return_sequences=True,\n",
    "                                           dropout=transformer_dropout, #stateful=True,\n",
    "                                            recurrent_dropout=transformer_dropout,name='fam_feature'))(family_input)\n",
    "family_prediction = Concatenate()([family_prediction,family_input])\n",
    "family_prediction = BatchNormalization()(family_prediction)\n",
    "family_prediction = (\n",
    "        Bidirectional(GRU(fam_num,name='fam_clf'))(family_prediction) #int((word_embedding_size+vocabulary_size)/4)\n",
    ") # 2nd stage，變成bidirectional加dense?，變成兩層?加L2\n",
    "family_prediction = Dense(fam_num,activation='sigmoid',name='family')(family_prediction)\n",
    "model = Model(inputs=model.input,outputs=family_prediction)\n",
    "\"\"\"\n",
    "# model = multi_gpu_model(model,gpus=2)\n",
    "# model.add(LSTM(2))\n",
    "# rnn = LSTM(64,input_shape =(502,128) )(model)\n",
    "mm = Sequential()\n",
    "mm.add(GRU(33,input_shape=model.output_shape[1:]))\n",
    "kk= Sequential()\n",
    "kk.add(GRU(66,input_shape=model.output_shape[1:]))\n",
    "model = Model(inputs=model.input,outputs=[mm(model.output),kk(model.output)])\n",
    "# rnn = LSTM(64,input_shape =(502,128) )(model)\n",
    "# model.add(LSTM(2))\n",
    "\"\"\"\n",
    "model.summary()\n",
    "model = multi_gpu_model(model,gpus=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 11925 samples, validate on 336 samples\n",
      "Epoch 1/300\n",
      "11925/11925 [==============================] - 1267s 106ms/step - loss: 0.9713 - f1: 0.0465 - val_loss: 0.9796 - val_f1: 0.0209\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.97959, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 2/300\n",
      "11925/11925 [==============================] - 1234s 104ms/step - loss: 0.9304 - f1: 0.0808 - val_loss: 0.9588 - val_f1: 0.0473\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.97959 to 0.95876, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 3/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.9160 - f1: 0.0942 - val_loss: 0.9777 - val_f1: 0.0297\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.95876\n",
      "Epoch 4/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.9156 - f1: 0.0980 - val_loss: 0.9619 - val_f1: 0.0411\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.95876\n",
      "Epoch 5/300\n",
      "11925/11925 [==============================] - 1232s 103ms/step - loss: 0.9093 - f1: 0.1009 - val_loss: 0.9638 - val_f1: 0.0443\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.95876\n",
      "Epoch 6/300\n",
      "11925/11925 [==============================] - 1234s 104ms/step - loss: 0.9024 - f1: 0.1061 - val_loss: 0.9584 - val_f1: 0.0438\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.95876 to 0.95840, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 7/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8997 - f1: 0.1063 - val_loss: 0.9562 - val_f1: 0.0446\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.95840 to 0.95616, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 8/300\n",
      "11925/11925 [==============================] - 1234s 104ms/step - loss: 0.8970 - f1: 0.1093 - val_loss: 0.9565 - val_f1: 0.0432\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.95616\n",
      "Epoch 9/300\n",
      "11925/11925 [==============================] - 1233s 103ms/step - loss: 0.8971 - f1: 0.1085 - val_loss: 0.9737 - val_f1: 0.0402\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.95616\n",
      "Epoch 10/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8988 - f1: 0.1099 - val_loss: 0.9560 - val_f1: 0.0455\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.95616 to 0.95604, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 11/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8945 - f1: 0.1102 - val_loss: 0.9591 - val_f1: 0.0422\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.95604\n",
      "Epoch 12/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.9156 - f1: 0.1095 - val_loss: 0.9859 - val_f1: 0.0511\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.95604\n",
      "Epoch 13/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.9078 - f1: 0.1115 - val_loss: 0.9609 - val_f1: 0.0422\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.95604\n",
      "Epoch 14/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8969 - f1: 0.1125 - val_loss: 0.9626 - val_f1: 0.0426\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.95604\n",
      "Epoch 15/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8943 - f1: 0.1120 - val_loss: 0.9511 - val_f1: 0.0517\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.95604 to 0.95111, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 16/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8904 - f1: 0.1138 - val_loss: 0.9501 - val_f1: 0.0551\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.95111 to 0.95005, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 17/300\n",
      "11925/11925 [==============================] - 1234s 104ms/step - loss: 0.8904 - f1: 0.1131 - val_loss: 0.9541 - val_f1: 0.0502\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.95005\n",
      "Epoch 18/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8895 - f1: 0.1138 - val_loss: 0.9495 - val_f1: 0.0528\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.95005 to 0.94947, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 19/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8898 - f1: 0.1131 - val_loss: 0.9518 - val_f1: 0.0514\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.94947\n",
      "Epoch 20/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8973 - f1: 0.1119 - val_loss: 0.9526 - val_f1: 0.0504\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.94947\n",
      "Epoch 21/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8901 - f1: 0.1132 - val_loss: 0.9482 - val_f1: 0.0508\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.94947 to 0.94819, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 22/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8893 - f1: 0.1133 - val_loss: 0.9493 - val_f1: 0.0539\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.94819\n",
      "Epoch 23/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8880 - f1: 0.1152 - val_loss: 0.9465 - val_f1: 0.0547\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.94819 to 0.94650, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 24/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8861 - f1: 0.1157 - val_loss: 0.9464 - val_f1: 0.0560\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.94650 to 0.94637, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 25/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8877 - f1: 0.1144 - val_loss: 0.9465 - val_f1: 0.0559\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.94637\n",
      "Epoch 26/300\n",
      "11925/11925 [==============================] - 1233s 103ms/step - loss: 0.8866 - f1: 0.1155 - val_loss: 0.9488 - val_f1: 0.0541\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.94637\n",
      "Epoch 27/300\n",
      "11925/11925 [==============================] - 1237s 104ms/step - loss: 0.8881 - f1: 0.1144 - val_loss: 0.9499 - val_f1: 0.0525\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.94637\n",
      "Epoch 28/300\n",
      "11925/11925 [==============================] - 1232s 103ms/step - loss: 0.8896 - f1: 0.1147 - val_loss: 0.9529 - val_f1: 0.0518\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.94637\n",
      "Epoch 29/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8882 - f1: 0.1147 - val_loss: 0.9436 - val_f1: 0.0580\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.94637 to 0.94357, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 30/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8872 - f1: 0.1158 - val_loss: 0.9484 - val_f1: 0.0547\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.94357\n",
      "Epoch 31/300\n",
      "11925/11925 [==============================] - 1233s 103ms/step - loss: 0.8882 - f1: 0.1139 - val_loss: 0.9469 - val_f1: 0.0581\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.94357\n",
      "Epoch 32/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8872 - f1: 0.1149 - val_loss: 0.9461 - val_f1: 0.0543\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.94357\n",
      "Epoch 33/300\n",
      "11925/11925 [==============================] - 1237s 104ms/step - loss: 0.8879 - f1: 0.1151 - val_loss: 0.9458 - val_f1: 0.0571\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.94357\n",
      "Epoch 34/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8857 - f1: 0.1162 - val_loss: 0.9458 - val_f1: 0.0556\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.94357\n",
      "Epoch 35/300\n",
      "11925/11925 [==============================] - 1237s 104ms/step - loss: 0.8857 - f1: 0.1158 - val_loss: 0.9448 - val_f1: 0.0567\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.94357\n",
      "Epoch 36/300\n",
      "11925/11925 [==============================] - 1234s 104ms/step - loss: 0.8857 - f1: 0.1159 - val_loss: 0.9472 - val_f1: 0.0517\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.94357\n",
      "Epoch 37/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8872 - f1: 0.1148 - val_loss: 0.9464 - val_f1: 0.0563\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.94357\n",
      "Epoch 38/300\n",
      "11925/11925 [==============================] - 1239s 104ms/step - loss: 0.8863 - f1: 0.1154 - val_loss: 0.9488 - val_f1: 0.0527\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.94357\n",
      "Epoch 39/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8895 - f1: 0.1137 - val_loss: 0.9433 - val_f1: 0.0565\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.94357 to 0.94334, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 40/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8865 - f1: 0.1155 - val_loss: 0.9459 - val_f1: 0.0560\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.94334\n",
      "Epoch 41/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8849 - f1: 0.1169 - val_loss: 0.9449 - val_f1: 0.0579\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.94334\n",
      "Epoch 42/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8860 - f1: 0.1155 - val_loss: 0.9460 - val_f1: 0.0566\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.94334\n",
      "Epoch 43/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8869 - f1: 0.1147 - val_loss: 0.9474 - val_f1: 0.0530\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.94334\n",
      "Epoch 44/300\n",
      "11925/11925 [==============================] - 1237s 104ms/step - loss: 0.8872 - f1: 0.1146 - val_loss: 0.9516 - val_f1: 0.0490\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.94334\n",
      "Epoch 45/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8867 - f1: 0.1159 - val_loss: 0.9460 - val_f1: 0.0561\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.94334\n",
      "Epoch 46/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8859 - f1: 0.1158 - val_loss: 0.9492 - val_f1: 0.0490\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.94334\n",
      "Epoch 47/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8855 - f1: 0.1159 - val_loss: 0.9456 - val_f1: 0.0551\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.94334\n",
      "Epoch 48/300\n",
      "11925/11925 [==============================] - 1233s 103ms/step - loss: 0.8854 - f1: 0.1162 - val_loss: 0.9427 - val_f1: 0.0565\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.94334 to 0.94270, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 49/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8848 - f1: 0.1169 - val_loss: 0.9444 - val_f1: 0.0573\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.94270\n",
      "Epoch 50/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8856 - f1: 0.1156 - val_loss: 0.9486 - val_f1: 0.0524\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.94270\n",
      "Epoch 51/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8845 - f1: 0.1171 - val_loss: 0.9439 - val_f1: 0.0538\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.94270\n",
      "Epoch 52/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8839 - f1: 0.1173 - val_loss: 0.9436 - val_f1: 0.0563\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.94270\n",
      "Epoch 53/300\n",
      "11925/11925 [==============================] - 1237s 104ms/step - loss: 0.8844 - f1: 0.1166 - val_loss: 0.9439 - val_f1: 0.0572\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.94270\n",
      "Epoch 54/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8855 - f1: 0.1157 - val_loss: 0.9450 - val_f1: 0.0566\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.94270\n",
      "Epoch 55/300\n",
      "11925/11925 [==============================] - 1236s 104ms/step - loss: 0.8850 - f1: 0.1167 - val_loss: 0.9447 - val_f1: 0.0562\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.94270\n",
      "Epoch 56/300\n",
      "11925/11925 [==============================] - 1234s 104ms/step - loss: 0.8830 - f1: 0.1182 - val_loss: 0.9428 - val_f1: 0.0568\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.94270\n",
      "Epoch 57/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8852 - f1: 0.1160 - val_loss: 0.9479 - val_f1: 0.0569\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.94270\n",
      "Epoch 58/300\n",
      "11925/11925 [==============================] - 1238s 104ms/step - loss: 0.8882 - f1: 0.1151 - val_loss: 0.9441 - val_f1: 0.0563\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.94270\n",
      "Epoch 59/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8859 - f1: 0.1164 - val_loss: 0.9432 - val_f1: 0.0557\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.94270\n",
      "Epoch 60/300\n",
      "11925/11925 [==============================] - 1234s 103ms/step - loss: 0.8852 - f1: 0.1159 - val_loss: 0.9418 - val_f1: 0.0620\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.94270 to 0.94180, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 61/300\n",
      "11925/11925 [==============================] - 1235s 104ms/step - loss: 0.8835 - f1: 0.1174 - val_loss: 0.9404 - val_f1: 0.0618\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.94180 to 0.94041, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 62/300\n",
      "11925/11925 [==============================] - 1186s 99ms/step - loss: 0.8844 - f1: 0.1165 - val_loss: 0.9399 - val_f1: 0.0619\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.94041 to 0.93994, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 63/300\n",
      "11925/11925 [==============================] - 1136s 95ms/step - loss: 0.8853 - f1: 0.1153 - val_loss: 0.9398 - val_f1: 0.0621\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.93994 to 0.93979, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 64/300\n",
      "11925/11925 [==============================] - 1152s 97ms/step - loss: 0.8834 - f1: 0.1173 - val_loss: 0.9440 - val_f1: 0.0548\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.93979\n",
      "Epoch 65/300\n",
      "11925/11925 [==============================] - 1139s 96ms/step - loss: 0.8831 - f1: 0.1177 - val_loss: 0.9410 - val_f1: 0.0585\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.93979\n",
      "Epoch 66/300\n",
      "11925/11925 [==============================] - 1138s 95ms/step - loss: 0.8839 - f1: 0.1167 - val_loss: 0.9415 - val_f1: 0.0570\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.93979\n",
      "Epoch 67/300\n",
      "11925/11925 [==============================] - 1142s 96ms/step - loss: 0.8831 - f1: 0.1174 - val_loss: 0.9393 - val_f1: 0.0623\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.93979 to 0.93928, saving model to model/NotScaleAttentionSeq2Seq_Param_700_213_213_64_350_2_0.05.h5\n",
      "Epoch 68/300\n",
      "11925/11925 [==============================] - 1136s 95ms/step - loss: 0.8821 - f1: 0.1183 - val_loss: 0.9396 - val_f1: 0.0619\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.93928\n",
      "Epoch 69/300\n",
      "11925/11925 [==============================] - 1136s 95ms/step - loss: 0.8832 - f1: 0.1171 - val_loss: 0.9402 - val_f1: 0.0619\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.93928\n",
      "Epoch 70/300\n",
      "11925/11925 [==============================] - 1173s 98ms/step - loss: 0.8826 - f1: 0.1177 - val_loss: 0.9405 - val_f1: 0.0587\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.93928\n",
      "Epoch 71/300\n",
      "   64/11925 [..............................] - ETA: 20:25 - loss: 0.9003 - f1: 0.0994"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ec607596a303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_fam_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 )\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveP\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_all.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=opt, loss=f1_loss, metrics=[f1])#binary_focal_loss(gamma=2., alpha=.25)\n",
    "callback=[ \n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/1.5),min_lr=1e-4,mode='min' ),\n",
    "    EarlyStopping(patience=patien,monitor='val_loss',verbose=1),\n",
    "    ModelCheckpoint(saveP,monitor='val_loss',verbose=1,save_best_only=True, save_weights_only=True),\n",
    "    TensorBoard(log_dir=logD), \n",
    "    history,\n",
    "]\n",
    "model.fit(train_emb, train_fam_ans,\n",
    "                epochs=epoch,\n",
    "                batch_size=batchSize,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_emb, valid_fam_ans),\n",
    "                callbacks=callback, \n",
    "                class_weight='auto'\n",
    "                )\n",
    "model.save(saveP+\"_all.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saveP+\"_all.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_model = model.layers[-2]\n",
    "single_model.save(saveP+\"_all.h5_single\")\n",
    "single_model.save_weights((saveP+\"_single\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_emb (InputLayer)           (None, 239, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_40 (LSTM)                  [(None, 239, 384), ( 1771008     sent_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 768)          0           lstm_40[0][1]                    \n",
      "                                                                 lstm_40[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 239)          183791      concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 239, 1)       0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_27 (Multiply)          (None, 239, 384)     0           lambda_9[0][0]                   \n",
      "                                                                 lstm_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 384)          664704      multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 1)            385         bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "                                                                 bidirectional_1[2][0]            \n",
      "                                                                 bidirectional_1[3][0]            \n",
      "                                                                 bidirectional_1[4][0]            \n",
      "                                                                 bidirectional_1[5][0]            \n",
      "                                                                 bidirectional_1[6][0]            \n",
      "                                                                 bidirectional_1[7][0]            \n",
      "                                                                 bidirectional_1[8][0]            \n",
      "                                                                 bidirectional_1[9][0]            \n",
      "                                                                 bidirectional_1[10][0]           \n",
      "                                                                 bidirectional_1[11][0]           \n",
      "                                                                 bidirectional_1[12][0]           \n",
      "                                                                 bidirectional_1[13][0]           \n",
      "                                                                 bidirectional_1[14][0]           \n",
      "                                                                 bidirectional_1[15][0]           \n",
      "                                                                 bidirectional_1[16][0]           \n",
      "                                                                 bidirectional_1[17][0]           \n",
      "                                                                 bidirectional_1[18][0]           \n",
      "                                                                 bidirectional_1[19][0]           \n",
      "                                                                 bidirectional_1[20][0]           \n",
      "                                                                 bidirectional_1[21][0]           \n",
      "                                                                 bidirectional_1[22][0]           \n",
      "                                                                 bidirectional_1[23][0]           \n",
      "                                                                 bidirectional_1[24][0]           \n",
      "                                                                 bidirectional_1[25][0]           \n",
      "                                                                 bidirectional_1[26][0]           \n",
      "                                                                 bidirectional_1[27][0]           \n",
      "                                                                 bidirectional_1[28][0]           \n",
      "                                                                 bidirectional_1[29][0]           \n",
      "                                                                 bidirectional_1[30][0]           \n",
      "                                                                 bidirectional_1[31][0]           \n",
      "                                                                 bidirectional_1[32][0]           \n",
      "                                                                 bidirectional_1[33][0]           \n",
      "                                                                 bidirectional_1[34][0]           \n",
      "                                                                 bidirectional_1[35][0]           \n",
      "                                                                 bidirectional_1[36][0]           \n",
      "                                                                 bidirectional_1[37][0]           \n",
      "                                                                 bidirectional_1[38][0]           \n",
      "                                                                 bidirectional_1[39][0]           \n",
      "                                                                 bidirectional_1[40][0]           \n",
      "                                                                 bidirectional_1[41][0]           \n",
      "                                                                 bidirectional_1[42][0]           \n",
      "                                                                 bidirectional_1[43][0]           \n",
      "                                                                 bidirectional_1[44][0]           \n",
      "                                                                 bidirectional_1[45][0]           \n",
      "                                                                 bidirectional_1[46][0]           \n",
      "                                                                 bidirectional_1[47][0]           \n",
      "                                                                 bidirectional_1[48][0]           \n",
      "                                                                 bidirectional_1[49][0]           \n",
      "                                                                 bidirectional_1[50][0]           \n",
      "                                                                 bidirectional_1[51][0]           \n",
      "                                                                 bidirectional_1[52][0]           \n",
      "                                                                 bidirectional_1[53][0]           \n",
      "                                                                 bidirectional_1[54][0]           \n",
      "                                                                 bidirectional_1[55][0]           \n",
      "                                                                 bidirectional_1[56][0]           \n",
      "                                                                 bidirectional_1[57][0]           \n",
      "                                                                 bidirectional_1[58][0]           \n",
      "                                                                 bidirectional_1[59][0]           \n",
      "                                                                 bidirectional_1[60][0]           \n",
      "                                                                 bidirectional_1[61][0]           \n",
      "                                                                 bidirectional_1[62][0]           \n",
      "                                                                 bidirectional_1[63][0]           \n",
      "                                                                 bidirectional_1[64][0]           \n",
      "                                                                 bidirectional_1[65][0]           \n",
      "                                                                 bidirectional_1[66][0]           \n",
      "                                                                 bidirectional_1[67][0]           \n",
      "                                                                 bidirectional_1[68][0]           \n",
      "                                                                 bidirectional_1[69][0]           \n",
      "                                                                 bidirectional_1[70][0]           \n",
      "                                                                 bidirectional_1[71][0]           \n",
      "                                                                 bidirectional_1[72][0]           \n",
      "                                                                 bidirectional_1[73][0]           \n",
      "                                                                 bidirectional_1[74][0]           \n",
      "                                                                 bidirectional_1[75][0]           \n",
      "                                                                 bidirectional_1[76][0]           \n",
      "                                                                 bidirectional_1[77][0]           \n",
      "                                                                 bidirectional_1[78][0]           \n",
      "                                                                 bidirectional_1[79][0]           \n",
      "                                                                 bidirectional_1[80][0]           \n",
      "                                                                 bidirectional_1[81][0]           \n",
      "                                                                 bidirectional_1[82][0]           \n",
      "                                                                 bidirectional_1[83][0]           \n",
      "                                                                 bidirectional_1[84][0]           \n",
      "                                                                 bidirectional_1[85][0]           \n",
      "                                                                 bidirectional_1[86][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 87)           0           dense_38[0][0]                   \n",
      "                                                                 dense_38[1][0]                   \n",
      "                                                                 dense_38[2][0]                   \n",
      "                                                                 dense_38[3][0]                   \n",
      "                                                                 dense_38[4][0]                   \n",
      "                                                                 dense_38[5][0]                   \n",
      "                                                                 dense_38[6][0]                   \n",
      "                                                                 dense_38[7][0]                   \n",
      "                                                                 dense_38[8][0]                   \n",
      "                                                                 dense_38[9][0]                   \n",
      "                                                                 dense_38[10][0]                  \n",
      "                                                                 dense_38[11][0]                  \n",
      "                                                                 dense_38[12][0]                  \n",
      "                                                                 dense_38[13][0]                  \n",
      "                                                                 dense_38[14][0]                  \n",
      "                                                                 dense_38[15][0]                  \n",
      "                                                                 dense_38[16][0]                  \n",
      "                                                                 dense_38[17][0]                  \n",
      "                                                                 dense_38[18][0]                  \n",
      "                                                                 dense_38[19][0]                  \n",
      "                                                                 dense_38[20][0]                  \n",
      "                                                                 dense_38[21][0]                  \n",
      "                                                                 dense_38[22][0]                  \n",
      "                                                                 dense_38[23][0]                  \n",
      "                                                                 dense_38[24][0]                  \n",
      "                                                                 dense_38[25][0]                  \n",
      "                                                                 dense_38[26][0]                  \n",
      "                                                                 dense_38[27][0]                  \n",
      "                                                                 dense_38[28][0]                  \n",
      "                                                                 dense_38[29][0]                  \n",
      "                                                                 dense_38[30][0]                  \n",
      "                                                                 dense_38[31][0]                  \n",
      "                                                                 dense_38[32][0]                  \n",
      "                                                                 dense_38[33][0]                  \n",
      "                                                                 dense_38[34][0]                  \n",
      "                                                                 dense_38[35][0]                  \n",
      "                                                                 dense_38[36][0]                  \n",
      "                                                                 dense_38[37][0]                  \n",
      "                                                                 dense_38[38][0]                  \n",
      "                                                                 dense_38[39][0]                  \n",
      "                                                                 dense_38[40][0]                  \n",
      "                                                                 dense_38[41][0]                  \n",
      "                                                                 dense_38[42][0]                  \n",
      "                                                                 dense_38[43][0]                  \n",
      "                                                                 dense_38[44][0]                  \n",
      "                                                                 dense_38[45][0]                  \n",
      "                                                                 dense_38[46][0]                  \n",
      "                                                                 dense_38[47][0]                  \n",
      "                                                                 dense_38[48][0]                  \n",
      "                                                                 dense_38[49][0]                  \n",
      "                                                                 dense_38[50][0]                  \n",
      "                                                                 dense_38[51][0]                  \n",
      "                                                                 dense_38[52][0]                  \n",
      "                                                                 dense_38[53][0]                  \n",
      "                                                                 dense_38[54][0]                  \n",
      "                                                                 dense_38[55][0]                  \n",
      "                                                                 dense_38[56][0]                  \n",
      "                                                                 dense_38[57][0]                  \n",
      "                                                                 dense_38[58][0]                  \n",
      "                                                                 dense_38[59][0]                  \n",
      "                                                                 dense_38[60][0]                  \n",
      "                                                                 dense_38[61][0]                  \n",
      "                                                                 dense_38[62][0]                  \n",
      "                                                                 dense_38[63][0]                  \n",
      "                                                                 dense_38[64][0]                  \n",
      "                                                                 dense_38[65][0]                  \n",
      "                                                                 dense_38[66][0]                  \n",
      "                                                                 dense_38[67][0]                  \n",
      "                                                                 dense_38[68][0]                  \n",
      "                                                                 dense_38[69][0]                  \n",
      "                                                                 dense_38[70][0]                  \n",
      "                                                                 dense_38[71][0]                  \n",
      "                                                                 dense_38[72][0]                  \n",
      "                                                                 dense_38[73][0]                  \n",
      "                                                                 dense_38[74][0]                  \n",
      "                                                                 dense_38[75][0]                  \n",
      "                                                                 dense_38[76][0]                  \n",
      "                                                                 dense_38[77][0]                  \n",
      "                                                                 dense_38[78][0]                  \n",
      "                                                                 dense_38[79][0]                  \n",
      "                                                                 dense_38[80][0]                  \n",
      "                                                                 dense_38[81][0]                  \n",
      "                                                                 dense_38[82][0]                  \n",
      "                                                                 dense_38[83][0]                  \n",
      "                                                                 dense_38[84][0]                  \n",
      "                                                                 dense_38[85][0]                  \n",
      "                                                                 dense_38[86][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,619,888\n",
      "Trainable params: 2,619,888\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 768\n",
    "max_length = 239\n",
    "fam_num = 87\n",
    "sentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\n",
    "timesteps,state_h,state_c = LSTM(int(emb_dim/2),return_sequences=True,return_state=True)(sentemb1)\n",
    "state = Concatenate()([state_h,state_c])\n",
    "fc = Dense(max_length,activation='sigmoid')(state)\n",
    "fc = Lambda(lambda x: keras.backend.expand_dims(x,axis=-1))(fc)\n",
    "# fc = keras.backend.repeat_elements(fc,256,axis=-1)\n",
    "# fc = keras.backend.expand_dims(fc,axis=-1)\n",
    "mul = Multiply()([fc,timesteps])\n",
    "alls = []\n",
    "gru = Bidirectional(GRU(int(emb_dim/4))) #/8\n",
    "dense = Dense(1,activation='sigmoid')\n",
    "for i in range(fam_num):\n",
    "    alls.append(dense(gru(mul)))\n",
    "out = Concatenate()(alls)\n",
    "model = Model(inputs=sentemb1, outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: <keras.engine.training.Model object at 0x7ff526946f60>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f4f84179c6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0malls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# out = Concatenate()(alls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentemb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    186\u001b[0m                                  \u001b[0;34m'the output of a Keras `Layer` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                                  \u001b[0;34m'(thus holding past layer metadata). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                                  'Found: ' + str(x))\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         self._compute_previous_mask = (\n",
      "\u001b[0;31mValueError\u001b[0m: Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: <keras.engine.training.Model object at 0x7ff526946f60>"
     ]
    }
   ],
   "source": [
    "'''emb_dim = 768\n",
    "max_length = 239\n",
    "fam_num = 87\n",
    "sentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\n",
    "timesteps,state_h,state_c = LSTM(int(emb_dim/2),return_sequences=True,return_state=True)(sentemb1)\n",
    "state = Concatenate()([state_h,state_c])\n",
    "fc = Dense(max_length,activation='sigmoid')(state)\n",
    "fc = Lambda(lambda x: keras.backend.expand_dims(x,axis=-1))(fc)\n",
    "# fc = keras.backend.repeat_elements(fc,256,axis=-1)\n",
    "# fc = keras.backend.expand_dims(fc,axis=-1)\n",
    "mul = Multiply()([fc,timesteps])\n",
    "\n",
    "shared = Model(inputs=sentemb1,outputs=mul)\n",
    "\n",
    "alls = []\n",
    "gru = Bidirectional(GRU(int(emb_dim/4))) #/8\n",
    "dense = Dense(1,activation='sigmoid')\n",
    "for i in range(fam_num):\n",
    "#     kk= Sequential()\n",
    "#     kk.add(gru)\n",
    "#     kk.add()\n",
    "    kk = dense(gru(shared.output))\n",
    "    alls.append(Model(inputs=shared.input,outputs=kk))\n",
    "# out = Concatenate()(alls)\n",
    "model = Model(inputs=sentemb1, outputs=alls)\n",
    "model.summary()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
