{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os,shutil,pickle,tqdm,sys,random,re,string,pause, datetime,glob\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "# from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "\n",
    "# from collections import Counter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import unicodedata as udata\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "from keras import backend \n",
    "from imblearn.ensemble import *\n",
    "from imblearn.combine import *\n",
    "# from python.keras import backend \n",
    "# Embedding(10,20)\n",
    "from keras_transformer.extras import ReusableEmbedding, TiedOutputEmbedding\n",
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "from keras_transformer.bert import (\n",
    "    BatchGeneratorForBERT, masked_perplexity,\n",
    "    MaskedPenalizedSparseCategoricalCrossentropy)\n",
    "\n",
    "import keras_metrics as km\n",
    "from keras_trans_mask import RemoveMask, RestoreMask\n",
    "\n",
    "from keras_multi_head import *\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import transformer_bert_model\n",
    "from bpe import BPEEncoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test of sent2vec vector: (508, 226, 768) (508, 226) (508, 226, 1)\n"
     ]
    }
   ],
   "source": [
    "#Phase1\n",
    "root_dir = 'data/tree-rep-profiles_o2o/'\n",
    "train_emb_api,train_emb , train_rep_ans = pickle.load(open(root_dir + 'pids_o2o.pkl','rb'))\n",
    "valid_emb_api,valid_emb, valid_rep_ans = pickle.load(open(root_dir + 'pids_valid.pkl','rb'))\n",
    "test_emb_api,test_emb ,test_rep_ans = pickle.load(open(root_dir + 'pids_test.pkl','rb'))\n",
    "exp_api,exp_emb = pickle.load(open(root_dir + 'pids_exp.pkl','rb'))\n",
    "# print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "# print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "train_rep_ans = np.expand_dims(train_rep_ans,axis=-1)\n",
    "valid_rep_ans = np.expand_dims(valid_rep_ans,axis=-1)\n",
    "test_rep_ans = np.expand_dims(test_rep_ans,axis=-1)\n",
    "print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_rep_ans.shape)\n",
    "emb_matrix = pickle.load(open('data/api_emb_matrix.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase2 補load:\n",
    "train_emb_api,train_emb , train_rep_ans = pickle.load(open(root_dir + 'pids_train_only.pkl','rb'))\n",
    "train_rep_ans = np.expand_dims(train_rep_ans,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, X2 ,X3):\n",
    "#     X3 = np.take(train_fam_ans,[0],axis=-1) #只train第幾個familiy\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], X2[randomize],X3[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train of sent2vec vector: (4104, 226, 768) (4104, 226) (4104, 226, 1)\n",
      "valid of sent2vec vector: (453, 226, 768) (453, 226) (453, 226, 1)\n"
     ]
    }
   ],
   "source": [
    "train_emb, train_emb_api, train_rep_ans = _shuffle(train_emb, train_emb_api, train_rep_ans)\n",
    "print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_rep_ans.shape)\n",
    "print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_rep_ans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = valid_emb.shape[-1] #被除數\n",
    "num_heads = 48#除數，要整除\n",
    "max_length = test_emb_api.shape[1] # max sequence length\n",
    "# fam_num = train_fam_ans.shape[1]\n",
    "vocabulary_size = emb_matrix.shape[0]-1\n",
    "transformer_depth = 1\n",
    "transformer_dropout = 0.1\n",
    "l2_reg_penalty = 1e-6#1e-4\n",
    "dp_rate = 0.01\n",
    "\n",
    "traina = True #改\n",
    "batch_size = 128 #改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "constrain = keras.constraints.MinMaxNorm(min_value=0.0, max_value=0.0, rate=1.0, axis=0)\n",
    "init = keras.initializers.Ones()\n",
    "coordinate_embedding_layer = TransformerCoordinateEmbedding(\n",
    "        transformer_depth , name='coordinate_embedding') #positional encoding\n",
    "act_layer = TransformerACT(\n",
    "            name='adaptive_computation_time')\n",
    "\n",
    "transformer_block = TransformerBlock(\n",
    "            name='transformer', num_heads=num_heads,\n",
    "            residual_dropout=transformer_dropout,\n",
    "            attention_dropout=transformer_dropout,\n",
    "            # Allow bi-directional attention\n",
    "            use_masking=False)\n",
    "add_segment_layer = Add(name='add_segment')\n",
    "l2_regularizer = (regularizers.l2(l2_reg_penalty) if l2_reg_penalty else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Archeitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leoqaz12/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sentemb1 = Input(shape=(max_length,emb_dim),name='sent_emb')\n",
    "sentemb = Masking(mask_value=0)(sentemb1)\n",
    "#shape=(max_length,emb_dim),,batch_shape=(batch_size,max_length,emb_dim)\n",
    "sent_ids1 = Input(shape=(max_length,), dtype='int32', name='sent_ids') # 輸入的api funvtion name ID\n",
    "sent_ids = Masking(mask_value=0)(sent_ids1)\n",
    "#shape=(max_length,),batch_shape=(batch_size,max_length)\n",
    "api_emb = Embedding(vocabulary_size+1, emb_dim,weights=[emb_matrix],input_length=max_length\n",
    "                    ,trainable=True,name='api_emb')(sent_ids) #改\n",
    "\n",
    "segment_embeddings = Add()([sentemb,api_emb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_rnn = GRU(int(emb_dim/4),return_sequences=True,return_state=False,name='common_extract'\n",
    "                      ,trainable=True)(segment_embeddings)\n",
    "att_rnn = BatchNormalization(name='bn')(att_rnn)\n",
    "self_att = SeqSelfAttention(kernel_initializer=keras.initializers.lecun_normal(), #sigmoid af\n",
    "                            attention_activation='selu',name='self_attention')(att_rnn)\n",
    "# self_att = MultiHead(SeqSelfAttention(attention_activation='sigmoid',name='self_attention'),layer_num=2,name='Multi_Head')(att_rnn)\n",
    "# multi_head = Reshape((max_length,-1))(self_att)\n",
    "# multi_head = Lambda(lambda x: keras.backend.concatenate(x,axis=-2),name='reshape')(self_att)\n",
    "rep_prediction = (\n",
    "        TimeDistributed(Dense(1, name='0_1_predict', activation='sigmoid',trainable=True),name='out_rep') # hard_sigmoid\n",
    "    (self_att))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_ids (InputLayer)           (None, 226)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sent_emb (InputLayer)           (None, 226, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 226)          0           sent_ids[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 226, 768)     0           sent_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "api_emb (Embedding)             (None, 226, 768)     20736       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 226, 768)     0           masking_1[0][0]                  \n",
      "                                                                 api_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "common_extract (GRU)            (None, 226, 192)     553536      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 226, 192)     768         common_extract[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "self_attention (SeqSelfAttentio (None, 226, 192)     12353       bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "out_rep (TimeDistributed)       (None, 226, 1)       193         self_attention[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 587,586\n",
      "Trainable params: 587,202\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[sent_ids1,sentemb1], outputs=[rep_prediction]) #out\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model/o2o_stage_gru_selfatt/byterep_1stStage_0706_gruatt_sent2vec.h5')\n",
    "# model.load_weights('./model/o2o_130_gru_selfatt/byterep_130fam_0701_gruatt_sent2vec.h5')#,by_name=True) #改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model loss opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_multi_label_metric(y_true, y_pred):\n",
    "    comp = K.equal(y_true, K.round(y_pred))\n",
    "    return K.cast(K.all(comp, axis=-1), K.floatx())\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def custom_acc1(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred,k=3)\n",
    "from keras.metrics import binary_accuracy\n",
    "def bin_acc(y_true, y_pred):\n",
    "    return binary_accuracy(y_true, y_pred)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    return tf.keras.metrics.Precision(y_true,y_pred)[1]\n",
    "def recall(y_true, y_pred):\n",
    "    return tf.keras.metrics.Recall(y_true,y_pred)[1]\n",
    "def Hamming_loss(y_true, y_pred):\n",
    "    tmp = K.abs(y_true-y_pred)\n",
    "    return K.mean(K.cast(K.greater(tmp,0.5),dtype=float))\n",
    "def hn_multilabel_loss(y_true, y_pred):\n",
    "    # Avoid divide by 0\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # Multi-task loss\n",
    "    return K.mean(K.sum(- y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred), axis=1))\n",
    "# from sklearn.metrics import f1_score\n",
    "# def f1_sk(y_true,y_pred):\n",
    "#     score = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "#     return score\n",
    "\n",
    "# 訓練參數\n",
    "los = [hn_multilabel_loss,binary_focal_loss(alpha=.25, gamma=2)] # 1st stage.  f1_loss\n",
    "#SINGLE\n",
    "# los = [binary_focal_loss(alpha=.25, gamma=2)] #改\n",
    "# los = [f1_loss]\n",
    "# los = [hn_multilabel_loss]\n",
    "los = [losses.binary_crossentropy]\n",
    "# MML\n",
    "'''los = []\n",
    "for i in range(fam_num):\n",
    "    los.append(binary_focal_loss(alpha=.25, gamma=2))\n",
    "los = [losses.binary_crossentropy] + los'''\n",
    "\n",
    "\n",
    "metric = {'out_rep': bin_acc,'family': f1_metric} # 1st stage. km.f1_score()\n",
    "#SINGLE\n",
    "metric = [f1_metric,bin_acc]\n",
    "# metric = [km.f1_score(),bin_acc,km.binary_f1_score()]\n",
    "# metric = {'RasMMA': 'acc'}\n",
    "metric = [bin_acc] #改\n",
    "#MML\n",
    "'''metrics = []\n",
    "for i in range(fam_num+1):\n",
    "    metrics.append('acc')\n",
    "# metrics = {}\n",
    "# metrics['RasMMA'] = 'acc'\n",
    "# for i in range(fam_num):\n",
    "#     metrics['fam'+str(i)]='acc'\n",
    "metric = metrics'''\n",
    "\n",
    "\n",
    "loss_weight = [1,1] #stage1 0.95,0.05  #1st stage # 2nd stage [0.01,0.99]\n",
    "#SINGLE\n",
    "loss_weight = [1]\n",
    "#MML\n",
    "'''loss_weight = []\n",
    "for i in range(fam_num):\n",
    "    loss_weight.append(0.95)\n",
    "loss_weight = [0.05] + loss_weight'''\n",
    "\n",
    "learning_rate = 5e-4#2e-4 # 2nd stage: 1e-4 @1st:2e-4 0.002\n",
    "# batch_size = 128 #32 #128\n",
    "\n",
    "num_epochs = 1000\n",
    "patien = 50\n",
    "\n",
    "model_save_path = './model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5' #改\n",
    "tensorboard_log_path = './logs/'+ model_save_path.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "optimizer = optimizers.Adam(\n",
    "            lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False) #clipnorm=1. , clipvalue=1.\n",
    "optimizer = keras.optimizers.Nadam(lr=learning_rate, clipvalue=1.) #改\n",
    "# tf.keras.optimizers.Nadam\n",
    "lr_scheduler1 = callbacks.LearningRateScheduler(\n",
    "        CosineLRSchedule(lr_high=0.001, lr_low=1e-8, #learning_rate #改 #2nd stage增加\n",
    "                         initial_period=num_epochs),\n",
    "        verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=int(patien/3),\n",
    "                                      min_lr=1e-8,mode='min') \n",
    "\n",
    "model.compile(\n",
    "            optimizer,\n",
    "            loss=los,\n",
    "            metrics=metric ,loss_weights=loss_weight)#{'word_predictions': masked_perplexity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "# save best, early stopping, 2 models ens weight:(best=0.8,last=0.2)\n",
    "history = History()\n",
    "stop_nan = callbacks.TerminateOnNaN()\n",
    "model_callbacks = [\n",
    "        callbacks.ModelCheckpoint(\n",
    "            model_save_path, #val_f1_metric,max。val_family_f1_metric\n",
    "            monitor='val_loss',mode='min' ,save_best_only=True, verbose=1,save_weights_only=True), #改\n",
    "            EarlyStopping(patience=patien,monitor='val_loss',verbose=1,mode='min'),\n",
    "        lr_scheduler,history,stop_nan ,lr_scheduler1\n",
    "    ]\n",
    "model_callbacks.append(callbacks.TensorBoard(tensorboard_log_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_batch(batch_size, X_train1, X_train2 , Y_train1):#, Y_train2):\n",
    "    '''\n",
    "    X_train1 = sent_ids: shape為(N, max_seq_length)\n",
    "    X_train2 = sentemb: shape為(N,max_seq_length, word_embedding_size)\n",
    "    Y_train1 = class_prediction: shape為(N, max_seq_length, 1)\n",
    "    Y_train2 = family_prediction(stage2): shape為(N, fam_num)\n",
    "    '''\n",
    "    idx = np.arange(len(X_train1))\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    while True:\n",
    "        for i in idx:\n",
    "            train_X1 = X_train1[idx[i]:idx[i]+batch_size]\n",
    "            train_X2 = X_train2[idx[i]:idx[i]+batch_size]\n",
    "            train_Y1 = Y_train1[idx[i]:idx[i]+batch_size]\n",
    "#             train_Y2 = Y_train2[idx[i]:idx[i]+batch_size]\n",
    "#             yield(train_X2,train_Y2)\n",
    "#             yield ([train_X1,train_X2],[train_Y1,train_Y2]) #ori\n",
    "            yield ([train_X1,train_X2],[train_Y1]) #改\n",
    "            if i == idx[-1]:\n",
    "                idx = np.arange(len(X_train1))\n",
    "                np.random.shuffle(idx)\n",
    "                break\n",
    "            \n",
    "#     data_size = X_train.shape[0]\n",
    "#     ep = data_size / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/.local/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "33/33 [==============================] - 23s 700ms/step - loss: 0.9465 - bin_acc: 0.7702 - val_loss: 0.4947 - val_bin_acc: 0.7749\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49472, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0009999975326256032.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.3710 - bin_acc: 0.8308 - val_loss: 0.4526 - val_bin_acc: 0.8119\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49472 to 0.45259, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0009999901305267642.\n",
      "33/33 [==============================] - 15s 470ms/step - loss: 0.3440 - bin_acc: 0.8540 - val_loss: 0.4194 - val_bin_acc: 0.8192\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.45259 to 0.41939, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0009999777937765395.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3590 - bin_acc: 0.8504 - val_loss: 0.4572 - val_bin_acc: 0.8005\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.41939\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.000999960522496687.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3080 - bin_acc: 0.8668 - val_loss: 0.4087 - val_bin_acc: 0.8286\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.41939 to 0.40870, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.0009999383168576678.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.3250 - bin_acc: 0.8593 - val_loss: 0.4043 - val_bin_acc: 0.8360\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.40870 to 0.40426, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.0009999111770786426.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.3170 - bin_acc: 0.8748 - val_loss: 0.3682 - val_bin_acc: 0.8396\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.40426 to 0.36823, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009998791034274699.\n",
      "33/33 [==============================] - 15s 468ms/step - loss: 0.3776 - bin_acc: 0.8375 - val_loss: 0.4081 - val_bin_acc: 0.8234\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.36823\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.0009998420962207036.\n",
      "33/33 [==============================] - 16s 472ms/step - loss: 0.3340 - bin_acc: 0.8585 - val_loss: 0.4033 - val_bin_acc: 0.8067\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.36823\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.00099980015582359.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3220 - bin_acc: 0.8648 - val_loss: 0.3987 - val_bin_acc: 0.8383\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.36823\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.000999753282650064.\n",
      "33/33 [==============================] - 15s 466ms/step - loss: 0.3576 - bin_acc: 0.8471 - val_loss: 0.5035 - val_bin_acc: 0.8327\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.36823\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.0009997014771627446.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.3192 - bin_acc: 0.8746 - val_loss: 0.4050 - val_bin_acc: 0.8389\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.36823\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.0009996447398729314.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.3113 - bin_acc: 0.8738 - val_loss: 0.4308 - val_bin_acc: 0.8193\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.36823\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.0009995830713405982.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3137 - bin_acc: 0.8659 - val_loss: 0.4279 - val_bin_acc: 0.8035\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.36823\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.000999516472174389.\n",
      "33/33 [==============================] - 15s 465ms/step - loss: 0.3328 - bin_acc: 0.8587 - val_loss: 0.4161 - val_bin_acc: 0.8280\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.36823\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.0009994449430316102.\n",
      "33/33 [==============================] - 15s 468ms/step - loss: 0.3152 - bin_acc: 0.8607 - val_loss: 0.3903 - val_bin_acc: 0.8242\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.36823\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.0009993684846182258.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3060 - bin_acc: 0.8710 - val_loss: 0.3655 - val_bin_acc: 0.8415\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.36823 to 0.36547, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.0009992870976888492.\n",
      "33/33 [==============================] - 15s 468ms/step - loss: 0.3317 - bin_acc: 0.8512 - val_loss: 0.3821 - val_bin_acc: 0.8530\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.36547\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.000999200783046737.\n",
      "33/33 [==============================] - 16s 481ms/step - loss: 0.3435 - bin_acc: 0.8581 - val_loss: 0.4583 - val_bin_acc: 0.7553\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.36547\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.0009991095415437796.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3192 - bin_acc: 0.8658 - val_loss: 0.5778 - val_bin_acc: 0.7760\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.36547\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.0009990133740804936.\n",
      "33/33 [==============================] - 15s 470ms/step - loss: 0.3176 - bin_acc: 0.8699 - val_loss: 0.3423 - val_bin_acc: 0.8569\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.36547 to 0.34231, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.0009989122816060135.\n",
      "33/33 [==============================] - 16s 477ms/step - loss: 0.2902 - bin_acc: 0.8754 - val_loss: 0.3337 - val_bin_acc: 0.8558\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.34231 to 0.33366, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.0009988062651180808.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3001 - bin_acc: 0.8714 - val_loss: 0.3237 - val_bin_acc: 0.8620\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.33366 to 0.32368, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.0009986953256630358.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3063 - bin_acc: 0.8716 - val_loss: 0.3411 - val_bin_acc: 0.8630\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.32368\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.0009985794643358056.\n",
      "33/33 [==============================] - 17s 501ms/step - loss: 0.2830 - bin_acc: 0.8795 - val_loss: 0.4683 - val_bin_acc: 0.8266\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.32368\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.0009984586822798954.\n",
      "33/33 [==============================] - 16s 482ms/step - loss: 0.3086 - bin_acc: 0.8691 - val_loss: 0.3450 - val_bin_acc: 0.8586\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.32368\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.0009983329806873748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 15s 470ms/step - loss: 0.3209 - bin_acc: 0.8686 - val_loss: 0.4493 - val_bin_acc: 0.8219\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.32368\n",
      "Epoch 28/1000\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.000998202360798868.\n",
      "33/33 [==============================] - 16s 477ms/step - loss: 0.2927 - bin_acc: 0.8793 - val_loss: 0.3701 - val_bin_acc: 0.8486\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.32368\n",
      "Epoch 29/1000\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.0009980668239035406.\n",
      "33/33 [==============================] - 15s 467ms/step - loss: 0.2857 - bin_acc: 0.8797 - val_loss: 0.3864 - val_bin_acc: 0.8442\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.32368\n",
      "Epoch 30/1000\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.0009979263713390868.\n",
      "33/33 [==============================] - 16s 476ms/step - loss: 0.2693 - bin_acc: 0.8895 - val_loss: 0.3685 - val_bin_acc: 0.8547\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.32368\n",
      "Epoch 31/1000\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.000997781004491717.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.2884 - bin_acc: 0.8757 - val_loss: 0.4050 - val_bin_acc: 0.8237\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.32368\n",
      "Epoch 32/1000\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.0009976307247961432.\n",
      "33/33 [==============================] - 15s 467ms/step - loss: 0.3378 - bin_acc: 0.8527 - val_loss: 0.3671 - val_bin_acc: 0.8486\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.32368\n",
      "Epoch 33/1000\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.0009974755337355653.\n",
      "33/33 [==============================] - 16s 472ms/step - loss: 0.2969 - bin_acc: 0.8729 - val_loss: 0.4000 - val_bin_acc: 0.8456\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.32368\n",
      "Epoch 34/1000\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.000997315432841656.\n",
      "33/33 [==============================] - 15s 466ms/step - loss: 0.2373 - bin_acc: 0.9030 - val_loss: 0.3518 - val_bin_acc: 0.8575\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.32368\n",
      "Epoch 35/1000\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.0009971504236945476.\n",
      "33/33 [==============================] - 16s 480ms/step - loss: 0.2834 - bin_acc: 0.8775 - val_loss: 0.3454 - val_bin_acc: 0.8610\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.32368\n",
      "Epoch 36/1000\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.0009969805079228127.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.2871 - bin_acc: 0.8885 - val_loss: 0.3351 - val_bin_acc: 0.8619\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.32368\n",
      "Epoch 37/1000\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.0009968056872034516.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2573 - bin_acc: 0.8977 - val_loss: 0.3578 - val_bin_acc: 0.8609\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.32368\n",
      "Epoch 38/1000\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.0009966259632618745.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2743 - bin_acc: 0.8924 - val_loss: 0.3621 - val_bin_acc: 0.8514\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.32368\n",
      "Epoch 39/1000\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.000996441337871884.\n",
      "33/33 [==============================] - 17s 519ms/step - loss: 0.2857 - bin_acc: 0.8848 - val_loss: 0.5520 - val_bin_acc: 0.8360\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.32368\n",
      "Epoch 40/1000\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.0009962518128556583.\n",
      "33/33 [==============================] - 15s 465ms/step - loss: 0.3004 - bin_acc: 0.8781 - val_loss: 0.4417 - val_bin_acc: 0.8122\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.32368\n",
      "Epoch 41/1000\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.0009960573900837325.\n",
      "33/33 [==============================] - 16s 478ms/step - loss: 0.3209 - bin_acc: 0.8682 - val_loss: 0.3893 - val_bin_acc: 0.8415\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.32368\n",
      "Epoch 42/1000\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.000995858071474981.\n",
      "33/33 [==============================] - 16s 479ms/step - loss: 0.3007 - bin_acc: 0.8740 - val_loss: 0.3743 - val_bin_acc: 0.8489\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.32368\n",
      "Epoch 43/1000\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.000995653858996598.\n",
      "33/33 [==============================] - 16s 484ms/step - loss: 0.3154 - bin_acc: 0.8647 - val_loss: 0.3414 - val_bin_acc: 0.8591\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.32368\n",
      "Epoch 44/1000\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.0009954447546640783.\n",
      "33/33 [==============================] - 16s 472ms/step - loss: 0.2841 - bin_acc: 0.8832 - val_loss: 0.3203 - val_bin_acc: 0.8665\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.32368 to 0.32033, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 45/1000\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.000995230760541197.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.2839 - bin_acc: 0.8795 - val_loss: 0.3520 - val_bin_acc: 0.8571\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.32033\n",
      "Epoch 46/1000\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.0009950118787399901.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2779 - bin_acc: 0.8818 - val_loss: 0.3340 - val_bin_acc: 0.8668\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.32033\n",
      "Epoch 47/1000\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.0009947881114207324.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.2998 - bin_acc: 0.8723 - val_loss: 0.3329 - val_bin_acc: 0.8687\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.32033\n",
      "Epoch 48/1000\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.000994559460791917.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.2689 - bin_acc: 0.8905 - val_loss: 0.3611 - val_bin_acc: 0.8663\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.32033\n",
      "Epoch 49/1000\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.0009943259291102333.\n",
      "33/33 [==============================] - 16s 497ms/step - loss: 0.2629 - bin_acc: 0.8926 - val_loss: 0.3424 - val_bin_acc: 0.8642\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.32033\n",
      "Epoch 50/1000\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.0009940875186805447.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.3034 - bin_acc: 0.8713 - val_loss: 0.4037 - val_bin_acc: 0.8379\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.32033\n",
      "Epoch 51/1000\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.000993844231855866.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.2526 - bin_acc: 0.8986 - val_loss: 0.3592 - val_bin_acc: 0.8572\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.32033\n",
      "Epoch 52/1000\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.0009935960710373397.\n",
      "33/33 [==============================] - 15s 466ms/step - loss: 0.2936 - bin_acc: 0.8795 - val_loss: 0.3543 - val_bin_acc: 0.8510\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.32033\n",
      "Epoch 53/1000\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.000993343038674213.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.2880 - bin_acc: 0.8761 - val_loss: 0.3702 - val_bin_acc: 0.8589\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.32033\n",
      "Epoch 54/1000\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.0009930851372638133.\n",
      "33/33 [==============================] - 16s 472ms/step - loss: 0.2813 - bin_acc: 0.8877 - val_loss: 0.3577 - val_bin_acc: 0.8449\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.32033\n",
      "Epoch 55/1000\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.0009928223693515233.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2822 - bin_acc: 0.8843 - val_loss: 0.3332 - val_bin_acc: 0.8655\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.32033\n",
      "Epoch 56/1000\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.0009925547375307562.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 16s 471ms/step - loss: 0.3030 - bin_acc: 0.8726 - val_loss: 0.3532 - val_bin_acc: 0.8389\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.32033\n",
      "Epoch 57/1000\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.00099228224444293.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3292 - bin_acc: 0.8664 - val_loss: 0.7100 - val_bin_acc: 0.6937\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.32033\n",
      "Epoch 58/1000\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.0009920048927774417.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2837 - bin_acc: 0.8859 - val_loss: 0.5449 - val_bin_acc: 0.7995\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.32033\n",
      "Epoch 59/1000\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.00099172268527164.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.2825 - bin_acc: 0.8837 - val_loss: 0.4763 - val_bin_acc: 0.8032\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.32033\n",
      "Epoch 60/1000\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.0009914356247107989.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.2773 - bin_acc: 0.8884 - val_loss: 0.4289 - val_bin_acc: 0.8361\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.32033\n",
      "Epoch 61/1000\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.0009911437139280908.\n",
      "33/33 [==============================] - 15s 467ms/step - loss: 0.2878 - bin_acc: 0.8833 - val_loss: 0.4558 - val_bin_acc: 0.8222\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.32033\n",
      "Epoch 62/1000\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.0009908469558045567.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2991 - bin_acc: 0.8759 - val_loss: 0.3642 - val_bin_acc: 0.8561\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.32033\n",
      "Epoch 63/1000\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.0009905453532690798.\n",
      "33/33 [==============================] - 16s 481ms/step - loss: 0.3017 - bin_acc: 0.8698 - val_loss: 0.3773 - val_bin_acc: 0.8451\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.32033\n",
      "Epoch 64/1000\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.0009902389092983554.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.2698 - bin_acc: 0.8930 - val_loss: 0.3284 - val_bin_acc: 0.8664\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.32033\n",
      "Epoch 65/1000\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.0009899276269168616.\n",
      "33/33 [==============================] - 16s 478ms/step - loss: 0.2622 - bin_acc: 0.8968 - val_loss: 0.3237 - val_bin_acc: 0.8664\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.32033\n",
      "Epoch 66/1000\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.0009896115091968297.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2613 - bin_acc: 0.8946 - val_loss: 0.3212 - val_bin_acc: 0.8670\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.32033\n",
      "Epoch 67/1000\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.0009892905592582145.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3253 - bin_acc: 0.8667 - val_loss: 0.3363 - val_bin_acc: 0.8574\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.32033\n",
      "Epoch 68/1000\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.0009889647802686617.\n",
      "33/33 [==============================] - 16s 477ms/step - loss: 0.2688 - bin_acc: 0.8839 - val_loss: 0.3219 - val_bin_acc: 0.8652\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.32033\n",
      "Epoch 69/1000\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.000988634175443479.\n",
      "33/33 [==============================] - 16s 477ms/step - loss: 0.2945 - bin_acc: 0.8802 - val_loss: 0.3362 - val_bin_acc: 0.8460\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.32033\n",
      "Epoch 70/1000\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.0009882987480456019.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2887 - bin_acc: 0.8780 - val_loss: 0.3797 - val_bin_acc: 0.8308\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.32033\n",
      "Epoch 71/1000\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.000987958501385564.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.3239 - bin_acc: 0.8537 - val_loss: 0.3200 - val_bin_acc: 0.8663\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.32033 to 0.31996, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 72/1000\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.000987613438821462.\n",
      "33/33 [==============================] - 15s 470ms/step - loss: 0.2531 - bin_acc: 0.8976 - val_loss: 0.3470 - val_bin_acc: 0.8537\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.31996\n",
      "Epoch 73/1000\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.0009872635637589247.\n",
      "33/33 [==============================] - 16s 499ms/step - loss: 0.2668 - bin_acc: 0.8937 - val_loss: 0.4714 - val_bin_acc: 0.8290\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.31996\n",
      "Epoch 74/1000\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.000986908879651077.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.3071 - bin_acc: 0.8654 - val_loss: 0.3621 - val_bin_acc: 0.8230\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.31996\n",
      "Epoch 75/1000\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.0009865493899985085.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.2805 - bin_acc: 0.8797 - val_loss: 0.3269 - val_bin_acc: 0.8537\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.31996\n",
      "Epoch 76/1000\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.0009861850983492364.\n",
      "33/33 [==============================] - 15s 470ms/step - loss: 0.2654 - bin_acc: 0.8864 - val_loss: 0.3263 - val_bin_acc: 0.8548\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.31996\n",
      "Epoch 77/1000\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.0009858160082986723.\n",
      "33/33 [==============================] - 16s 480ms/step - loss: 0.2656 - bin_acc: 0.8902 - val_loss: 0.3292 - val_bin_acc: 0.8573\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.31996\n",
      "Epoch 78/1000\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.0009854421234895864.\n",
      "33/33 [==============================] - 16s 482ms/step - loss: 0.3186 - bin_acc: 0.8644 - val_loss: 0.3403 - val_bin_acc: 0.8563\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.31996\n",
      "Epoch 79/1000\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.0009850634476120705.\n",
      "33/33 [==============================] - 15s 470ms/step - loss: 0.2642 - bin_acc: 0.8907 - val_loss: 0.3167 - val_bin_acc: 0.8610\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.31996 to 0.31671, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 80/1000\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.0009846799844035025.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3108 - bin_acc: 0.8730 - val_loss: 0.3073 - val_bin_acc: 0.8705\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.31671 to 0.30731, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 81/1000\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.00098429173764851.\n",
      "33/33 [==============================] - 16s 478ms/step - loss: 0.3100 - bin_acc: 0.8678 - val_loss: 0.3400 - val_bin_acc: 0.8575\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.30731\n",
      "Epoch 82/1000\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.000983898711178931.\n",
      "33/33 [==============================] - 15s 467ms/step - loss: 0.2613 - bin_acc: 0.8920 - val_loss: 0.3089 - val_bin_acc: 0.8697\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.30731\n",
      "Epoch 83/1000\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.0009835009088737788.\n",
      "33/33 [==============================] - 16s 482ms/step - loss: 0.2747 - bin_acc: 0.8794 - val_loss: 0.3470 - val_bin_acc: 0.8543\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.30731\n",
      "Epoch 84/1000\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.000983098334659201.\n",
      "33/33 [==============================] - 15s 468ms/step - loss: 0.2728 - bin_acc: 0.8889 - val_loss: 0.3071 - val_bin_acc: 0.8706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00084: val_loss improved from 0.30731 to 0.30708, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 85/1000\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.0009826909925084429.\n",
      "33/33 [==============================] - 15s 466ms/step - loss: 0.3012 - bin_acc: 0.8763 - val_loss: 0.3284 - val_bin_acc: 0.8683\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.30708\n",
      "Epoch 86/1000\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.0009822788864418067.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.2598 - bin_acc: 0.8931 - val_loss: 0.3160 - val_bin_acc: 0.8655\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.30708\n",
      "Epoch 87/1000\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.0009818620205266134.\n",
      "33/33 [==============================] - 15s 467ms/step - loss: 0.3020 - bin_acc: 0.8707 - val_loss: 0.3165 - val_bin_acc: 0.8716\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.30708\n",
      "Epoch 88/1000\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.000981440398877161.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.2545 - bin_acc: 0.8976 - val_loss: 0.3211 - val_bin_acc: 0.8747\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.30708\n",
      "Epoch 89/1000\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.000981014025654685.\n",
      "33/33 [==============================] - 15s 468ms/step - loss: 0.2881 - bin_acc: 0.8845 - val_loss: 0.3263 - val_bin_acc: 0.8678\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.30708\n",
      "Epoch 90/1000\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.0009805829050673171.\n",
      "33/33 [==============================] - 16s 472ms/step - loss: 0.2775 - bin_acc: 0.8855 - val_loss: 0.2988 - val_bin_acc: 0.8736\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.30708 to 0.29879, saving model to ./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5\n",
      "Epoch 91/1000\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.0009801470413700432.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2837 - bin_acc: 0.8720 - val_loss: 0.3091 - val_bin_acc: 0.8743\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.29879\n",
      "Epoch 92/1000\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.0009797064388646622.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.3040 - bin_acc: 0.8728 - val_loss: 0.3153 - val_bin_acc: 0.8721\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.29879\n",
      "Epoch 93/1000\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.000979261101899743.\n",
      "33/33 [==============================] - 16s 471ms/step - loss: 0.3001 - bin_acc: 0.8675 - val_loss: 0.3210 - val_bin_acc: 0.8694\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.29879\n",
      "Epoch 94/1000\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.0009788110348705813.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.3021 - bin_acc: 0.8684 - val_loss: 0.3701 - val_bin_acc: 0.8463\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.29879\n",
      "Epoch 95/1000\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.0009783562422191576.\n",
      "33/33 [==============================] - 15s 468ms/step - loss: 0.2846 - bin_acc: 0.8767 - val_loss: 0.3504 - val_bin_acc: 0.8649\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.29879\n",
      "Epoch 96/1000\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.000977896728434091.\n",
      "33/33 [==============================] - 16s 473ms/step - loss: 0.3144 - bin_acc: 0.8670 - val_loss: 0.3673 - val_bin_acc: 0.8498\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.29879\n",
      "Epoch 97/1000\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.0009774324980505978.\n",
      "33/33 [==============================] - 16s 475ms/step - loss: 0.3062 - bin_acc: 0.8708 - val_loss: 0.3371 - val_bin_acc: 0.8593\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.29879\n",
      "Epoch 98/1000\n",
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.000976963555650444.\n",
      "33/33 [==============================] - 16s 472ms/step - loss: 0.3159 - bin_acc: 0.8598 - val_loss: 0.3244 - val_bin_acc: 0.8666\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.29879\n",
      "Epoch 99/1000\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.0009764899058619021.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.2773 - bin_acc: 0.8801 - val_loss: 0.3281 - val_bin_acc: 0.8676\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.29879\n",
      "Epoch 100/1000\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.0009760115533597037.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.2770 - bin_acc: 0.8850 - val_loss: 0.3224 - val_bin_acc: 0.8645\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.29879\n",
      "Epoch 101/1000\n",
      "\n",
      "Epoch 00101: LearningRateScheduler setting learning rate to 0.0009755285028649954.\n",
      "33/33 [==============================] - 15s 450ms/step - loss: 0.2230 - bin_acc: 0.9159 - val_loss: 0.4280 - val_bin_acc: 0.8547\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.29879\n",
      "Epoch 102/1000\n",
      "\n",
      "Epoch 00102: LearningRateScheduler setting learning rate to 0.0009750407591452904.\n",
      "33/33 [==============================] - 15s 467ms/step - loss: 0.2885 - bin_acc: 0.8796 - val_loss: 0.3579 - val_bin_acc: 0.8496\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.29879\n",
      "Epoch 103/1000\n",
      "\n",
      "Epoch 00103: LearningRateScheduler setting learning rate to 0.0009745483270144225.\n",
      "33/33 [==============================] - 16s 470ms/step - loss: 0.2715 - bin_acc: 0.8893 - val_loss: 0.3370 - val_bin_acc: 0.8506\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.29879\n",
      "Epoch 104/1000\n",
      "\n",
      "Epoch 00104: LearningRateScheduler setting learning rate to 0.0009740512113324977.\n",
      "33/33 [==============================] - 16s 479ms/step - loss: 0.2765 - bin_acc: 0.8800 - val_loss: 0.3332 - val_bin_acc: 0.8637\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.29879\n",
      "Epoch 105/1000\n",
      "\n",
      "Epoch 00105: LearningRateScheduler setting learning rate to 0.0009735494170058473.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.3226 - bin_acc: 0.8524 - val_loss: 0.3295 - val_bin_acc: 0.8661\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.29879\n",
      "Epoch 106/1000\n",
      "\n",
      "Epoch 00106: LearningRateScheduler setting learning rate to 0.0009730429489869787.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.3114 - bin_acc: 0.8758 - val_loss: 0.3486 - val_bin_acc: 0.8558\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.29879\n",
      "Epoch 107/1000\n",
      "\n",
      "Epoch 00107: LearningRateScheduler setting learning rate to 0.0009725318122745266.\n",
      "33/33 [==============================] - 15s 468ms/step - loss: 0.2580 - bin_acc: 0.9020 - val_loss: 0.3367 - val_bin_acc: 0.8429\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.29879\n",
      "Epoch 108/1000\n",
      "\n",
      "Epoch 00108: LearningRateScheduler setting learning rate to 0.0009720160119132044.\n",
      "33/33 [==============================] - 16s 479ms/step - loss: 0.2372 - bin_acc: 0.9000 - val_loss: 0.3425 - val_bin_acc: 0.8599\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.29879\n",
      "Epoch 109/1000\n",
      "\n",
      "Epoch 00109: LearningRateScheduler setting learning rate to 0.000971495552993753.\n",
      "33/33 [==============================] - 15s 464ms/step - loss: 0.2812 - bin_acc: 0.8799 - val_loss: 0.3514 - val_bin_acc: 0.8590\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.29879\n",
      "Epoch 110/1000\n",
      "\n",
      "Epoch 00110: LearningRateScheduler setting learning rate to 0.0009709704406528918.\n",
      "33/33 [==============================] - 15s 469ms/step - loss: 0.2632 - bin_acc: 0.8858 - val_loss: 0.3551 - val_bin_acc: 0.8382\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.29879\n",
      "Epoch 111/1000\n",
      "\n",
      "Epoch 00111: LearningRateScheduler setting learning rate to 0.0009704406800732681.\n",
      "33/33 [==============================] - 15s 464ms/step - loss: 0.2811 - bin_acc: 0.8767 - val_loss: 0.3581 - val_bin_acc: 0.8653\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.29879\n",
      "Epoch 112/1000\n",
      "\n",
      "Epoch 00112: LearningRateScheduler setting learning rate to 0.0009699062764834045.\n",
      "33/33 [==============================] - 14s 420ms/step - loss: 0.2553 - bin_acc: 0.9003 - val_loss: 0.3512 - val_bin_acc: 0.8393\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.29879\n",
      "Epoch 113/1000\n",
      "\n",
      "Epoch 00113: LearningRateScheduler setting learning rate to 0.000969367235157649.\n",
      "33/33 [==============================] - 14s 422ms/step - loss: 0.2338 - bin_acc: 0.9048 - val_loss: 0.3675 - val_bin_acc: 0.8614\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.29879\n",
      "Epoch 114/1000\n",
      "\n",
      "Epoch 00114: LearningRateScheduler setting learning rate to 0.0009688235614161214.\n",
      "33/33 [==============================] - 14s 429ms/step - loss: 0.2821 - bin_acc: 0.8835 - val_loss: 0.3479 - val_bin_acc: 0.8462\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.29879\n",
      "Epoch 115/1000\n",
      "\n",
      "Epoch 00115: LearningRateScheduler setting learning rate to 0.0009682752606246627.\n",
      "33/33 [==============================] - 14s 422ms/step - loss: 0.2963 - bin_acc: 0.8665 - val_loss: 0.3335 - val_bin_acc: 0.8551\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.29879\n",
      "Epoch 116/1000\n",
      "\n",
      "Epoch 00116: LearningRateScheduler setting learning rate to 0.0009677223381947798.\n",
      "33/33 [==============================] - 14s 432ms/step - loss: 0.2725 - bin_acc: 0.8835 - val_loss: 0.3339 - val_bin_acc: 0.8663\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.29879\n",
      "Epoch 117/1000\n",
      "\n",
      "Epoch 00117: LearningRateScheduler setting learning rate to 0.000967164799583594.\n",
      "33/33 [==============================] - 14s 432ms/step - loss: 0.3039 - bin_acc: 0.8699 - val_loss: 0.3283 - val_bin_acc: 0.8706\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.29879\n",
      "Epoch 118/1000\n",
      "\n",
      "Epoch 00118: LearningRateScheduler setting learning rate to 0.0009666026502937863.\n",
      "33/33 [==============================] - 14s 428ms/step - loss: 0.3280 - bin_acc: 0.8618 - val_loss: 0.3470 - val_bin_acc: 0.8599\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.29879\n",
      "Epoch 119/1000\n",
      "\n",
      "Epoch 00119: LearningRateScheduler setting learning rate to 0.0009660358958735433.\n",
      "33/33 [==============================] - 14s 420ms/step - loss: 0.2723 - bin_acc: 0.8929 - val_loss: 0.3522 - val_bin_acc: 0.8517\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.29879\n",
      "Epoch 120/1000\n",
      "\n",
      "Epoch 00120: LearningRateScheduler setting learning rate to 0.0009654645419165024.\n",
      "33/33 [==============================] - 14s 426ms/step - loss: 0.3328 - bin_acc: 0.8539 - val_loss: 0.3164 - val_bin_acc: 0.8710\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.29879\n",
      "Epoch 121/1000\n",
      "\n",
      "Epoch 00121: LearningRateScheduler setting learning rate to 0.0009648885940616963.\n",
      "33/33 [==============================] - 14s 426ms/step - loss: 0.2761 - bin_acc: 0.8898 - val_loss: 0.3343 - val_bin_acc: 0.8634\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.29879\n",
      "Epoch 122/1000\n",
      "\n",
      "Epoch 00122: LearningRateScheduler setting learning rate to 0.0009643080579934981.\n",
      "33/33 [==============================] - 14s 421ms/step - loss: 0.2629 - bin_acc: 0.8877 - val_loss: 0.3273 - val_bin_acc: 0.8633\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.29879\n",
      "Epoch 123/1000\n",
      "\n",
      "Epoch 00123: LearningRateScheduler setting learning rate to 0.0009637229394415641.\n",
      "33/33 [==============================] - 14s 424ms/step - loss: 0.2865 - bin_acc: 0.8795 - val_loss: 0.3186 - val_bin_acc: 0.8728\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.29879\n",
      "Epoch 124/1000\n",
      "\n",
      "Epoch 00124: LearningRateScheduler setting learning rate to 0.0009631332441807784.\n",
      "33/33 [==============================] - 14s 426ms/step - loss: 0.2687 - bin_acc: 0.8853 - val_loss: 0.3216 - val_bin_acc: 0.8664\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.29879\n",
      "Epoch 125/1000\n",
      "\n",
      "Epoch 00125: LearningRateScheduler setting learning rate to 0.000962538978031195.\n",
      "33/33 [==============================] - 14s 429ms/step - loss: 0.2635 - bin_acc: 0.8877 - val_loss: 0.3375 - val_bin_acc: 0.8577\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.29879\n",
      "Epoch 126/1000\n",
      "\n",
      "Epoch 00126: LearningRateScheduler setting learning rate to 0.000961940146857981.\n",
      "33/33 [==============================] - 14s 429ms/step - loss: 0.2760 - bin_acc: 0.8883 - val_loss: 0.3196 - val_bin_acc: 0.8621\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.29879\n",
      "Epoch 127/1000\n",
      "\n",
      "Epoch 00127: LearningRateScheduler setting learning rate to 0.0009613367565713582.\n",
      "33/33 [==============================] - 14s 426ms/step - loss: 0.2974 - bin_acc: 0.8783 - val_loss: 0.3091 - val_bin_acc: 0.8714\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.29879\n",
      "Epoch 128/1000\n",
      "\n",
      "Epoch 00128: LearningRateScheduler setting learning rate to 0.0009607288131265453.\n",
      "33/33 [==============================] - 14s 427ms/step - loss: 0.2667 - bin_acc: 0.8883 - val_loss: 0.3201 - val_bin_acc: 0.8645\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.29879\n",
      "Epoch 129/1000\n",
      "\n",
      "Epoch 00129: LearningRateScheduler setting learning rate to 0.0009601163225236985.\n",
      "33/33 [==============================] - 14s 430ms/step - loss: 0.2719 - bin_acc: 0.8813 - val_loss: 0.3378 - val_bin_acc: 0.8563\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.29879\n",
      "Epoch 130/1000\n",
      "\n",
      "Epoch 00130: LearningRateScheduler setting learning rate to 0.0009594992908078528.\n",
      "33/33 [==============================] - 14s 429ms/step - loss: 0.2730 - bin_acc: 0.8884 - val_loss: 0.3146 - val_bin_acc: 0.8701\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.29879\n",
      "Epoch 131/1000\n",
      "\n",
      "Epoch 00131: LearningRateScheduler setting learning rate to 0.0009588777240688622.\n",
      "33/33 [==============================] - 14s 426ms/step - loss: 0.2670 - bin_acc: 0.8907 - val_loss: 0.3139 - val_bin_acc: 0.8723\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.29879\n",
      "Epoch 132/1000\n",
      "\n",
      "Epoch 00132: LearningRateScheduler setting learning rate to 0.0009582516284413395.\n",
      "33/33 [==============================] - 14s 427ms/step - loss: 0.2850 - bin_acc: 0.8757 - val_loss: 0.3261 - val_bin_acc: 0.8627\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.29879\n",
      "Epoch 133/1000\n",
      "\n",
      "Epoch 00133: LearningRateScheduler setting learning rate to 0.0009576210101045957.\n",
      "33/33 [==============================] - 14s 426ms/step - loss: 0.2471 - bin_acc: 0.9038 - val_loss: 0.3155 - val_bin_acc: 0.8680\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.29879\n",
      "Epoch 134/1000\n",
      "\n",
      "Epoch 00134: LearningRateScheduler setting learning rate to 0.0009569858752825793.\n",
      "33/33 [==============================] - 14s 427ms/step - loss: 0.2878 - bin_acc: 0.8769 - val_loss: 0.3144 - val_bin_acc: 0.8746\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.29879\n",
      "Epoch 135/1000\n",
      "\n",
      "Epoch 00135: LearningRateScheduler setting learning rate to 0.0009563462302438145.\n",
      "33/33 [==============================] - 14s 430ms/step - loss: 0.2863 - bin_acc: 0.8770 - val_loss: 0.3198 - val_bin_acc: 0.8632\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.29879\n",
      "Epoch 136/1000\n",
      "\n",
      "Epoch 00136: LearningRateScheduler setting learning rate to 0.0009557020813013396.\n",
      "33/33 [==============================] - 14s 431ms/step - loss: 0.2987 - bin_acc: 0.8807 - val_loss: 0.3194 - val_bin_acc: 0.8659\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.29879\n",
      "Epoch 137/1000\n",
      "\n",
      "Epoch 00137: LearningRateScheduler setting learning rate to 0.0009550534348126445.\n",
      "33/33 [==============================] - 14s 422ms/step - loss: 0.2512 - bin_acc: 0.9013 - val_loss: 0.3159 - val_bin_acc: 0.8663\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.29879\n",
      "Epoch 138/1000\n",
      "\n",
      "Epoch 00138: LearningRateScheduler setting learning rate to 0.0009544002971796085.\n",
      "33/33 [==============================] - 14s 427ms/step - loss: 0.2971 - bin_acc: 0.8810 - val_loss: 0.3214 - val_bin_acc: 0.8682\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.29879\n",
      "Epoch 139/1000\n",
      "\n",
      "Epoch 00139: LearningRateScheduler setting learning rate to 0.0009537426748484359.\n",
      "33/33 [==============================] - 14s 431ms/step - loss: 0.3090 - bin_acc: 0.8750 - val_loss: 0.3403 - val_bin_acc: 0.8656\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.29879\n",
      "Epoch 140/1000\n",
      "\n",
      "Epoch 00140: LearningRateScheduler setting learning rate to 0.000953080574309594.\n",
      "33/33 [==============================] - 14s 428ms/step - loss: 0.2671 - bin_acc: 0.8883 - val_loss: 0.3751 - val_bin_acc: 0.8507\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.29879\n",
      "Epoch 00140: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "H = model.fit_generator( #train_rep_ans\n",
    "    generator=training_batch(batch_size=batch_size,X_train1=train_emb_api,X_train2=train_emb ,\n",
    "                                             Y_train1=train_rep_ans)#,Y_train2=train_fam_ans) #Y_train2\n",
    "#                     generator=training_batch(batch_size=batch_size,X_train1=valid_emb_api,X_train2=valid_emb ,\n",
    "#                                              Y_train1=train_rep_ans,Y_train2=train_fam_ans)\n",
    "                        , steps_per_epoch=int(np.ceil(len(train_emb_api)/batch_size)) ,\n",
    "                    epochs=num_epochs,callbacks=model_callbacks\n",
    "                   ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans]) #ori\n",
    "#                    ,validation_data= (valid_emb, valid_fam_ans) \n",
    "#                    ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans]) #ori #改\n",
    "#                    ,validation_data= ([valid_emb_api,valid_emb], [valid_rep_ans]+valid_Y2) #validY2\n",
    "                    ,max_queue_size=10 #,class_weight=fam_weights#[None,fam_weights] #改\n",
    "                    ,workers=10,use_multiprocessing=True   \n",
    "                   ,shuffle=True,verbose=1)\n",
    "model.save(model_save_path+\"_all.h5\")\n",
    "#1st:train 0_1_prediction=0.14XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 00028: LearningRateScheduler setting learning rate to 0.000998202360798868.\n",
    "79/79 [==============================] - 36s 450ms/step - loss: 0.4248 - bin_acc: 0.8226 - val_loss: 0.4296 - val_bin_acc: 0.8253\n",
    "\n",
    "Epoch 00028: val_loss improved from 0.43866 to 0.42955, saving model to ./model/o2o_stage_gru_selfatt/byterep_1stStage_0706_gruatt_sent2vec.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 00003: LearningRateScheduler setting learning rate to 0.0014999851957407985.\n",
    "90/90 [==============================] - 39s 431ms/step - loss: 0.6088 - bin_acc: 0.6647 - val_loss: 0.5624 - val_bin_acc: 0.7292\n",
    "\n",
    "Epoch 00003: val_loss did not improve from 0.53809"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 00002: LearningRateScheduler setting learning rate to 0.0014999962989260677.\n",
    "56/56 [==============================] - 23s 419ms/step - loss: 0.5530 - bin_acc: 0.7259 - val_loss: 0.5224 - val_bin_acc: 0.7585\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "* 挑選threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_ids (InputLayer)           (None, 226)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sent_emb (InputLayer)           (None, 226, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 226)          0           sent_ids[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 226, 768)     0           sent_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "api_emb (Embedding)             (None, 226, 768)     20736       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 226, 768)     0           masking_1[0][0]                  \n",
      "                                                                 api_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "common_extract (GRU)            (None, 226, 192)     553536      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 226, 192)     768         common_extract[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "self_attention (SeqSelfAttentio (None, 226, 192)     12353       bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "out_rep (TimeDistributed)       (None, 226, 1)       193         self_attention[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 587,586\n",
      "Trainable params: 587,202\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./model/o2o_stage_gru_selfatt/byterep_2ndStage_0706_gruatt_sent2vec.h5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(model_save_path)\n",
    "model.summary()\n",
    "model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json, model_from_yaml\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score,hamming_loss , roc_auc_score\n",
    "\n",
    "# pickle.dump(file=open(tensorboard_log_path + '/'+'byterep_gruatt_H_2stage.pkl','wb'),obj=H.history) \n",
    "# json_string = model.to_json()\n",
    "# yaml_string = model.to_yaml()\n",
    "# pickle.dump(file=open(tensorboard_log_path + '/'+'byterep_gruatt_arch_2stage.pkl','wb'),obj=(json_string,yaml_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 226, 1)\n"
     ]
    }
   ],
   "source": [
    "ans_t = model.predict([test_emb_api,test_emb])\n",
    "ans_v = model.predict([valid_emb_api,valid_emb])\n",
    "print(ans_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid roc auc score: 0.8866524230103138 test: 0.8787838534467494\n"
     ]
    }
   ],
   "source": [
    "y_true = test_rep_ans.squeeze(axis=-1)\n",
    "ans_t = ans_t.squeeze(axis=-1)\n",
    "area_score_t = roc_auc_score(y_true,ans_t,average='micro')\n",
    "y_true = valid_rep_ans.squeeze(axis=-1)\n",
    "ans_v = ans_v.squeeze(axis=-1)\n",
    "area_score_v = roc_auc_score(y_true,ans_v,average='micro')\n",
    "print('valid roc auc score:',area_score_v,'test:',area_score_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ori highest: valid roc auc score: 0.894314815832789 test: 0.8725151558369375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/99 [00:15<00:15,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best choice threashold now is: 0.49 0.8848084764097301 0.8734031336586322 0.04056535583816836 0.04497073374677723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 51/99 [00:16<00:14,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best choice threashold now is: 0.51 0.8848087553123524 0.8734898340045183 0.04050674949696224 0.044874921608250296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:30<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8443020058153902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "score_list_t = []\n",
    "f1_list_v = []\n",
    "pre_list_v = []\n",
    "rec_list_v = []\n",
    "f1_list_t = []\n",
    "pre_list_t = []\n",
    "rec_list_t = []\n",
    "hloss_list_v = []\n",
    "hloss_list_t = []\n",
    "threashold_list = []\n",
    "max_score = 0\n",
    "for i in tqdm(range(1,100)): #system: 500 (0.002)\n",
    "    thr = i/100 # system: 500 (0.002)\n",
    "    final_ans_t = []\n",
    "    for sample in ans_t:\n",
    "        sample_ans = []\n",
    "        for value in sample:\n",
    "            if value < thr: #0.26 #0.33\n",
    "                sample_ans.append(0)\n",
    "            else:\n",
    "                sample_ans.append(1)\n",
    "        final_ans_t.append(sample_ans)\n",
    "    final_ans_t = np.array(final_ans_t)\n",
    "#     print(final_ans_t.shape , sum(final_ans_t[0]))\n",
    "\n",
    "    final_ans_v = []\n",
    "    for sample in ans_v:\n",
    "        sample_ans = []\n",
    "        for value in sample:\n",
    "            if value < thr: #0.26 #0.33\n",
    "                sample_ans.append(0)\n",
    "            else:\n",
    "                sample_ans.append(1)\n",
    "        final_ans_v.append(sample_ans)\n",
    "    final_ans_v = np.array(final_ans_v)\n",
    "#     print(final_ans_v.shape , sum(final_ans_v[0]))\n",
    "\n",
    "    y_true = test_rep_ans.squeeze(axis=-1)\n",
    "    recall_t = recall_score(y_true=y_true, y_pred=final_ans_t, average='micro')\n",
    "    precision_t = precision_score(y_true=y_true, y_pred=final_ans_t, average='micro')\n",
    "    f1_t = f1_score(y_true=y_true, y_pred=final_ans_t, average='micro')\n",
    "    h_loss_t = hamming_loss(y_true,final_ans_t)\n",
    "#     print('Test: (recall/precision/f1/h_loss)',recall_t ,precision_t, f1_t , h_loss_t)\n",
    "\n",
    "    y_true = valid_rep_ans.squeeze(axis=-1)\n",
    "    recall_v = recall_score(y_true=y_true, y_pred=final_ans_v, average='micro')\n",
    "    precision_v = precision_score(y_true=y_true, y_pred=final_ans_v, average='micro')\n",
    "    f1_v = f1_score(y_true=y_true, y_pred=final_ans_v, average='micro')\n",
    "    h_loss_v = hamming_loss(y_true,final_ans_v)\n",
    "#     score = (f1_v+1.5*f1_t-h_loss_v-1.5*h_loss_t)/2.5\n",
    "    score = f1_v - h_loss_v\n",
    "    score_t = f1_t - h_loss_t\n",
    "    if score>max_score:\n",
    "        print('Best choice threashold now is:',thr,f1_v,f1_t,h_loss_v,h_loss_t)\n",
    "        max_score = score\n",
    "    score_list.append(score)\n",
    "    score_list_t.append(score_t)\n",
    "    threashold_list.append(thr)\n",
    "    f1_list_v.append(f1_v)\n",
    "    pre_list_v.append(precision_v)\n",
    "    rec_list_v.append(recall_v)\n",
    "    f1_list_t.append(f1_t)\n",
    "    pre_list_t.append(precision_t)\n",
    "    rec_list_t.append(recall_t)\n",
    "    hloss_list_v.append(h_loss_v)\n",
    "    hloss_list_t.append(h_loss_t)\n",
    "#     print('Valid: (recall/precision/f1/h_loss)=>',recall_v ,precision_v, f1_v , h_loss_v)\n",
    "print(max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>Precision_valid</th>\n",
       "      <th>Recall_valid</th>\n",
       "      <th>F1_valid</th>\n",
       "      <th>Hloss_valid</th>\n",
       "      <th>score_valid</th>\n",
       "      <th>Precision_test</th>\n",
       "      <th>Recall_test</th>\n",
       "      <th>F1_test</th>\n",
       "      <th>Hloss_test</th>\n",
       "      <th>score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.185124</td>\n",
       "      <td>0.999179</td>\n",
       "      <td>0.312373</td>\n",
       "      <td>0.785501</td>\n",
       "      <td>-0.473128</td>\n",
       "      <td>0.185536</td>\n",
       "      <td>0.999611</td>\n",
       "      <td>0.312981</td>\n",
       "      <td>0.785433</td>\n",
       "      <td>-0.472452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.187375</td>\n",
       "      <td>0.998742</td>\n",
       "      <td>0.315549</td>\n",
       "      <td>0.773662</td>\n",
       "      <td>-0.458113</td>\n",
       "      <td>0.187685</td>\n",
       "      <td>0.998783</td>\n",
       "      <td>0.315991</td>\n",
       "      <td>0.773901</td>\n",
       "      <td>-0.457910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.188940</td>\n",
       "      <td>0.998031</td>\n",
       "      <td>0.317730</td>\n",
       "      <td>0.765360</td>\n",
       "      <td>-0.447630</td>\n",
       "      <td>0.188931</td>\n",
       "      <td>0.995425</td>\n",
       "      <td>0.317584</td>\n",
       "      <td>0.765644</td>\n",
       "      <td>-0.448059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.189952</td>\n",
       "      <td>0.996280</td>\n",
       "      <td>0.319070</td>\n",
       "      <td>0.759314</td>\n",
       "      <td>-0.440243</td>\n",
       "      <td>0.189944</td>\n",
       "      <td>0.993673</td>\n",
       "      <td>0.318924</td>\n",
       "      <td>0.759590</td>\n",
       "      <td>-0.440666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.190384</td>\n",
       "      <td>0.996171</td>\n",
       "      <td>0.319673</td>\n",
       "      <td>0.757126</td>\n",
       "      <td>-0.437452</td>\n",
       "      <td>0.190359</td>\n",
       "      <td>0.993625</td>\n",
       "      <td>0.319507</td>\n",
       "      <td>0.757517</td>\n",
       "      <td>-0.438010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.190589</td>\n",
       "      <td>0.995952</td>\n",
       "      <td>0.319951</td>\n",
       "      <td>0.755992</td>\n",
       "      <td>-0.436041</td>\n",
       "      <td>0.190569</td>\n",
       "      <td>0.993430</td>\n",
       "      <td>0.319793</td>\n",
       "      <td>0.756376</td>\n",
       "      <td>-0.436583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.190794</td>\n",
       "      <td>0.995569</td>\n",
       "      <td>0.320220</td>\n",
       "      <td>0.754772</td>\n",
       "      <td>-0.434552</td>\n",
       "      <td>0.190799</td>\n",
       "      <td>0.993041</td>\n",
       "      <td>0.320096</td>\n",
       "      <td>0.755026</td>\n",
       "      <td>-0.434930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.190894</td>\n",
       "      <td>0.995350</td>\n",
       "      <td>0.320349</td>\n",
       "      <td>0.754156</td>\n",
       "      <td>-0.433807</td>\n",
       "      <td>0.190866</td>\n",
       "      <td>0.992505</td>\n",
       "      <td>0.320162</td>\n",
       "      <td>0.754390</td>\n",
       "      <td>-0.434228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.190963</td>\n",
       "      <td>0.994967</td>\n",
       "      <td>0.320426</td>\n",
       "      <td>0.753599</td>\n",
       "      <td>-0.433173</td>\n",
       "      <td>0.190898</td>\n",
       "      <td>0.991921</td>\n",
       "      <td>0.320177</td>\n",
       "      <td>0.753893</td>\n",
       "      <td>-0.433716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.190928</td>\n",
       "      <td>0.993709</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>0.753043</td>\n",
       "      <td>-0.432731</td>\n",
       "      <td>0.190901</td>\n",
       "      <td>0.990802</td>\n",
       "      <td>0.320123</td>\n",
       "      <td>0.753231</td>\n",
       "      <td>-0.433109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.191142</td>\n",
       "      <td>0.993162</td>\n",
       "      <td>0.320585</td>\n",
       "      <td>0.751685</td>\n",
       "      <td>-0.431100</td>\n",
       "      <td>0.191137</td>\n",
       "      <td>0.990315</td>\n",
       "      <td>0.320429</td>\n",
       "      <td>0.751803</td>\n",
       "      <td>-0.431374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.191380</td>\n",
       "      <td>0.992943</td>\n",
       "      <td>0.320908</td>\n",
       "      <td>0.750405</td>\n",
       "      <td>-0.429497</td>\n",
       "      <td>0.191284</td>\n",
       "      <td>0.989683</td>\n",
       "      <td>0.320603</td>\n",
       "      <td>0.750723</td>\n",
       "      <td>-0.430120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.192050</td>\n",
       "      <td>0.992342</td>\n",
       "      <td>0.321817</td>\n",
       "      <td>0.746830</td>\n",
       "      <td>-0.425013</td>\n",
       "      <td>0.191595</td>\n",
       "      <td>0.987103</td>\n",
       "      <td>0.320903</td>\n",
       "      <td>0.747735</td>\n",
       "      <td>-0.426832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.192445</td>\n",
       "      <td>0.991302</td>\n",
       "      <td>0.322318</td>\n",
       "      <td>0.744340</td>\n",
       "      <td>-0.422022</td>\n",
       "      <td>0.191763</td>\n",
       "      <td>0.984086</td>\n",
       "      <td>0.320978</td>\n",
       "      <td>0.745192</td>\n",
       "      <td>-0.424214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.192600</td>\n",
       "      <td>0.990865</td>\n",
       "      <td>0.322511</td>\n",
       "      <td>0.743353</td>\n",
       "      <td>-0.420842</td>\n",
       "      <td>0.191865</td>\n",
       "      <td>0.983259</td>\n",
       "      <td>0.321078</td>\n",
       "      <td>0.744225</td>\n",
       "      <td>-0.423147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.192838</td>\n",
       "      <td>0.990591</td>\n",
       "      <td>0.322830</td>\n",
       "      <td>0.742064</td>\n",
       "      <td>-0.419234</td>\n",
       "      <td>0.191947</td>\n",
       "      <td>0.982091</td>\n",
       "      <td>0.321130</td>\n",
       "      <td>0.743162</td>\n",
       "      <td>-0.422032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.193085</td>\n",
       "      <td>0.990044</td>\n",
       "      <td>0.323147</td>\n",
       "      <td>0.740579</td>\n",
       "      <td>-0.417432</td>\n",
       "      <td>0.191993</td>\n",
       "      <td>0.980436</td>\n",
       "      <td>0.321106</td>\n",
       "      <td>0.741995</td>\n",
       "      <td>-0.420890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.193112</td>\n",
       "      <td>0.988896</td>\n",
       "      <td>0.323124</td>\n",
       "      <td>0.739798</td>\n",
       "      <td>-0.416673</td>\n",
       "      <td>0.191909</td>\n",
       "      <td>0.978587</td>\n",
       "      <td>0.320889</td>\n",
       "      <td>0.741333</td>\n",
       "      <td>-0.420445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.193117</td>\n",
       "      <td>0.986872</td>\n",
       "      <td>0.323023</td>\n",
       "      <td>0.738625</td>\n",
       "      <td>-0.415602</td>\n",
       "      <td>0.191892</td>\n",
       "      <td>0.976494</td>\n",
       "      <td>0.320753</td>\n",
       "      <td>0.740210</td>\n",
       "      <td>-0.419457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.193044</td>\n",
       "      <td>0.985231</td>\n",
       "      <td>0.322833</td>\n",
       "      <td>0.738039</td>\n",
       "      <td>-0.415207</td>\n",
       "      <td>0.191728</td>\n",
       "      <td>0.974304</td>\n",
       "      <td>0.320405</td>\n",
       "      <td>0.739731</td>\n",
       "      <td>-0.419326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.193133</td>\n",
       "      <td>0.984902</td>\n",
       "      <td>0.322939</td>\n",
       "      <td>0.737434</td>\n",
       "      <td>-0.414494</td>\n",
       "      <td>0.191772</td>\n",
       "      <td>0.973720</td>\n",
       "      <td>0.320436</td>\n",
       "      <td>0.739182</td>\n",
       "      <td>-0.418746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.193117</td>\n",
       "      <td>0.984410</td>\n",
       "      <td>0.322891</td>\n",
       "      <td>0.737229</td>\n",
       "      <td>-0.414338</td>\n",
       "      <td>0.191722</td>\n",
       "      <td>0.972990</td>\n",
       "      <td>0.320326</td>\n",
       "      <td>0.738999</td>\n",
       "      <td>-0.418673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.193139</td>\n",
       "      <td>0.983371</td>\n",
       "      <td>0.322866</td>\n",
       "      <td>0.736535</td>\n",
       "      <td>-0.413670</td>\n",
       "      <td>0.191614</td>\n",
       "      <td>0.971238</td>\n",
       "      <td>0.320080</td>\n",
       "      <td>0.738503</td>\n",
       "      <td>-0.418422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.193417</td>\n",
       "      <td>0.981620</td>\n",
       "      <td>0.323159</td>\n",
       "      <td>0.734240</td>\n",
       "      <td>-0.411081</td>\n",
       "      <td>0.191849</td>\n",
       "      <td>0.969291</td>\n",
       "      <td>0.320302</td>\n",
       "      <td>0.736273</td>\n",
       "      <td>-0.415971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.193420</td>\n",
       "      <td>0.981128</td>\n",
       "      <td>0.323136</td>\n",
       "      <td>0.733947</td>\n",
       "      <td>-0.410811</td>\n",
       "      <td>0.191898</td>\n",
       "      <td>0.968999</td>\n",
       "      <td>0.320355</td>\n",
       "      <td>0.735872</td>\n",
       "      <td>-0.415517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.193326</td>\n",
       "      <td>0.978940</td>\n",
       "      <td>0.322887</td>\n",
       "      <td>0.733146</td>\n",
       "      <td>-0.410259</td>\n",
       "      <td>0.191918</td>\n",
       "      <td>0.967734</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>0.735053</td>\n",
       "      <td>-0.414741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.193919</td>\n",
       "      <td>0.972321</td>\n",
       "      <td>0.323349</td>\n",
       "      <td>0.726650</td>\n",
       "      <td>-0.403301</td>\n",
       "      <td>0.190237</td>\n",
       "      <td>0.951674</td>\n",
       "      <td>0.317088</td>\n",
       "      <td>0.733668</td>\n",
       "      <td>-0.416580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.193883</td>\n",
       "      <td>0.968875</td>\n",
       "      <td>0.323108</td>\n",
       "      <td>0.724873</td>\n",
       "      <td>-0.401764</td>\n",
       "      <td>0.189962</td>\n",
       "      <td>0.947148</td>\n",
       "      <td>0.316455</td>\n",
       "      <td>0.732318</td>\n",
       "      <td>-0.415863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.193554</td>\n",
       "      <td>0.964827</td>\n",
       "      <td>0.322426</td>\n",
       "      <td>0.724101</td>\n",
       "      <td>-0.401675</td>\n",
       "      <td>0.189779</td>\n",
       "      <td>0.944131</td>\n",
       "      <td>0.316033</td>\n",
       "      <td>0.731412</td>\n",
       "      <td>-0.415379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.193286</td>\n",
       "      <td>0.961490</td>\n",
       "      <td>0.321867</td>\n",
       "      <td>0.723446</td>\n",
       "      <td>-0.401579</td>\n",
       "      <td>0.189560</td>\n",
       "      <td>0.941405</td>\n",
       "      <td>0.315576</td>\n",
       "      <td>0.730846</td>\n",
       "      <td>-0.415271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.911609</td>\n",
       "      <td>0.840600</td>\n",
       "      <td>0.874666</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.831649</td>\n",
       "      <td>0.894899</td>\n",
       "      <td>0.832490</td>\n",
       "      <td>0.862567</td>\n",
       "      <td>0.047479</td>\n",
       "      <td>0.815088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.912973</td>\n",
       "      <td>0.837263</td>\n",
       "      <td>0.873481</td>\n",
       "      <td>0.043310</td>\n",
       "      <td>0.830170</td>\n",
       "      <td>0.896349</td>\n",
       "      <td>0.829083</td>\n",
       "      <td>0.861405</td>\n",
       "      <td>0.047749</td>\n",
       "      <td>0.813655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.913413</td>\n",
       "      <td>0.834418</td>\n",
       "      <td>0.872131</td>\n",
       "      <td>0.043691</td>\n",
       "      <td>0.828440</td>\n",
       "      <td>0.896974</td>\n",
       "      <td>0.826650</td>\n",
       "      <td>0.860377</td>\n",
       "      <td>0.048019</td>\n",
       "      <td>0.812358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.913565</td>\n",
       "      <td>0.833707</td>\n",
       "      <td>0.871811</td>\n",
       "      <td>0.043779</td>\n",
       "      <td>0.828032</td>\n",
       "      <td>0.897075</td>\n",
       "      <td>0.825433</td>\n",
       "      <td>0.859764</td>\n",
       "      <td>0.048194</td>\n",
       "      <td>0.811571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.914186</td>\n",
       "      <td>0.829823</td>\n",
       "      <td>0.869964</td>\n",
       "      <td>0.044297</td>\n",
       "      <td>0.825668</td>\n",
       "      <td>0.897745</td>\n",
       "      <td>0.821637</td>\n",
       "      <td>0.858007</td>\n",
       "      <td>0.048673</td>\n",
       "      <td>0.809334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.914665</td>\n",
       "      <td>0.829057</td>\n",
       "      <td>0.869760</td>\n",
       "      <td>0.044336</td>\n",
       "      <td>0.825424</td>\n",
       "      <td>0.897833</td>\n",
       "      <td>0.820712</td>\n",
       "      <td>0.857542</td>\n",
       "      <td>0.048803</td>\n",
       "      <td>0.808739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.915547</td>\n",
       "      <td>0.827854</td>\n",
       "      <td>0.869495</td>\n",
       "      <td>0.044375</td>\n",
       "      <td>0.825121</td>\n",
       "      <td>0.898925</td>\n",
       "      <td>0.818036</td>\n",
       "      <td>0.856575</td>\n",
       "      <td>0.049030</td>\n",
       "      <td>0.807545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.916186</td>\n",
       "      <td>0.825775</td>\n",
       "      <td>0.868635</td>\n",
       "      <td>0.044599</td>\n",
       "      <td>0.824035</td>\n",
       "      <td>0.899737</td>\n",
       "      <td>0.815797</td>\n",
       "      <td>0.855714</td>\n",
       "      <td>0.049239</td>\n",
       "      <td>0.806475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.919705</td>\n",
       "      <td>0.818281</td>\n",
       "      <td>0.866034</td>\n",
       "      <td>0.045205</td>\n",
       "      <td>0.820829</td>\n",
       "      <td>0.901641</td>\n",
       "      <td>0.810152</td>\n",
       "      <td>0.853452</td>\n",
       "      <td>0.049796</td>\n",
       "      <td>0.803655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.920755</td>\n",
       "      <td>0.816093</td>\n",
       "      <td>0.865271</td>\n",
       "      <td>0.045381</td>\n",
       "      <td>0.819890</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.808351</td>\n",
       "      <td>0.852516</td>\n",
       "      <td>0.050057</td>\n",
       "      <td>0.802459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.921418</td>\n",
       "      <td>0.813303</td>\n",
       "      <td>0.863992</td>\n",
       "      <td>0.045723</td>\n",
       "      <td>0.818269</td>\n",
       "      <td>0.902184</td>\n",
       "      <td>0.806161</td>\n",
       "      <td>0.851474</td>\n",
       "      <td>0.050336</td>\n",
       "      <td>0.801138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.921537</td>\n",
       "      <td>0.808216</td>\n",
       "      <td>0.861165</td>\n",
       "      <td>0.046533</td>\n",
       "      <td>0.814631</td>\n",
       "      <td>0.902667</td>\n",
       "      <td>0.800662</td>\n",
       "      <td>0.848610</td>\n",
       "      <td>0.051129</td>\n",
       "      <td>0.797481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.922391</td>\n",
       "      <td>0.803567</td>\n",
       "      <td>0.858889</td>\n",
       "      <td>0.047149</td>\n",
       "      <td>0.811740</td>\n",
       "      <td>0.904141</td>\n",
       "      <td>0.795941</td>\n",
       "      <td>0.846598</td>\n",
       "      <td>0.051625</td>\n",
       "      <td>0.794972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.923014</td>\n",
       "      <td>0.797495</td>\n",
       "      <td>0.855676</td>\n",
       "      <td>0.048038</td>\n",
       "      <td>0.807638</td>\n",
       "      <td>0.905369</td>\n",
       "      <td>0.791999</td>\n",
       "      <td>0.844898</td>\n",
       "      <td>0.052043</td>\n",
       "      <td>0.792854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.925385</td>\n",
       "      <td>0.782889</td>\n",
       "      <td>0.848194</td>\n",
       "      <td>0.050040</td>\n",
       "      <td>0.798154</td>\n",
       "      <td>0.907597</td>\n",
       "      <td>0.776767</td>\n",
       "      <td>0.837101</td>\n",
       "      <td>0.054108</td>\n",
       "      <td>0.782993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.937990</td>\n",
       "      <td>0.687599</td>\n",
       "      <td>0.793511</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.729610</td>\n",
       "      <td>0.919410</td>\n",
       "      <td>0.679580</td>\n",
       "      <td>0.781509</td>\n",
       "      <td>0.068009</td>\n",
       "      <td>0.713500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.942132</td>\n",
       "      <td>0.666156</td>\n",
       "      <td>0.780466</td>\n",
       "      <td>0.066919</td>\n",
       "      <td>0.713547</td>\n",
       "      <td>0.922830</td>\n",
       "      <td>0.658799</td>\n",
       "      <td>0.768776</td>\n",
       "      <td>0.070927</td>\n",
       "      <td>0.697849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.947169</td>\n",
       "      <td>0.642361</td>\n",
       "      <td>0.765540</td>\n",
       "      <td>0.070259</td>\n",
       "      <td>0.695281</td>\n",
       "      <td>0.927566</td>\n",
       "      <td>0.631935</td>\n",
       "      <td>0.751730</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>0.677022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.949617</td>\n",
       "      <td>0.623762</td>\n",
       "      <td>0.752947</td>\n",
       "      <td>0.073092</td>\n",
       "      <td>0.679855</td>\n",
       "      <td>0.929594</td>\n",
       "      <td>0.613004</td>\n",
       "      <td>0.738812</td>\n",
       "      <td>0.077573</td>\n",
       "      <td>0.661239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.953404</td>\n",
       "      <td>0.606641</td>\n",
       "      <td>0.741484</td>\n",
       "      <td>0.075534</td>\n",
       "      <td>0.665950</td>\n",
       "      <td>0.933090</td>\n",
       "      <td>0.596554</td>\n",
       "      <td>0.727802</td>\n",
       "      <td>0.079864</td>\n",
       "      <td>0.647938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.955308</td>\n",
       "      <td>0.595153</td>\n",
       "      <td>0.733401</td>\n",
       "      <td>0.077263</td>\n",
       "      <td>0.656138</td>\n",
       "      <td>0.935135</td>\n",
       "      <td>0.584436</td>\n",
       "      <td>0.719317</td>\n",
       "      <td>0.081632</td>\n",
       "      <td>0.637685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.956359</td>\n",
       "      <td>0.587386</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.078464</td>\n",
       "      <td>0.649314</td>\n",
       "      <td>0.936298</td>\n",
       "      <td>0.575822</td>\n",
       "      <td>0.713093</td>\n",
       "      <td>0.082930</td>\n",
       "      <td>0.630164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.957920</td>\n",
       "      <td>0.570319</td>\n",
       "      <td>0.714967</td>\n",
       "      <td>0.081199</td>\n",
       "      <td>0.633767</td>\n",
       "      <td>0.940070</td>\n",
       "      <td>0.559568</td>\n",
       "      <td>0.701547</td>\n",
       "      <td>0.085212</td>\n",
       "      <td>0.616335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.960430</td>\n",
       "      <td>0.552322</td>\n",
       "      <td>0.701327</td>\n",
       "      <td>0.084002</td>\n",
       "      <td>0.617324</td>\n",
       "      <td>0.943887</td>\n",
       "      <td>0.540296</td>\n",
       "      <td>0.687218</td>\n",
       "      <td>0.088025</td>\n",
       "      <td>0.599192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.962367</td>\n",
       "      <td>0.532958</td>\n",
       "      <td>0.686006</td>\n",
       "      <td>0.087118</td>\n",
       "      <td>0.598888</td>\n",
       "      <td>0.946331</td>\n",
       "      <td>0.520878</td>\n",
       "      <td>0.671919</td>\n",
       "      <td>0.091039</td>\n",
       "      <td>0.580880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.965007</td>\n",
       "      <td>0.506865</td>\n",
       "      <td>0.664634</td>\n",
       "      <td>0.091338</td>\n",
       "      <td>0.573296</td>\n",
       "      <td>0.952756</td>\n",
       "      <td>0.494647</td>\n",
       "      <td>0.651205</td>\n",
       "      <td>0.094837</td>\n",
       "      <td>0.556368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.971404</td>\n",
       "      <td>0.431103</td>\n",
       "      <td>0.597181</td>\n",
       "      <td>0.103850</td>\n",
       "      <td>0.493331</td>\n",
       "      <td>0.962025</td>\n",
       "      <td>0.421647</td>\n",
       "      <td>0.586317</td>\n",
       "      <td>0.106491</td>\n",
       "      <td>0.479826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.980046</td>\n",
       "      <td>0.370767</td>\n",
       "      <td>0.538001</td>\n",
       "      <td>0.113706</td>\n",
       "      <td>0.424294</td>\n",
       "      <td>0.971414</td>\n",
       "      <td>0.360522</td>\n",
       "      <td>0.525875</td>\n",
       "      <td>0.116351</td>\n",
       "      <td>0.409524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.988591</td>\n",
       "      <td>0.341283</td>\n",
       "      <td>0.507401</td>\n",
       "      <td>0.118326</td>\n",
       "      <td>0.389075</td>\n",
       "      <td>0.976408</td>\n",
       "      <td>0.328304</td>\n",
       "      <td>0.491387</td>\n",
       "      <td>0.121638</td>\n",
       "      <td>0.369749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.997058</td>\n",
       "      <td>0.296592</td>\n",
       "      <td>0.457186</td>\n",
       "      <td>0.125759</td>\n",
       "      <td>0.331427</td>\n",
       "      <td>0.986915</td>\n",
       "      <td>0.286305</td>\n",
       "      <td>0.443849</td>\n",
       "      <td>0.128414</td>\n",
       "      <td>0.315435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold  Precision_valid  Recall_valid  F1_valid  Hloss_valid  \\\n",
       "0        0.01         0.185124      0.999179  0.312373     0.785501   \n",
       "1        0.02         0.187375      0.998742  0.315549     0.773662   \n",
       "2        0.03         0.188940      0.998031  0.317730     0.765360   \n",
       "3        0.04         0.189952      0.996280  0.319070     0.759314   \n",
       "4        0.05         0.190384      0.996171  0.319673     0.757126   \n",
       "5        0.06         0.190589      0.995952  0.319951     0.755992   \n",
       "6        0.07         0.190794      0.995569  0.320220     0.754772   \n",
       "7        0.08         0.190894      0.995350  0.320349     0.754156   \n",
       "8        0.09         0.190963      0.994967  0.320426     0.753599   \n",
       "9        0.10         0.190928      0.993709  0.320312     0.753043   \n",
       "10       0.11         0.191142      0.993162  0.320585     0.751685   \n",
       "11       0.12         0.191380      0.992943  0.320908     0.750405   \n",
       "12       0.13         0.192050      0.992342  0.321817     0.746830   \n",
       "13       0.14         0.192445      0.991302  0.322318     0.744340   \n",
       "14       0.15         0.192600      0.990865  0.322511     0.743353   \n",
       "15       0.16         0.192838      0.990591  0.322830     0.742064   \n",
       "16       0.17         0.193085      0.990044  0.323147     0.740579   \n",
       "17       0.18         0.193112      0.988896  0.323124     0.739798   \n",
       "18       0.19         0.193117      0.986872  0.323023     0.738625   \n",
       "19       0.20         0.193044      0.985231  0.322833     0.738039   \n",
       "20       0.21         0.193133      0.984902  0.322939     0.737434   \n",
       "21       0.22         0.193117      0.984410  0.322891     0.737229   \n",
       "22       0.23         0.193139      0.983371  0.322866     0.736535   \n",
       "23       0.24         0.193417      0.981620  0.323159     0.734240   \n",
       "24       0.25         0.193420      0.981128  0.323136     0.733947   \n",
       "25       0.26         0.193326      0.978940  0.322887     0.733146   \n",
       "26       0.27         0.193919      0.972321  0.323349     0.726650   \n",
       "27       0.28         0.193883      0.968875  0.323108     0.724873   \n",
       "28       0.29         0.193554      0.964827  0.322426     0.724101   \n",
       "29       0.30         0.193286      0.961490  0.321867     0.723446   \n",
       "..        ...              ...           ...       ...          ...   \n",
       "69       0.70         0.911609      0.840600  0.874666     0.043017   \n",
       "70       0.71         0.912973      0.837263  0.873481     0.043310   \n",
       "71       0.72         0.913413      0.834418  0.872131     0.043691   \n",
       "72       0.73         0.913565      0.833707  0.871811     0.043779   \n",
       "73       0.74         0.914186      0.829823  0.869964     0.044297   \n",
       "74       0.75         0.914665      0.829057  0.869760     0.044336   \n",
       "75       0.76         0.915547      0.827854  0.869495     0.044375   \n",
       "76       0.77         0.916186      0.825775  0.868635     0.044599   \n",
       "77       0.78         0.919705      0.818281  0.866034     0.045205   \n",
       "78       0.79         0.920755      0.816093  0.865271     0.045381   \n",
       "79       0.80         0.921418      0.813303  0.863992     0.045723   \n",
       "80       0.81         0.921537      0.808216  0.861165     0.046533   \n",
       "81       0.82         0.922391      0.803567  0.858889     0.047149   \n",
       "82       0.83         0.923014      0.797495  0.855676     0.048038   \n",
       "83       0.84         0.925385      0.782889  0.848194     0.050040   \n",
       "84       0.85         0.937990      0.687599  0.793511     0.063900   \n",
       "85       0.86         0.942132      0.666156  0.780466     0.066919   \n",
       "86       0.87         0.947169      0.642361  0.765540     0.070259   \n",
       "87       0.88         0.949617      0.623762  0.752947     0.073092   \n",
       "88       0.89         0.953404      0.606641  0.741484     0.075534   \n",
       "89       0.90         0.955308      0.595153  0.733401     0.077263   \n",
       "90       0.91         0.956359      0.587386  0.727778     0.078464   \n",
       "91       0.92         0.957920      0.570319  0.714967     0.081199   \n",
       "92       0.93         0.960430      0.552322  0.701327     0.084002   \n",
       "93       0.94         0.962367      0.532958  0.686006     0.087118   \n",
       "94       0.95         0.965007      0.506865  0.664634     0.091338   \n",
       "95       0.96         0.971404      0.431103  0.597181     0.103850   \n",
       "96       0.97         0.980046      0.370767  0.538001     0.113706   \n",
       "97       0.98         0.988591      0.341283  0.507401     0.118326   \n",
       "98       0.99         0.997058      0.296592  0.457186     0.125759   \n",
       "\n",
       "    score_valid  Precision_test  Recall_test   F1_test  Hloss_test  score_test  \n",
       "0     -0.473128        0.185536     0.999611  0.312981    0.785433   -0.472452  \n",
       "1     -0.458113        0.187685     0.998783  0.315991    0.773901   -0.457910  \n",
       "2     -0.447630        0.188931     0.995425  0.317584    0.765644   -0.448059  \n",
       "3     -0.440243        0.189944     0.993673  0.318924    0.759590   -0.440666  \n",
       "4     -0.437452        0.190359     0.993625  0.319507    0.757517   -0.438010  \n",
       "5     -0.436041        0.190569     0.993430  0.319793    0.756376   -0.436583  \n",
       "6     -0.434552        0.190799     0.993041  0.320096    0.755026   -0.434930  \n",
       "7     -0.433807        0.190866     0.992505  0.320162    0.754390   -0.434228  \n",
       "8     -0.433173        0.190898     0.991921  0.320177    0.753893   -0.433716  \n",
       "9     -0.432731        0.190901     0.990802  0.320123    0.753231   -0.433109  \n",
       "10    -0.431100        0.191137     0.990315  0.320429    0.751803   -0.431374  \n",
       "11    -0.429497        0.191284     0.989683  0.320603    0.750723   -0.430120  \n",
       "12    -0.425013        0.191595     0.987103  0.320903    0.747735   -0.426832  \n",
       "13    -0.422022        0.191763     0.984086  0.320978    0.745192   -0.424214  \n",
       "14    -0.420842        0.191865     0.983259  0.321078    0.744225   -0.423147  \n",
       "15    -0.419234        0.191947     0.982091  0.321130    0.743162   -0.422032  \n",
       "16    -0.417432        0.191993     0.980436  0.321106    0.741995   -0.420890  \n",
       "17    -0.416673        0.191909     0.978587  0.320889    0.741333   -0.420445  \n",
       "18    -0.415602        0.191892     0.976494  0.320753    0.740210   -0.419457  \n",
       "19    -0.415207        0.191728     0.974304  0.320405    0.739731   -0.419326  \n",
       "20    -0.414494        0.191772     0.973720  0.320436    0.739182   -0.418746  \n",
       "21    -0.414338        0.191722     0.972990  0.320326    0.738999   -0.418673  \n",
       "22    -0.413670        0.191614     0.971238  0.320080    0.738503   -0.418422  \n",
       "23    -0.411081        0.191849     0.969291  0.320302    0.736273   -0.415971  \n",
       "24    -0.410811        0.191898     0.968999  0.320355    0.735872   -0.415517  \n",
       "25    -0.410259        0.191918     0.967734  0.320312    0.735053   -0.414741  \n",
       "26    -0.403301        0.190237     0.951674  0.317088    0.733668   -0.416580  \n",
       "27    -0.401764        0.189962     0.947148  0.316455    0.732318   -0.415863  \n",
       "28    -0.401675        0.189779     0.944131  0.316033    0.731412   -0.415379  \n",
       "29    -0.401579        0.189560     0.941405  0.315576    0.730846   -0.415271  \n",
       "..          ...             ...          ...       ...         ...         ...  \n",
       "69     0.831649        0.894899     0.832490  0.862567    0.047479    0.815088  \n",
       "70     0.830170        0.896349     0.829083  0.861405    0.047749    0.813655  \n",
       "71     0.828440        0.896974     0.826650  0.860377    0.048019    0.812358  \n",
       "72     0.828032        0.897075     0.825433  0.859764    0.048194    0.811571  \n",
       "73     0.825668        0.897745     0.821637  0.858007    0.048673    0.809334  \n",
       "74     0.825424        0.897833     0.820712  0.857542    0.048803    0.808739  \n",
       "75     0.825121        0.898925     0.818036  0.856575    0.049030    0.807545  \n",
       "76     0.824035        0.899737     0.815797  0.855714    0.049239    0.806475  \n",
       "77     0.820829        0.901641     0.810152  0.853452    0.049796    0.803655  \n",
       "78     0.819890        0.901786     0.808351  0.852516    0.050057    0.802459  \n",
       "79     0.818269        0.902184     0.806161  0.851474    0.050336    0.801138  \n",
       "80     0.814631        0.902667     0.800662  0.848610    0.051129    0.797481  \n",
       "81     0.811740        0.904141     0.795941  0.846598    0.051625    0.794972  \n",
       "82     0.807638        0.905369     0.791999  0.844898    0.052043    0.792854  \n",
       "83     0.798154        0.907597     0.776767  0.837101    0.054108    0.782993  \n",
       "84     0.729610        0.919410     0.679580  0.781509    0.068009    0.713500  \n",
       "85     0.713547        0.922830     0.658799  0.768776    0.070927    0.697849  \n",
       "86     0.695281        0.927566     0.631935  0.751730    0.074707    0.677022  \n",
       "87     0.679855        0.929594     0.613004  0.738812    0.077573    0.661239  \n",
       "88     0.665950        0.933090     0.596554  0.727802    0.079864    0.647938  \n",
       "89     0.656138        0.935135     0.584436  0.719317    0.081632    0.637685  \n",
       "90     0.649314        0.936298     0.575822  0.713093    0.082930    0.630164  \n",
       "91     0.633767        0.940070     0.559568  0.701547    0.085212    0.616335  \n",
       "92     0.617324        0.943887     0.540296  0.687218    0.088025    0.599192  \n",
       "93     0.598888        0.946331     0.520878  0.671919    0.091039    0.580880  \n",
       "94     0.573296        0.952756     0.494647  0.651205    0.094837    0.556368  \n",
       "95     0.493331        0.962025     0.421647  0.586317    0.106491    0.479826  \n",
       "96     0.424294        0.971414     0.360522  0.525875    0.116351    0.409524  \n",
       "97     0.389075        0.976408     0.328304  0.491387    0.121638    0.369749  \n",
       "98     0.331427        0.986915     0.286305  0.443849    0.128414    0.315435  \n",
       "\n",
       "[99 rows x 11 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame(data={'threshold':threashold_list,'Precision_valid':pre_list_v,'Recall_valid':rec_list_v\n",
    "                              ,'F1_valid':f1_list_v,'Hloss_valid':hloss_list_v\n",
    "                              ,'score_valid':score_list,'Precision_test':pre_list_t,'Recall_test':rec_list_t,\n",
    "                              'F1_test':f1_list_t,'Hloss_test':hloss_list_t\n",
    "                              ,'score_test':score_list_t})\n",
    "score_df.to_excel('data/tree-rep-profiles_o2o/threxp_F1_rec_pre_Hloss.xlsx',index=False)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>Precision_valid</th>\n",
       "      <th>Recall_valid</th>\n",
       "      <th>F1_valid</th>\n",
       "      <th>Hloss_valid</th>\n",
       "      <th>score_valid</th>\n",
       "      <th>Precision_test</th>\n",
       "      <th>Recall_test</th>\n",
       "      <th>F1_test</th>\n",
       "      <th>Hloss_test</th>\n",
       "      <th>score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.898815</td>\n",
       "      <td>0.871232</td>\n",
       "      <td>0.884809</td>\n",
       "      <td>0.040507</td>\n",
       "      <td>0.844302</td>\n",
       "      <td>0.881542</td>\n",
       "      <td>0.865583</td>\n",
       "      <td>0.87349</td>\n",
       "      <td>0.044875</td>\n",
       "      <td>0.828615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold  Precision_valid  Recall_valid  F1_valid  Hloss_valid  \\\n",
       "50       0.51         0.898815      0.871232  0.884809     0.040507   \n",
       "\n",
       "    score_valid  Precision_test  Recall_test  F1_test  Hloss_test  score_test  \n",
       "50     0.844302        0.881542     0.865583  0.87349    0.044875    0.828615  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df[score_df['threshold']== 0.51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_ids (InputLayer)           (None, 226)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sent_emb (InputLayer)           (None, 226, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 226)          0           sent_ids[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 226, 768)     0           sent_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "api_emb (Embedding)             (None, 226, 768)     20736       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 226, 768)     0           masking_1[0][0]                  \n",
      "                                                                 api_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "common_extract (GRU)            (None, 226, 192)     553536      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 226, 192)     768         common_extract[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "self_attention (SeqSelfAttentio (None, 226, 192)     12353       bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "out_rep (TimeDistributed)       (None, 226, 1)       193         self_attention[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 587,586\n",
      "Trainable params: 587,202\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best choice threashold now is: 0.5 0.8819430841984498 0.8658494218075338 0.043309107333497576 0.04946201141853316\n",
    "\n",
    "Best choice threashold now is: 0.52 0.8817946990116802 0.8657549261390528 0.04321672004598834 0.049306470502122675\n",
    "1.655026434602622\n",
    "***\n",
    "Best choice threashold now is: 0.482 0.885380311340092 0.8738796101288141 0.04041883998515306 0.04485750121942722\n",
    "2.088494634719019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_assetion(root_dir,ans_d,pad_num=0.49756694,max_pad=0.49756694,min_pad=0.49756693): #改treshold\n",
    "    '''\n",
    "    檢查profile順序是否與embedding順序一致\n",
    "    Input: root_dir\n",
    "    Output: length list\n",
    "    '''\n",
    "    dev_length = []\n",
    "    dev_path_li = []\n",
    "    fam_dir = next(os.walk(root_dir))[1]\n",
    "    for fam in tqdm(fam_dir):\n",
    "        tree_dir = next(os.walk(root_dir +fam))[1]\n",
    "        for tree in tree_dir:\n",
    "            in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "#             dev_path_li.extend(hl_list)\n",
    "            for hl_f in hl_list:\n",
    "                with open(hl_f,encoding='ISO 8859-1') as f: #X2\n",
    "                    lines = f.read()\n",
    "                lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "                lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "                lines = re.sub('=\\\\n','',lines)\n",
    "                lines = lines.splitlines()\n",
    "                dev_length.append(len(lines))\n",
    "                dev_path_li.append(hl_f)\n",
    "            \n",
    "    for ans,length in zip(ans_d,dev_length):\n",
    "        assert ans[:length][-1] != pad_num\n",
    "        assert ans[:length+1][-1] < max_pad\n",
    "        assert ans[:length+1][-1] > min_pad\n",
    "    return dev_path_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 225.99it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 223.95it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_path_li = chk_assetion('./data/tree-rep-profiles_o2o/DEV/',ans_v)\n",
    "test_path_li = chk_assetion('./data/tree-rep-profiles_o2o/TEST/',ans_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/tree-rep-profiles_o2o/TEST/18.domaiq_0.8/G395/44fe487808dd8fc0ff048d416be41a9414b66003d15cd6f0d75251f921c537e1_3416.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/18.domaiq_0.8/G395/88a899e31e4d89952f807b9ee2f69c3344e2d0137976b4988e5e95d2c7a95333_3236.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/18.domaiq_0.8/G394/413876356b48a780ee82c4452a511d81065e84fc69b748c0752eb3064781d5b8_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/18.domaiq_0.8/G394/0c1ce1ef6a41b78c59aa4a3096c8777c70ba06cfe102046fc6b7a9617a72cad3_3320.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/18.domaiq_0.8/G397/01519c4765be0d99fce9a9bb4aa338793ab4742294a63f16540c05b34f74c175_2936.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/18.domaiq_0.8/G397/6f3460e84dbbe9d2d1065f58b57e5ac048fe34584fe2a17b5909919013301cef_3404.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/85.ipamor_0.8/G102/6e19cb71a89604d4b65c48fdc70d7fb2d39c7e835eeb60f860f535ecdd3af813_3368.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/85.ipamor_0.8/G102/46f684bc93b1bfd80e1d13682c3b28c285540c7115f605237c50ca8216abf45a_3344.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/85.ipamor_0.8/G102/29c58c2851c5294a796126d3cdc97fb1fb224d6f25ac2840a58c6e3f3d68291f_3232.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/79.lydra_0.8/G55/37d4ae7e6231835a1cbb104dd59bb2cede47096dcd49f62ec32f3fa74c8692d4_3224.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/11.zusy_0.8/G594/d611555836428186023640070eb958f436ee67facc878ef521a2be24ee04980d_3424.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/11.zusy_0.8/G594/d3a92624825cd2cee7b4d4375d7e5bdb95b66346466fec64fe84e82758698efc_2936.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/11.zusy_0.8/G549/1e50cb641e5105916f6a5ed8c103e7ac13bd24b583678f0c8e7f6d53a043afc8_3316.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/11.zusy_0.8/G592/091aefa9e2f5feee1fdbdb76fc26656e6bfecd9ede121234f47c9644b6bd4411_2900.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1265/0c6f07c8e0e4110761b160951442e57daa1ec4b0a2bacdeac445d31616f1fd1b_3332.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1375/1da1768e6576913da52e13f48a693c9dde93b88024bbe088536f0366a54ade39_3168.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1375/0c34a2e5c3404da504c19f52be630a9947387aa4df3fe95247114fa969fa9730_3380.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1359/0039ae68915bbf2a381c59b572fa3fc7e799eb7c7d6e07adf26a0c82e03124eb_3312.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1359/15b23faf041b76c39bdfe6befc52008859e1d614512f6c47d792fa67476ed5bb_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1359/0ffe0ca00b5bde0709966cc3d2a3774b6c288289d5143917a497d7c61c08bb18_2908.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1386/0113c35b4d837420d68fdb3d5641ff4333a0b07cbacf925f83652e03fab13185_2956.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1386/8af51539717338a7421957f2c55ebf387e5b6f3b44f640e3d6059cb36f70d48c_3284.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1386/0f325023066cae9d8586a64c537e00cd7117da28954987d2a442d93335a3a965_2856.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1386/2cbfd034ce5dfa163df70e4d498d5ab1bbc51a4c9e70e228d9dab3dda0ed760b_2864.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1386/33e3ab3dba77b97587648c2261599093f9677d86c9000bba7ea423119793866f_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1386/6f20557c2bec1d9f9dc24adbd7f92738a838113d27221e91e3b8e59614a99030_3156.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1386/6e1aa9120bdeb5f8174bc070e553df58411ac24f0da859839f5edb2579dfbebc_3296.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1386/133a772781f9727e1e9dd8605e56c1336928d89b4440f0d91a30467bb504e374_3128.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1390/1f0120aaff93d99a9be1444b7a543a6237c48d2b2c67c2758e5250da293ced43_3184.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1390/0cf9efb73d8008ea05e90dfda95e0c5746b4a2df01e890a6feb1f3e8c14e6a0d_3332.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1390/0ac045358275526576ca43273060286fa8256643b2d8b94e129c906786ec4b48_3268.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1390/0b2c50cb726c07ec4315291153ec8a50f623b4e0a9e31a5a9e0f133ae4e2fe9f_3204.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1390/93ff0d3d6cfad51b838d77018c4a3f4a2520480ddd6256c4c6be410c5164edaa_3268.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1390/061bd3bb51c0712806bd1fbc9a3de1ae79dded33a4cebf1355425f42fed37324_3192.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1387/2b9cbb1bc526d5fa4b9be6a9d2ed593bd9916b3213cb0283b9a1f1cfcd9ce831_3332.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1387/7e85e765269fe81a37e9bee6c610de6b45488dbd00a75ddcd3631da423a3b5f4_3148.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1387/0e3de956d960aa537c4a8bd314451ef092e0d99c34533c607e4487e437d9500d_3268.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1365/0b8403f66129e9da9972e84592dc3c525d4a70a2f2cc3f19250783289e721e32_3268.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1379/0039ae68915bbf2a381c59b572fa3fc7e799eb7c7d6e07adf26a0c82e03124eb_3344.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1258/06cca7388adbef050cee144e440a9d1ff4493413987878299268158872807fa1_3220.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1258/48cb2cd2e1e4e782aae0b6e52d740d6d292864f37f5cfdd02da6b60f6da94627_3176.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1381/0caa0d952b442b5a57492969476bbe9e4fd5b5363f4ecb6e76a90e28cfb1b002_3272.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1370/0ebb708926607dc85bd949771d47cb9a2c5d73f2489e25c4df8da3b439b8b8df_3184.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/2.zbot_0.8/G1370/04ecad09dc2576b9afb7fea48c392085ee04225697c6b2b5458575fc4a565a7b_3180.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/26.autoit_0.8/G265/2ee5b23f5ca77f1e66ab14ddced8a305c0926ac1a47be93a491cfcb9a3699c68_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/26.autoit_0.8/G265/aff4202a5c2a73bf9ce9a0ee5d3c6500c8df10fa6f9beb5d8479748745ac30cb_3024.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G435/ac9f0586093a4fda38c7dca83f9efc388700462d9451bbf0ca79d181c06e89ef_3012.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G435/fc70c403c732348f5d953f376200eed1bc6043c920457238741b809700cdad58_3580.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G435/dbc66836833b47dc800bcf3f820c2ed64160e3c4f82d0bb1595c300969c99e1e_3584.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G429/02ab72ab1a987f264256cf1fd8399df101bf6703bfae9533c63a7c1c7abf7d44_3352.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G433/920f1a7281850c211854ce96d4f42ef4bfc8d3e2e37fe3fee125ac5db6be9878_2992.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G433/7fbbc00fb1503da75926543c6cbe01bf15eb9370113da95d7afb5be3cbc90cf1_3068.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G433/dbc66836833b47dc800bcf3f820c2ed64160e3c4f82d0bb1595c300969c99e1e_3424.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G436/84779f0a1639699bfbfc667b4f8ea25b22e8450d858e1864de17975d5fe8a177_3348.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G436/7c196c432432be3a495a24edfaacb5f638c033ede8c6b25e28811e39693d2bcc_3496.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G436/3b7abfa87f09c11de1c605b2a1d091d5913564d117014dffe714ae18a8248b05_3420.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G436/7fbbc00fb1503da75926543c6cbe01bf15eb9370113da95d7afb5be3cbc90cf1_3156.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G436/149917e290fd3e71a9a297463efcb1fd39c9299ab149be48e841f51ecdb1099e_3108.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G427/7c407183d74e8373a567da381b0d563682ebb1a2b9c0abd84c12bb1430dc2b69_3408.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G427/7fbbc00fb1503da75926543c6cbe01bf15eb9370113da95d7afb5be3cbc90cf1_3140.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/57.vilsel_0.8/G427/1be2c42df79d2cb82193b0882ee8397d51fa02535dae4861da66ef22ea916ae0_3244.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/25.browsefox_0.8/G329/cb8f178e6c6d9d149ce01b60b720be50a172528b437f1df162077be9e9bc6794_3024.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/25.browsefox_0.8/G330/9fa6ea0faf66678381af94aceae79abc11b628d9d91dcf99b050b615407ef022_3360.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/25.browsefox_0.8/G327/c2bbdf36d3a2b5751abe547ed4e5dbc6b078386000f66bbb401b60909a8fecbd_3368.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/25.browsefox_0.8/G252/80e87724cdfa90dac1119981744bb7af2d5658c717f154a76d4f10d1a9559740_3460.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/25.browsefox_0.8/G252/b92832cf79c4018fcd18cf5bbc0e548ecad486b43427b4ce1b07684bd3e52973_3360.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/20.ramnit_0.8/G522/49085d00015457094f77973f5e5af32490fad272fa1c231cf7dd25c50f0a2255_3208.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/20.ramnit_0.8/G522/1d1300f8de46ad757aa7fd71cd8e6298ff08ded7e53a259a690e5528335087a5_2904.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/17.sality_0.8/G368/2b50a852dc3e3a389b31c920785a2c73c9c614165c2bc578112675d9542e5d23_3284.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G481/0a54d0585a28930217516482124fbe1fc5c1da8f722685c964930134933b8ba4_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/4df5cddc533d67b32abdb3754424affdc957f6bef1a52219c7bc7e545a0acdaa_3268.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/be5b4fef6264c198f0388f628fdf7d64c0dc8d177f89fda39be25c429afe9508_3164.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/0362e3cae14d760cb1d89c2c4d8271091c9a9348462a7837849d4642db034a6b_3424.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/1e28ec956548f60a86f652cc1d079b327ee69360a959dd8dde04036de5339888_3292.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/49f2a247c8ff95fd866ed0ff31a8c8baa8a75d3b7a9bf929095a4a1f0306afb6_2916.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/7fbf32d77aeffbaa2d3afa106fac7a7e1554c9cd2be7b2778f546f5ad9a9391f_2912.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/40629faad958efb78d305eaf4eb17642d08817b2658a9e573768cb5541aae366_2932.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/ed9d48e8414809b75e7e2c0b3a04e3a4aea5715806293c9ea8d84f6c5755add5_3348.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/6c44968a8dbd932be80afe58cc3c9229a35750db4237da09c9176d166b292156_2932.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/5290fb1b4964e0d435e1ecd3b792854ab3e0b4e873d126b74eea9d2a9e348edf_3120.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/0a99ac0384866ec4da753b402f339dfe2ac941303b3162f8f46906f6447a3ded_3216.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/356ee2c79021e048e5d03fc4cf807cc7f193c46da2c8d6fc1346b6ea27673189_2936.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/acf761c9b70bef6c5496da1590d910f761ccdcaf803f5a6dc50b91de1fa94a0e_2904.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/9a1fa7a3b3cf30d390aca14d30ac0ad12d8cfb5cba2a04315c6a30e5f810fbfa_2924.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/16.sytro_0.8/G485/29b1dd2a93a9b866e6cb3bae45e495a7cf0322debc3b3a550cc03b36569e4786_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/55.medfos_0.8/G82/136f745bc3590b8a55720fd4e341e5d162a7a9b7cd5a1a96af7bfc5a16d5d41b_3152.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1443/40dce05c6a22c944cb49735dc0c0bdcc2c659e4a891907bec8ba7541fcef6d22_3228.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1443/00549bd99ed9d32121b1288017abc3d115e26ae6747da91d0316f62f61c8d868_3364.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1436/1d798e14c7dcd3d37d95dcdcaa09984a86312f8d366195e0ac9bc6914a2ef9e2_3320.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1435/79c11a1d8a13d073c6fe46b7f449eaddf2385b213a3cd472885d2e8226091d72_3304.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1435/1dd06caed89c10bd01a861648958ac4853b5fdea5927d4e9eba5c2e1536e60b2_3384.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1440/d9bda6a10c7e2ad4eb4f717fbcbac9077012d5029aac8d4286ad1757dec1767f_3284.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1440/fd8f581194d49d5b57c96c73ec0f06d3a793889f215c07d8a7f2db193e893dda_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1429/1dd06caed89c10bd01a861648958ac4853b5fdea5927d4e9eba5c2e1536e60b2_3124.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1429/b5e2b1df13d1f31159935ecf0fa8ec62047f2477aa56a1386d4b386955d2b9ef_3392.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1445/3b993db69881e2c2baf724745f0cf695ce3206a4c0e27935a36ff7c5a33a8c98_3392.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1445/54bcbd08dddf25d5dd0069ae0ce0f54f0de8ed08986f4af13de63e7923410aea_2988.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1445/07a649571239910580536e557528d1aaf895af1217bb11db8f291afe33564ea2_3360.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1445/2262a5987f390d3e4063a48387ce64ecb456fd91ce495d6bba179b9c4cfe1804_3300.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1445/0df069ce299bd59f9297726b95f38721530fcfaeea890b9718f571d59383a870_3352.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1445/387b44ca31c49585c3037dc2accc8ed1815818e03213c0c50ab5ce04bf6ba1b5_3428.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1445/54bcbd08dddf25d5dd0069ae0ce0f54f0de8ed08986f4af13de63e7923410aea_2964.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1445/3ddc8631e2302998d554dca18687dd1c506a5aaaf942ff3ee02950c2e8c36402_3360.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/6.virut_0.8/G1445/54bcbd08dddf25d5dd0069ae0ce0f54f0de8ed08986f4af13de63e7923410aea_2928.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G963/00b0b27ec913670d36ed0af6e49bee49fc5a2380cb25b5d16e40bad376ca0661_3376.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G963/0cde5dc4602f37a094c9c5b297a5257187f7725ad5a49b3755723d669d352298_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G963/0f93c4b02da08e36d0296b6c64d7b7637f84ae552af5401ad504cb6a82191963_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G963/01de911b27c41d6db77d06125cc6a05f46a7c6fbd8838307869ef16250af1c8b_3432.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1051/00c596e2f2d0eb5b1c3b335b41a75c9f918321aa6f60dea7fa03e34b11029320_3440.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1055/0a9e2a302387c504760123e2e5c7f5a64ed3a30bd36c16b87badf56a8e8debe8_3384.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1055/01abfa162629435d66e82764f1dc814bcc3f1c3109e6c4ddaeaefc99ab1d0fac_3472.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1055/0d0bf934fa97b4af7c28a51af107820b90df9ebc76b4129d55c948934cbedf5b_3088.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1055/0e2e19ef003791edf18eaada53a98a0840ef8b51a967ad5d0e4889427c8a1ef8_3080.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1055/00a55819e677482f04f6b0da9f5e967b651ca867f7700618c5be1d9f6972c4e6_3476.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1055/0d1c001671d208fa34f87fbb230b7ea4c83fb0e470619f16f8d03c7c2263a5f4_3384.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G787/0ec0232c7cdd830087e492a472260e408f35ccd514b2d12277c8d54538b8c0aa_3396.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G787/0a0ad2b3429ed19c9371870967c1d60cbc131c6ccf3666d3f6dac878c59c7f32_3128.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0a4d98e67fb31face048372e0872728c0072119320d61e55fe0d74624ac8d31c_3300.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0f5f6a4d09e14c56eb1f009a043cc93a22cac5a71813587543bf31df4572b601_2952.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0b1a5d938a77d015df32f931603242f308abe7ccf90d869a198c9927d09defa9_2976.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/00f8ba65fd5c0da87b7903ecfef9c5b198c7f760c37f86210cb2f1e59a5d7bb3_2912.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0d8d1f2c6273445ad47d17d1d80e8864aadd0433491dc4f7e1798d8a5fb16d54_3284.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0d0bf934fa97b4af7c28a51af107820b90df9ebc76b4129d55c948934cbedf5b_2888.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0a3a48027d171d1f334edb01d24ac484efaba385260f2689f6c5acbbcf8b5328_3068.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0c03ff6a446091592d604c191fa9dec264200dc561e62c1535a4f14e6a9d67fc_2928.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0ea4cc84e750a1bee8a99db098bb3411ffdbc31353df969041686095afe51baf_2876.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0f618b4508e779f1e2b75e2f68ac82029b69349a9bffe26874d08d9e16dbe9f0_3408.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0c9c0268aae9e83dc3084c896aabd094be35a7b072b66211c7bb4a59852be1ea_3360.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0e02b2bf13f6755480f82aa423784b000b93e825392fc9039aeca8495defc69c_3356.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/1d0bfd38cf5bea071ccb8d70f787d8fc05f5ac4db7de31c42dce632d7637c1bb_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/01d102b3df5822ee285f0e058d6c1b389584d7998c00bd8b3dfa21e7543f0155_3048.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/0bd16b6b36a595289f53d6a43fbf28702a5ffa5c03eddc59f7726bb5d64df12c_2932.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G1057/01d32590be599ee96c0fc1e23ab119a889aa25a9aefc5a80d4c760a76c098d14_3308.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G995/00e75f12290b8e4e9d309fac1af1e83e4cdd91baf4b58198b2fe80dc27a7617d_3368.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/5.ursu_0.8/G995/01ca2be200ce22b5f16e43ac16cada65f9f63e5223d1de94d7710f031defb3bb_3312.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/12.expiro_0.8/G646/06e4999f8aa8e245e2f107e7d94250ea8c251b3938a5eea0a7b89599acc08c25_3132.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/12.expiro_0.8/G646/424f6749117d89e6381e13f5360b2d009f873cd5a209b958b392d9f944109cc8_3440.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/12.expiro_0.8/G646/ad1f679b15e681175fede6a0979496489edc224e4d9f38c3ecdeb66640733f41_3460.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/12.expiro_0.8/G645/0be30a762d2a5123ad727be4f121193cc114672e803d7843e444126949cac748_3348.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/12.expiro_0.8/G641/b349b35c539359b08a89e0593bfb760fe7e0e5698a40f145948772024715cb7e_2924.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/12.expiro_0.8/G642/bda9f8a765f3038729060d6d198ca5452235553844a5c6f57992cb009d607ebc_3172.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/12.expiro_0.8/G642/e285ba15ea19b08ac961311f925bfc86a33ae3a487f878ee7adb032a65b4e6f3_3436.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/12.expiro_0.8/G643/1e9e1fc1217988fe14009a5b593b70b66064c2fd591a2e9dc5f9af18ec270816_2944.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/32.outbrowse_0.8/G262/7e652d79e0d6d809675d0fcf8e1fa3191e9485ef67d3217ca3e8e60cb7ce581e_2792.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/32.outbrowse_0.8/G262/0f82306c7601d914ae12f30c4bd875ea473f688db6dbbe63a6e52ad49478c99e_3292.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/32.outbrowse_0.8/G262/c54c965c034e7e5d1dcc687d3ec20e72d4bd19f87ee5909264e860539c98024f_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/32.outbrowse_0.8/G262/bdd092c5b422f1593df7249eb47b2b1105fbf379c51973c9d579df3cf3552de3_3328.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/32.outbrowse_0.8/G262/c6631ad12440b64e82b5740366bec4236b916af251d97cc6778eda926fb0f5dc_3248.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/32.outbrowse_0.8/G267/ebd962a3773bbdb610127d2cb8c187ffd25a0dd9298c8b31dbd65a8758385f9f_3340.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/32.outbrowse_0.8/G267/ebd962a3773bbdb610127d2cb8c187ffd25a0dd9298c8b31dbd65a8758385f9f_3376.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G450/c2fb0fab2e7545a248641b8da2f3d16331a774dcb8c72f8e45c4433ba882fc8d_3396.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G450/726c64a20a45d74974abe331f2a9bd9f08a8178ef2593cb19a8c47c13ca8647f_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G450/002f5b3b3d992cb16d0d519a2a2cf58d6e80c3ef532d39029fd2cd0fb43e4435_2920.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G459/00118f4416cea80a227fce1d2cc275f36b7854bad88f94f24e0d30ffebfd442a_3172.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G462/00cb401253958dd230f32faf87d239ef16c617432026b69257c042d847265c1d_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G462/0142a321a81943b83946666b452c5ea8762b4f147f66d560a3699a1c5b7253ef_3408.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G462/0f77a67c258789f440126d4ad045e6dc7fb719728ef3686efa4b94ebca38d509_3340.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G462/a45f07ad9ecb1e23c59337c34e14344e6f2a6863c8a75cea56b2c1bfd300843b_3188.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G462/530d8e4e5c6ae854a648030567c4f275c51d78b47b89a47bad75954ac9409154_3248.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/13.vobfus_0.8/G462/002b1ea364d98cb94d90af4bc17b169b022858631a4aca3d8297216cccf89372_2868.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/15.graftor_0.8/G447/4ac6f358c006f948ba8fd9a8bcc154c8b86b5e311d4fd1da5561cc6dc5431e95_2996.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/15.graftor_0.8/G447/e9a11a7bb962d3f619473456890c2d9645047a5c3b235777fe9221fcdb4ec647_3160.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/15.graftor_0.8/G447/73e38443e4626330a756cacad94ba01c6dab61c7da5c7b099432e02202f6abb7_3316.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1359/7e82ef85118dc6ad1b88384a807a8bf473a681d4de6df28c3a69679f9301332d_3332.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1359/1e5ce9d0180f8f92a7e9a04113a072e1b1b6c57f8a500b0ebea86d967e594884_2900.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1362/cac30cbf0db05a810ed349857f338a762487fbc761bc824d75ae00131eab775f_3396.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1362/105e73a7cef4246157a494bbf2ebea1ae7ed2d7ffc8477f16f381c2b30fa156c_3320.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1362/105e73a7cef4246157a494bbf2ebea1ae7ed2d7ffc8477f16f381c2b30fa156c_3312.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1362/105e73a7cef4246157a494bbf2ebea1ae7ed2d7ffc8477f16f381c2b30fa156c_3412.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1362/105e73a7cef4246157a494bbf2ebea1ae7ed2d7ffc8477f16f381c2b30fa156c_3428.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1362/792337ed19b78f714d7e26bff2e278ade8d54835696da5cdfda8d269a8fac52f_3244.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1353/0c96856bdd95dbcbd14f3b3b617757f03733594476d469a7ed1e9177549de668_2872.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1353/004ba08adb036eb7cf0acea22a3c8c85e7a02ceb659d44752a9abda8ae7b5133_2884.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1352/9a01c102b2bccea8779760170742685f868e0c47a2ee36c19ee0c015a9700ca2_3296.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1352/3b70f76b57bdff04d7ae50d1a5f2b301701017b8ecf076b156bf2a86e9efd131_3200.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1357/7fb0c75ed06a1443a1aa34e29e96e3d4de93c36beb12c5156378ec60c12a5b4b_3128.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1244/089e124156bc3989ce1032818202c22a36539d979f66bcd97a4402e81d9ca5d3_3252.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1244/0e95129a4ab15e20f785ab1c278457c51e051d70fda0de4d14b92a3b4ce04788_2944.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1354/9bb4e7da44699fb06696c4edbfecd49475fd7dcd6219ead833ceb0e20868c70a_3356.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1354/4d85e7dd4511fd4a1591d19c27126749170d04f5fc4bfb1980701bdfcf87363d_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1354/31aaef0a3c1313c96b909f6b2cff674040f49f221fd361300ec2e82864dfc04e_3236.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1354/9cc145f400d0bcbbe40d935bff1b4ac85d1f2d9aaf2e3ccb5f4f3604b11fd4a6_2864.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1316/7d8ed383a0d993f5c7ad62dca56e6b7c670a13d630f55bb5646f71ce1392c000_3252.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/4.kazy_0.8/G1316/6b732d6fcc139d3c7ea22676960d4b4d906fb16959c38cde9c744d5a61a69b43_3196.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G762/cf2ba3afe6539618d46554985fed33c270e4bfd6464acd294de73e488b4a250d_3364.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G762/0eaee9d63a27c7833d1326c34dcd9ef247d4c9087849fd8ea5a0de8fb2412967_2920.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G760/0f1d45d04fb0cb1eea4655ba76812078b354c81f1bebc953737e6ad7d3c05a3a_2960.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/11e00e6f9a885985fff6605dde2adf022a5131cda3a688b0b66fe6f228b51079_3264.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0b90f580818b4c1a115f01b62ffb2d28ba3bc69c337ad82cde2673e486c2be9e_3364.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0eb5eb456e817f81489e4192f7bbe83e57f4cf187b190259aed0bcc30c0f4a47_2916.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0b56cf4429d0c4d490482193fb98c67103b2cbbd357c97436aed08980447ae8c_3160.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0de6db9a17c2e90a3440a0c8382f256876bf2085db260a5c371f07aadb058215_3260.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0e529ddc9059f3e4dd8b61996b3a8ce1e3e3254798c19bd2cbe2b81d1facfc37_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0e83e001df283175cfdad0ad1cd7aea29618a1d5362906464a850503d1c86c2a_3204.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0f17481b2a1abea946c0288b772a2731e3f62079461a585f000dc6f1579a89ac_3408.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0e461b207194d5d24b047279b44ace3dd8b7c16b6903f2cd5248359a6e1b1fe9_3300.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/01c3782391599e959bd74fbebef09793bb715d69867618a804a34a990fe2b614_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0e84f09ddc25765552699c6603be3a0b5f38bb12247aeaea36e8dd5514f350cb_3424.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0b9609292dac4ea0b495a6f9208f1c1b9f47c05cef8a805c7118a2258716f347_2940.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0d4e559b6bd3604e85d46c95510fd2307a9720f1f59022763b8e953a4ddf7fb1_3308.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0eef563fdb61811db6366b3665ab0e0fa22d931ff345beb8fab43d500329d975_3408.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0c77aedd894ff8c927dbfbe458bb5fcb85a5ce675405830d21abf0385e193958_2864.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0fba94ff02481d397d4f4c1cc408e47732fb00aca9c2e9ea96c649a8b3d9c969_3300.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0acba8a0d2ce0109f0ef5dad0d637d0f80869e23ba71716b043dc136235a85f0_2904.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0b15fee1e5c0942fe701d0d92e766789a87c2631bf5343b2ef2480e243a2b81d_3312.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0e80ed684c4568bd44b2e11ef73c905ffd66659016d46cb80d9774bd2af8331d_2940.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/0e92e6f11464bbef2fdcbe515e2dbadcd5c55888490577c5bf647c754e1f9953_3408.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G756/016c5ce087bdcc4377dccdd8ded5f17312c1aa4b69b4b5f0cb65f844c9ee2385_2796.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G758/0fca3a6a2f062ff8c5bc7e835754dddcc3168672d22432f8550eb30f95813340_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/7.solimba_0.8/G758/149cdf486d1d316557631eb0be1285cbab3fc48996011c6d74e8ba539c4f93d9_3272.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/52.megasearch_0.8/G132/02f8aeffd978067fe6d3b05695d3c6ae0576fdeb4925b5b62bbdabda0a852218_3184.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/52.megasearch_0.8/G132/1f88915ff7227156d65bdeb4473ebef2811cfeb1d558deaa634af3f5177d83a2_3220.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/14.hotbar_0.8/G406/0d7bc78069c382caf6f3b1f6a8496363ff27b5e7f8de3b78959ec6c6d52d8a79_3352.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/14.hotbar_0.8/G406/0ed91e5af8e13c160812f76f42070d7baa99f501611d808b3e373be0ea524881_2992.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/14.hotbar_0.8/G377/0de6f46725f48d516a0738d63b33e170d4f1a72a497b1f0a7261b8e3690f4af3_3248.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/14.hotbar_0.8/G377/0f93765c1f49cbf5a205a9381ad0ebda1fe3c2912da3a317da8acf85108f4009_2972.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/14.hotbar_0.8/G407/14f4e4a2c2cef23102fa265011fdfb2d7809d97907004f1d981df84fcf00e586_2976.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/14.hotbar_0.8/G407/fc6121746707171a81cd8a6686478c4fd478d9f4c1f6ed763c942ea1d76f7746_3392.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/14.hotbar_0.8/G407/02aeb718ddbca28c2360d053fe5451fc857bbcad4485f9c61e5713039968c739_3176.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/14.hotbar_0.8/G407/0140da1cda16f050a78c51eac9038f061ca1d0e8946d233d6c092eceed2d5616_3208.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/45.bdmj_0.8/G117/ba74e4723315d9d4b0c7366e46c0a2760d27eda9c7dc1172203d2cea243a3e62_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/45.bdmj_0.8/G117/12b0658e6816188ac2094f817787204443db73392f01da74e6278e239cf0ca98_3164.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/45.bdmj_0.8/G117/fa85af138b9aad08ede5731b95f12174133fec07993ea7226718973e47ce3244_3284.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/45.bdmj_0.8/G117/802bf17140fe6ee4e233c603e53373d3f1ca718d52424b6003f5364c10dc854e_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/45.bdmj_0.8/G117/831203a74d8196dafd518a49e4a19705bd563d6847ab93185fcb805bf2a9e3de_3200.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/45.bdmj_0.8/G117/90d5a62d1a87e3950e672f7d5115dbe86eb8fcfc509f5a136bc832b641a6435f_3268.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/45.bdmj_0.8/G117/1f34ea89f3b8345c1f6929604b0b4751abd29a7187b108b0204706b62a432956_3304.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/29.parite_0.8/G299/e86fcc08eb8dec19fdac33f2b21e8a55bdf677c03d51fc438373187788c477e4_3496.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2169/1cdce53d0462d82498326d7f55ac17ff9399cb5c63c638159711dca37c9ff6d8_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2169/82ab2617c33766315187417c73aff95b6ebfcd7c701d46c562ea5542bd07b76b_3232.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G1842/6cb7f9496a2a0be1c3ac3d52e734cc0f2cabbc01785c82508cdc387bea0c17bb_2876.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2368/1eefa6cd4e20164ba82e1f29ee574d89fcdb15a6acd46434b2d04f8cf2fc0b59_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2368/1f91b973074b22d04199a3f2005e73160d0292b97654106bec7d2092cf607016_3300.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/65c3f64bd4495196c0155f36c5eade0e7d32edc7980ccb7cc2d6e1f0e674abc8_2888.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/2aed48b338702926d268b093b6e0dcb1c7d4a435a1b692a1242b19dee16ad890_3168.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/30d561c1797e2a6b582885f5c4c7d58d1146d6b22e02af2740b5f3c8ae01b48c_3228.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/150cabadb72b22e761543466e18a7907bcbe2b03716b9155a126d96edc5212df_2832.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/9a19b95bf4a7f867931edc2e77f957d4763ebee32cdcfa8f77bc2ae63cd3c09a_3244.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/9cf4b82bc5b536c5e2d605ab5cbe6889333b699ebeedb8539af04653d64ca752_3064.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/9bb35c1830837cf0c8580294cbf009b31998caf2add4e7328554520ce472aa5b_3132.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/39de5f0e64e3e25c7fe00750fa4809ad67e53c2c38617b86ed227fd55da955f3_3260.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/97af6bb33e3a7034d6a317dd9dfe7cd6bc73b4516ff9f80f5a64de1f53ca8687_3348.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/9e8d27bce9a6101558ff283a6f4495402ae73262fc686a3ec89394c83aee5190_2880.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/2b5940e9bc576274ac49adcf153a04328b21d3a6db10d0c0156025f380d0a722_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/79a920066b83a574f69c0764fe50f3ae7c5e15f9fb20238767e1363b9e5e02f0_3300.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/24ef9a514a8822f5d02f4444cb0c7dc7d5a4e207f0526061a6476eb46ff2718a_2868.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2363/6fcd8d67cbea3fca2796a62df0788163ce5daedfda8271cbd350cc17d7cf0aa4_2888.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2321/1c0c57512230cc14ed2668bbd6b9d5a2dc3bcf122abdb2860e8a6a00938cf09e_3244.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2321/0d4d63849cf5d4820e8bc0fd58c30514a0b86a967c1a9e00be17ac5c9b28688d_3424.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2321/0f882533bfae99e15d261b9afc6c2519b78be203ec01bfacb54fa98feb3d3b61_3512.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2321/115f1b00c9f4eb74a5f1befce0f40d2cd067c05ff6375878f646657480de02a9_2940.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/32f66d200f6086f33feff2c15724b95be726fcaf77f301f5aaef8bb54ae71422_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/34d563e3d40619839721e959b5c3c33445f78f92d7202117ad9b91d13eb5ed01_3272.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/71e2916e83a2779ad7d0ff651321ae82d2d0f890c9b12888583f36ebf98a3936_3188.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/6f36663b4b4cdac13377869c231339ff0c1cc4cc6a4a76e7a14656b86dea058f_3208.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/27a5bf838768e5f945c5419e49edb6f436a3ce9f370eef30b6ee58ddcf1f9e4b_3184.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/7faa53856400c87bfb06738f2424e58d659b84c69c288049677aa9e61e4b77af_3260.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/42f23b328074126184853a63b56fbc616979cca5238528b3f87fb67d5deaf6f6_2836.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/7a0d885809b7f22427cb5b241e179f0849bb9f2359b8bb752be959e4199b78d4_3384.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/3be39539b1b8b9511946ff76fe466660eac1bc405d069ac33ec6de26f90fbab4_3136.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/8d402b94acd1710319937464e62c277bf70f428621aebb888e471ea072a0f6d6_3208.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/91c7b7e9d14f25ff0694ddf2ae3c3ee3a827e71e0e5755560b47ffcee326dab5_3172.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/41d5d9e8d4569d718b2061eff1ee8e96d2af4b1d508ce34e35a044a44163cf88_3248.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/0df18a3ff9743539cd39452bfee184e69ebc4598331f38f32a862775e3b016d3_3360.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/5e2009a617435d487e92fc593169f8f4feddfc985e835c41c98cd117a811dd04_3144.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/1ede1debd9792d664df61ce172092a595c3d73bb82d069130ee4dedd22ebcde8_3188.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/2b298164a2f9edda63c2a098643661e0e6eb78cd5386f0fa82765b8af9ad86fe_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/6c8c02ee821445abe3e66d6de71e05c56f790f0b1787fe3b4c6b4b5dbd8cf959_3208.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/4fd3a139aacdf035d360d7c7a3c872c36e73f3627cf7f656aa48a38e574e5dac_3320.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/97e1901bb7cef8e1478480857802437b83af3b54705855a99ad475f4337726d3_3352.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/66e3c740a31af6fd5eb5d4749d27fc1f072e2bca52baad9a215d24cba6a8a9f5_3192.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/39b9130296448cd39f5b95e354b48fc39a708c34ee88a7d55ffb2166baf68e8d_3232.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/6a702df7a0f8da1e0fb66f1a3901633eb077cbf4a5a007f3ccd5a62408640fc9_3200.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/8cd1576ce6bc1ea42df25daa4df9b0a38f10427f589aa93676ea7b31d3f4b449_3324.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/8aad9264b2dfd15f7158c515d429ca3546bd6d43b76c34ef930efa7b3ed0ded7_2900.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/41dd127ef6b9174dae1c6d4e22076970f24309aad90df57432ac4119435cae4e_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/2c97f34f9f2c5043a2fb9940566f16e717b5596dfbe2e3266e47022fef356259_3244.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/3e35c8a6bd72527988f52390eb50a58eaa50c8f0aea5700894df966e735f08be_3148.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/8f6bcd3e5277e891f5e0d7cbeae61ca0e218e0120f238cd3344d2376e4b298a2_3180.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/2c9e08609e3e8ac82478e0b64055f97df479a23aebfcddea1bf8cb725fb00469_3336.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/9d8b87063e2bdce9cc8c67b2eeb4afdc161ed1691f6ac476f4b62fdd256cfe77_3232.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/8bfc988826ada39dded311d560e240967177d130bbacb6d28c461a157c511c95_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/1f5a103076de40cb6b462f406339b2e8bcb26f0ae8d038ad55ee15e682c32e31_2940.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/1af2687720617c563f1dd97d52df5be576dc5d26bf2b96673420d6bc5824e407_3216.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2370/1dd304879c4da40b83a23df44726fd280b3cb8e125b58b293f6051ee73b724c3_3208.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2351/075cca98cf950d1e84106445674e0f9af5ea53fc61c53508e8eda9447b7b7695_3328.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2351/94c215bd43688ef04f9129331dc6ba5cfaf6b8929b166d47bb0ebf2b662b1dcf_2832.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2351/151db4a68203268c79bcc4874ff511fa4825add817fc2b791dc30bbd27faa2ac_2948.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2351/5d450766986f8760b2580ec74a69f4484efd8f6711dbdabf2d4fb14185b26d33_3136.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2349/0b7205b3c948334bbdb5ea566f494b77bea19ad8c2b261fb61cb82c2cb7f3716_3328.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2349/0c1d5f088bc90490ae794f5401b156537c313482dc4f852a8bd1ab0255a094c8_3004.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2348/13b10a3f8bf6a3f14f7950211a233f50931c1cca8088ab6e1ee8be42933cb762_2888.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2348/02fa08225b649e02c6f94c40a78f49a0231293af9918552f1186dcd6399a94f9_2896.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2277/2e5bb51804a59fc54a9da8162259a9f07b570eb3e57f381e8d21564faea8733c_3196.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2277/53e1b92b36ef322e561fd3b180a9d3caeab62563676c17362ab37b24d4c875e5_3252.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2356/3f5e9244be78cf90131de6b54b3fd74d06d30e62dd2d635478da3acb9020186c_3320.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2356/6d76e9d478518947acbcd92a7003b81679e93e1afe699730deb457aaafa67313_3304.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2356/97c02ce3cccfab336f39957d5707af008b59b1c8ae983702458d17d364030701_3192.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2356/92efad6b503879d45151791b9ecb4f0b8240b62c0f0a7f00aebc4986e74dda3a_3172.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2356/93a09cfb9668d95e809b8ca1ac36d7bb15ef1ab057b830af514c9100d7a36d48_3324.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2342/0c3e11d690e9fa621dd0a9a9b3efaf6df5f27d3b3a73355ff4966f418c67e8a4_3196.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2342/00a0d8c3adc67e930fd89331e4e41cfe2a7128072d5d3ca0ec369da5b7847a45_3364.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G1778/16d7339de45b41e8a9ccc11269ccb04488bd1b5ec7aca7c1d286854fed3ab3e9_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G1778/68b958c76b257398e42764ca10dca895cbabde74c78550abd10ff9f8ce66383c_3136.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2350/8aaaccba6a8b35f516421c08ef7720ba234de612c507633c12cd9dd480ca5497_3344.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2350/31eb20b82e8677002b8433dd3dfc272d15f563352d9567c3fdf474c0a7c83402_3144.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2265/77eb9e4f1ca59d3b0c98992338404191ee08fc3e4f5246c4767c20f9a740447b_3304.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2265/01c523806af6f32b6e6b4ca59349d3e2d6f0c217719cb051e9c0b58d0af1fbec_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2333/6d74cbdf936e7532d7008f2d628675dc24f81a233c3b9959fc26529a6362ed81_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2333/75ea8dff56ca9dc569d616a23e598776665e6c66d8f602611ae8852aa9290aa8_3392.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2333/59f42cdb8d46b308db51482d36cf9616afe9d7f7042403f6fb9b9cd40742de5a_3272.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2333/26afb06531c7eb45267b5d32a2ebc8e201abf0641b54d435f8f7281ea283fbcb_3192.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2333/37d40c0bef4d8f4bf771a282fd5f267b2fae31f745a96c3fcad11a51046266b9_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2333/3f5f8937912113d1e61ae9e1d6b27624bc5a9e1ab86a4b48fa935e646600c84a_2784.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2333/3ca0a0523615157c08656d9b5fc64b7fdfcc9552039cc52638b5074c1df6eb29_2804.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2333/6ee3b41480eb4c65cbb04cc89096a0744ee53ddbe94b68ef5ee5d8e4ee7431ed_3204.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2315/01bd0ebe0f7649b4e9e300d60315c2e2ea5f3efa5a38e5c565d66d053bfea0fb_3332.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2122/2eb34b8ebb239b86e7672aae769e55959dd7a1731fe4b8594e7be360e379c248_3168.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2122/5b209226419a66d1b102f0534b73e16abd7bcaecd4f75de8443af04f5f3b6272_3220.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/4ae1ad6e1ee6c7336526b0cf5175ebc7c113c489f1b547122cce8c5361fc2271_3228.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/8a2303acb00761baa9f785dc7d5541b5236571360a576854d3f9a042bba7b994_3344.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/68e1caaabaf2ad8234ea02af19951e7f2b4481c16c4e0ddfa6c485f1d90ce983_2876.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/6c54dc0df12ad80a50e04b9c9d23dce88c3057b00c6edf387469949634a9d8bf_3176.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/64df45767fbc27f0eae060c95e12ff54a8adf66824d981619a22cca7d591a31a_3272.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/1af65e47992b406fb9167221301abe0aa8d65528abbc10380f1059b0a1487111_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/3fad13e624176c0fda91806aa970d897b28ced37d0edd576e6d8b16f3e69d8aa_2916.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/0e70816b45171654f0d3adaaaa300f744ccd2cced1a581ab0dee121391852ea8_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/0079ec1be1596ab0beac984488e2c3b37b84bbab98e7d69518e1f9ff7221b1cf_3200.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/4f52dfc3556912cfd5ad0c377c4edc998e0c2773518886465c4b40a8d26d945a_3224.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/3a988bbbea90d529c812a035b9fec021b6f3f8164e6b7b5690adf0e5676bcec8_2824.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/5a7400ea9e8fd51519123c5c47f2b9db94ebb62546c8a5054d269e5024066e57_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/28b4a0b6916bf0918e64ca9b7732d211653f1e88230fbc72669e01d7ae3cca5f_3268.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2365/47cec463cf73ccb122f6b24ebc10699db8fad7e9df999ec597d9284692b0a78b_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2332/1b38c57ca1901d60b33008ab5c9be6ec1f6a908f5b978c8c66b2ef7daf74e63b_3304.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2332/1e82db1c5f0f3a60f43c3017b37e7f0e8670d8880e4686d94f54425729ed415d_3256.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2323/5e0d8191c78206a360e68fe3aa63fa9a39c1356ba600a9bf9320a0c381a25bcd_3336.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2323/4f14cbcd8bdd675df5b1dbc051c36c9243c533bee1dc11fa691ad460e56d5251_3344.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/1.fakeav_0.8/G2323/4da298a9f2bb9758245c0abe40ae0515190ae74c6bffa7a08a6eb16e87f080ce_3360.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/37.somoto_0.8/G224/4d1256b131839c219261a1afc1aee4e231e1a7eb40f9708fc1b05ce647c69ef7_3320.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/37.somoto_0.8/G224/085ada04e78fda592ecf949dd7d327e81fc1d4e803cdbd9c7165a6cb10d6125d_2852.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/37.somoto_0.8/G224/900f966a9887266b0804b24b2381696cb6de4357406da24098389b574acfe53b_3248.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/37.somoto_0.8/G224/d140884a8e7e5291553f7b50157411571e30db86fb0325c71ec942522f4495d0_3372.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/37.somoto_0.8/G224/98889421f5309230977c4ba56867c87a44d3e09efeaf650665537bcd28a170a9_2980.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/37.somoto_0.8/G220/148d257413f08da0a36e8932212d6e72f9e82682f5bcf13f17ad0c1ccf4d8f74_2900.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G539/6a6ddc2e37f92cdfc31032c53a38fb09c1065e7d37996004f5df6813786bda48_276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G556/0a5faf5624a993bd716ca6d7a9b2750e8b06af4965d49fc8a514a8da06c2a01e_3412.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G556/0a5faf5624a993bd716ca6d7a9b2750e8b06af4965d49fc8a514a8da06c2a01e_3396.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G558/5bbf0c0b84262c6d4a43e81df668cfeb7be006efc49e5b61dd6357efd78f0175_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G558/1c0977d5b72a161cb72ca8fe79bf70f7d8e6ef0395a706966420f7b4bb545b0e_3404.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G548/8e371f4b483c18de07928b0f02b541409a789b46e10c3ffb6fccbd4a16303307_3244.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G548/60da51594e3093ca8180fcda75200413cb24c7153d4b39e3103bfffc8da86e52_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G564/61c7cfde98b46c168609d32f87cee590ae0947752dde7d0ce7e10893317c119e_3312.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G564/5e9fa7883bc48ec9919687afcfa188051566e490c57f6479ba031336fe6c6334_2900.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G564/2aa23bc6e8eb6a268bd2f05c15c829574fc51f5160b737e2cf5523e0e17332c5_3292.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G564/7dc4af86f947f7a5468e142c4f76972563d39ad2b513186075318e66f80140b6_3164.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G564/3ca35f616967874289cff2edcc0b98cce422f965542fbd064459fcf81b0cf5c3_3156.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G564/52a38dc297bc7f1638bfa65fa740c5eae8a8c1ce6e8b2a747060e62feda38080_2896.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G563/29aa49933c669d0d692fb77752c6d7eda5595e310f8c449296e7924460a96460_3264.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G563/6def4cbe326a357616137ff280d6687491579c63a4944a6f4b861c54e48749f7_3256.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G563/35c6c044527e400bdf989c2b5efae25ca497ec0bf461bc9602f751d6f6527dfc_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G563/7bf6b9c1e5e87090fe799bb745c43b44c42ffd3e1106b30971a80bb59f35c41f_3308.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/8.fakealert_0.8/G563/71a7ba97962a840c9a41414d977035182ec4cc28f67ce819e011556c434ecb38_3164.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G194/00ae919ad896ed6e1d75a373b96bc7784f2c4c99e7172fe3cbdbd03e6a92fce5_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G194/0f37dd76c8f0cf5a412594ad9872c83d1a7c750d2bf5b04edbace4a19935532a_3272.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G196/95de2d82be72aea9e9731bcb88a6a36da47ae2cd9ef28faa94035e219c507d59_3412.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G196/09fb0f76bed0746aea6f310d3d99b2f9b283be34286d2c5425c4adaaa2a86df6_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G196/00b1f9b2d3effc7dc485b2dbe05c2f4b9c14384243bbaec7d85d6963494c9e30_3044.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G196/09fb0f76bed0746aea6f310d3d99b2f9b283be34286d2c5425c4adaaa2a86df6_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G196/0f37dd76c8f0cf5a412594ad9872c83d1a7c750d2bf5b04edbace4a19935532a_3336.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G196/95de2d82be72aea9e9731bcb88a6a36da47ae2cd9ef28faa94035e219c507d59_3420.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G196/060f80294f0901c21617b11f12a02410e78007881cc57151d966c6b1ec2d1186_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G196/060f80294f0901c21617b11f12a02410e78007881cc57151d966c6b1ec2d1186_3300.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/62.directdow_0.8/G195/0e47c5b852c4711bff6f5612ff836b6fc33af1ebd168acc425ed0ec2f5a51f3d_3348.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/39.sirefef_0.8/G120/1d6c6c6b7cd4fb4c6cb72ca4be489075d9776520d415df80688117987e6b1e25_3268.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/50.shodi_0.8/G126/1e350076f9be17c98f9528371323bac9c5e11ba6f682c4bd79a2f24def3713b4_3436.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/24.softpulse_0.8/G339/63487d01a728983325ab305942fe9704f2a56b28e8b9fd072e884602ba83549f_3400.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/24.softpulse_0.8/G339/0eae293b3cb1cb00e045d2b126e55bee2bb01083ccc99b7bf41ea97e4937b8e8_2948.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/312.katusha_0.8/G18/134afa6b0904b4f308ad6841b55e3c4832fd9008f08632e2126913bec6179eae_3448.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/96.pcclient_0.8/G77/c85cd1685bb330658a777d800877309d6ea29bba5e065adc15f8ef8c9e925d19_3532.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/96.pcclient_0.8/G77/b633db47b6a9adf6e7258da849df9828f5d4745024f602bf10f2a9bd74dacfe0_3508.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/58.picsys_0.8/G118/d76b44d069afe54ec939759e9a93e3c9870b61f47d14b0c28b28ff2d3ed6fd56_3316.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/58.picsys_0.8/G118/bb4c179f613a56cdc4bcfd82410557e4704dd923b87061e127f05b65d9dc4445_3372.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/58.picsys_0.8/G118/31917e7ce31d8192a062498ede37e10f20d5e2c1eb953e99cac2498a3e7b9b31_3248.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/58.picsys_0.8/G115/dacdd4fde79cb743851d6dc3ed09e1419320ff835a3889af2f7d866765d09f65_3284.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/54.fesber_0.8/G373/031aaf958933aacb1b7a6553343071b8698157caf875298f35ff02e9dc1bd2e8_2992.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/54.fesber_0.8/G373/09c89f1c4f2014d05a398690f31b9254a7d760ce3a7da7571a15c0b3b5e3af29_3392.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/54.fesber_0.8/G373/15bfa3e6d4d491a721ba4ed3da5b07c46fcd622a04d68d7dceb6626303cce06d_3292.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/54.fesber_0.8/G373/0cad4743f3d814f23fe49f85c1025e29a19f6847caf522cdc58c727f4a56432c_3412.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/54.fesber_0.8/G368/0cad4743f3d814f23fe49f85c1025e29a19f6847caf522cdc58c727f4a56432c_3576.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/54.fesber_0.8/G368/0eb8278958723f4583d4bc94394ee6cdf95e86b2ea5570621c830352cc323b03_3436.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/54.fesber_0.8/G368/1dadd49642520ba1194b0e05b10fcc18d62e09a507825eafb48ffc07c9e2704c_2924.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/22.razy_0.8/G281/577faeacc02c9e22cf4978b6d5da7115d4e527547b325615bb48884e35ee8359_3348.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/22.razy_0.8/G275/5fea0e52ad95604e3fe386ce6b451c2d81cd1fc26599b70de7c96013104f77f6_3356.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/22.razy_0.8/G230/0f824a35161471f4504b69bb16e50342cc34b347040bf3973ba802701add1edc_3248.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/73.yantai_0.8/G48/56a0fd1188c5bcfee39f0823c5ac738daef0f9f05220abcf4aa7920ee3f85830_2976.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/73.yantai_0.8/G48/f63c23c73a5884bb17805954e1f08f067c70c92df38ecd646e1fe7ea4affa4f7_3332.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/73.yantai_0.8/G48/c626eb2c605b389cd3fd5593f23067c5e846dccacb8d244d36869ee34f786095_3008.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/10.symmi_0.8/G668/be1d2a5b258fb077d4719e4c92c316802219c95b641c282917332a2c5bf509f9_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/10.symmi_0.8/G668/be1d2a5b258fb077d4719e4c92c316802219c95b641c282917332a2c5bf509f9_3248.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/10.symmi_0.8/G668/105ccd3a59bc024a09548fa403d60c2b4c0324074a66ff8c75ec083b5e0a8156_3324.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/10.symmi_0.8/G663/025ea137777a97561b0811ec30b779aca5766ea3ca5ce81dc0539063234a5550_3300.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/10.symmi_0.8/G669/9b734ab67b5273672bf6c4fe12cf9213c78ed3b0b346bbfb286d9fb93fac8a21_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/10.symmi_0.8/G669/1c18737af8b20b5bc9fdde688b920aed7e7befa78d5f7a46e86427d0107d7937_3292.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/33.mira_0.8/G227/182f58f00905868e0b15428576bce245b4a022de88ee00974f38f239722e4f61_3428.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/33.mira_0.8/G227/e6365f5887730762e6d74e9090f648e2d24a7da253a9ba015e19ae270c0474dc_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/33.mira_0.8/G227/dc3eaeeefab25b81ee27c9c6d44010401258aa970e4149aab471c385116ab73f_3348.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/33.mira_0.8/G227/09298f70aea6795d520006dca7d8d43f510ca57bf1ce4383428f06c652679874_3312.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/49.eggnog_0.8/G116/cfc485f23075a93120052b044e6d68d425cc1ce96088df4bdde1e7b3dbfd44dd_3328.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/49.eggnog_0.8/G116/24d2556518285ff6e8ba39f1e8ea448c940575917f003040aa31f3e18ad8123a_3264.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/49.eggnog_0.8/G116/e64f4019c5d594f4b15b71e369f12343771ef257d2707cadbdc98922b366fd24_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/49.eggnog_0.8/G116/c9955ccdec6faf4cf1d70173b3e2a595cb3846b30605796e5dda1b8d706db862_3040.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/23.mplug_0.8/G359/13a56667267b9ba75df59828f63e84c8939b35a0b788b989ddb9e871450f90fc_3236.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/02fbdfec70fb86535110d36b9650a2440d9e961ce646ce809bab9ec43cadb6e8_3208.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/0eda398f75eb060890bf4e8716119bcb253530fb986f82350d80f17409719aab_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/8f7e3256c23c4328d0c9696a68325ded5792c00c1e62ccc4fcbc3618e8d44f55_3224.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/304e2fb4e2356b1ebca99aa0eaf316c0baeae0d51e953632e48dc73b6d347e4d_2912.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/a5d35e3fe8ab949ed3d315683f60f8d6a9fe455099b265466afc7523fe277429_3488.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/06fef13f2269fea57cae228dec1499c58d00a4424b4a8cbc30fad69a6e9052cb_3152.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/4829e44817639a84ef7ade2cbf271828dde6bd89b9ec8c0da10c69ed362a5510_3360.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/8a1aa5eac1a1d4ff05a9890f29f1a50f434d0eaf6925b2274cd83a7691f7ec9d_3044.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/48ecca57fe0b4c7b02507b73b146bc9d818076fcc23109289017f2dd9c9a01d5_2928.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/df2ed7a1586129f5278449407bb84421c2903818ca7c24ef557038040b5a8a29_2932.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/3609daa5f5b6c6b5c7ccc14b5bf1db75a49ec983a38fe6e503b64ad31996cf9c_3328.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/3bda35ac3847a92c47879e9de547824c66ecbf9145304aed4337c8e4327c886d_3356.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/1f19ff5d42f11c149814edddc2f873c5f2841e10b60770af554a7839e8859950_2952.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/1e1c31594fb5adfe2eb31a0d23b7cdcd29ab217cd085a3c2015d3d036b4f51b3_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/5b9343b73bc29b2f83503c3d5e75456e2d16b19f12cba91773c77d152fe3c6cf_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/8c10cfe0582ab3d203ef6c18720470fe7eb3168ed5a269999a34128bc2a1ae37_3336.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/b52de376606f8b670acc2c46e64466dbb3db87b1c196fbffa16042fd98e10d8f_3480.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/d20cd04755483c3714fd3809b4cc3adcb5c8fdf4bd890d39959f54345bb5dbe2_3188.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/9ce8f078cb0ee86220dd525a00a80b62b354448eeacecfdd8d11b911a3ebb02c_3316.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/9f5fde2b87d7bb8416c8eb54695cfcb089746d7880a866e07c7dad6bc2601ed6_3424.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/8f7e3256c23c4328d0c9696a68325ded5792c00c1e62ccc4fcbc3618e8d44f55_3444.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/04300bd726df9fedb6e198960d27e6228f9aa8ccca3b279454740fe414c7db63_3356.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/db396f6e72d1ff5c63440f2556629cb9bae814af5f29c651538faf956be686fe_3364.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/064e8ce5066b7fe820c7aefed0707a061c24e065e59e0f478a8b34abc03af279_3320.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/304e2fb4e2356b1ebca99aa0eaf316c0baeae0d51e953632e48dc73b6d347e4d_2920.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/07a084ded7702e59b029181f1a2572baec19b2c230d5ddda2af169ec03c2cbf0_3208.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/c60ead779db386873fb1df770f34eb28393e006997e7ee581586acf3e5a8b54d_2964.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/031ad2f194a97af218069491b2fe692c929663aa7d733f28f4c351f10e1febf6_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/329d35de1e16f1060805b04bc4ee3a3a14c5474d754cd13ad82692a202f53cb9_3344.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/0b783973e9fc6a3003384dd7df5cd9cd30e7e22713a0ea74490436f47c55ca79_3188.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/3bda35ac3847a92c47879e9de547824c66ecbf9145304aed4337c8e4327c886d_3372.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/0d7d57befedc779881303971f662f119915d4caedd3ff5efefc85789657bfbbf_3376.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/0bd9422a33705e3c6eee7a016bff6a289dbec41c4151df95a21145b6d3599b43_3384.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/026f982619d2883505c147b27dad939737e255216160e9deef5ecdaaf806814d_3236.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/026f982619d2883505c147b27dad939737e255216160e9deef5ecdaaf806814d_3244.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/08d0994b0ffc2396a73da85cb96ff61d1672cc4e970c813b8d3495c6bc0b2a1a_3152.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/533f77aa22894466649f2d75e474d5c40ae4d8ec6843a7e5ae2760338d283f9a_2928.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/06d0248d92e23451624e625fb48382e70381aa77ee9d8edf58cdac8972a8ab64_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/0023eff74c703017035903d6e70b4c63e3ffa85658cc3340c47d1196f2ada930_3168.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/4829e44817639a84ef7ade2cbf271828dde6bd89b9ec8c0da10c69ed362a5510_3308.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/b52de376606f8b670acc2c46e64466dbb3db87b1c196fbffa16042fd98e10d8f_3460.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/0efe627d012f6918fd166e1e6c1ef205e99c321bf5a732ab2e92ff2952a0f232_3396.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/118f31b5cb6631f1df01ec255f701c5de053fe4cb9ae01ec35926eb0f322f8e4_3272.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/0b783973e9fc6a3003384dd7df5cd9cd30e7e22713a0ea74490436f47c55ca79_3244.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/047252fbd97882697f6708cee1642d71dafcf9779128d902dcbd77dcd3adb9af_3272.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/37a4d750bbb394656e7e78190bb9d5c1f255c22c5f8f72e8d02cd0a5458ba85c_3440.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/a513ae3e0cc668fb2799bd7bae2135015c88cebb7f7c573a6f10625882a5442b_3432.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/0e03657c1976b347dffc42a8bd5b4e18a20985e6c47cef75d87f80356f168598_3304.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/047252fbd97882697f6708cee1642d71dafcf9779128d902dcbd77dcd3adb9af_3224.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/7d25b23bdd47d67dc065473b1f266186be2a86dd5bcc9ec950926336b31b80e7_3288.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/02fbdfec70fb86535110d36b9650a2440d9e961ce646ce809bab9ec43cadb6e8_3172.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/0023eff74c703017035903d6e70b4c63e3ffa85658cc3340c47d1196f2ada930_3192.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2100/02e9d72110ebaa396ce8d3e8d5fef162a3bf764e873ad859579f0f69c0be05c4_3328.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2087/9f3d9f6675868614f7bc247b86df70e0174542026b638b2c35e4eafd97c95a0d_2948.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2087/b574a57460b9e9370f3b7e3f55d13434c18434350370f053da0d7ad7530058a3_3316.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2082/f2bb18122adf4271ddbddc545ba7ef7f4b70bf18f58815171f22ed4b790dc6ce_3376.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/dc919f43e6c69c797d3d07c02a3a7e7e8cfa94c40e12b90180798317aebd712b_3284.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/d7460eae0151c73f554b9290514466e4ee95861aaf004b73579dace6740a5586_3164.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/5385950dd8849946576d870b35a620c5a68be427f0b5675ed92db3dfa5a272a3_3260.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/9683d6ed32b5ae9263f3858c61d01d5841ef3cc41d3abb934c4fd66198d48357_3320.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/b04e89ef1b7cb360f38b49350ac61f5140ad802c06b28e21c9771efeb5c08b44_3320.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/26398fab8afac47827c31f7b45220e8600ff529ca1abd893d58c31d627fc414a_2880.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/0da1fbaa4fc68426449d6ecc77890923dacf478252be66f37b32978c74f1f18d_3240.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/35bfd52b9239d3bcc8055e135192704c8a717ca37b3f757b9c6b1832f8d21376_3248.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/35bfd52b9239d3bcc8055e135192704c8a717ca37b3f757b9c6b1832f8d21376_3328.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/24a3ce08e4a7a08fe70005f3e4c4bb0770eadea8642b58f2b000911b2931fac5_3232.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/a70c1f66c37b0aa1f68a6bc7502b10a56a16a5e8ee01c41128a525891f166d1f_3336.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/24fa1f4d3e162cdbec03a1450139bf60ce53e847200acbdd61934deeeecae424_3436.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/7dcd8435b37b7401f5e244a35f9fb6e042e0960d257d89c16f9957c730d00c42_3228.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/363ec6373d3660e0520dd0cd46cf70f791b78420ca10b28e373dd3d1af1e0683_3252.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/b4ce7014a0e6a588a87d4625e92a91573156fbf00b7fae305e0a2e1775145a27_3264.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/0ed028288ea49b43c68bc016b0fbe7397c0c7721b9c1359109061231a31e6bf1_3352.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/ee5136d74f69489d699ad1f500d76afac03ce028c83a632ebd9e785061dd9f38_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/63c990e924d8ff3f684eedf79b086662eaf648e3da41280066e9b94fba0573af_3072.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/a471ffe973275bb9f336becd11002da18a474616cf8dea5888ecf87cb5be106c_3436.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/64d91fd6ade228451a159b52333e7be0db6462446204d229c45fd82ecc16b00c_3304.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/bdf0021c232b935ce78840c1cd7b9698f7591c5b8c48195b378e52986c2da59a_672.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/5e2649ef7efb5295d0a58591c987965fc9152bbd414ccfa6aced45e1c1c67bae_3352.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/a022ac343d4c9af43189bc1ae01a922c7286de6d56052d48bce2c3b5f0e60808_3252.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/7f67c978a0e142f238410f73fc8d60380017e7ed17e462b2c90f999d108edb1d_3244.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/2f8b974d32b8233e6080d45691a3511f94343f8ee2b553f4e5442102acf93e95_2984.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/d852577519e9d48717aa5eec3500ad40aca5dbc690fef89e7979253ed7cf07e7_3272.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/73babb477ffa530f1f4cd17d68dbca0d4cbc09d3f927ac8bccb16905be23b10d_3376.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/b740a55f36b509962f497e0a7c4148f40ded6adf7549a9fdd99e1dd696b61620_3412.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/162d2326ec5f774c15d0eb2fd23a082c54e97bc81fca31047fc199d8981802c8_3280.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/b71a4a954fffb6f0d9c4ce49c65be796462b9ec4b0c163d8d722beb99acc3e6c_3172.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/e980282147565f1cc67237f09b2d28a13949dea1e3c5a9ce591a26180b904bb5_3232.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/1da7c1039b5f358bf9adf23cb25e7a07215f285c85a4402738edeb22f02937bb_2968.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/d589b81282eb714fed2d8a7d05211cd2f9cebccb41197dae408539a7291b8200_3268.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/3.allaple_0.8/G2097/eb8945a27a6dab15b8043f3eaecc6a7896d4c3d95efcb5a6925a5f6bc747607f_3212.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/35.morstar_0.8/G239/003b68462c055620128b02482cbd488dc65f11d83db82a643b839bf2e35ab9a2_3220.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/69.clickdownload_0.8/G66/0c2dd81c33d50a5ece986c274c06a95fca59e959d761967c1979122b06514334_3276.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/69.clickdownload_0.8/G66/0b9c8282359c2a7de54e6a0d81ac24000d4ab6b218961b725d742adc5de714e3_3144.profile',\n",
       " './data/tree-rep-profiles_o2o/TEST/68.installerex_0.8/G94/e9d6708ef76a7338e81b573625250eb095a75e729c1e64a09aa546c20c22f3a9_3352.profile']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_path_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_add_emb(dev_emb_api,dev_emb,model=model):\n",
    "    intermediate_layer_model = Model(inputs=model.inputs,\n",
    "                                 outputs=model.layers[5].output)\n",
    "    intermediate_output = intermediate_layer_model.predict([valid_emb_api,valid_emb])\n",
    "    print('Emb shape:',intermediate_output.shape)\n",
    "    return intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_ids (InputLayer)           (None, 216)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sent_emb (InputLayer)           (None, 216, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 216)          0           sent_ids[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 216, 768)     0           sent_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "api_emb (Embedding)             (None, 216, 768)     20736       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 216, 768)     0           masking_1[0][0]                  \n",
      "                                                                 api_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "common_extract (GRU)            (None, 216, 192)     553536      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "self_attention (SeqSelfAttentio (None, 216, 192)     12353       common_extract[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "out_rep (TimeDistributed)       (None, 216, 1)       193         self_attention[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 586,818\n",
      "Trainable params: 33,282\n",
      "Non-trainable params: 553,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(451, 216, 768)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "intermediate_layer_model = Model(inputs=model.inputs,\n",
    "                                 outputs=model.layers[5].output)\n",
    "intermediate_output = intermediate_layer_model.predict([valid_emb_api,valid_emb])\n",
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.2618005e-01, -2.1089106e+00, -3.0442843e+00,  5.3742480e-01,\n",
       "        3.0402291e+00,  1.4082639e+00, -3.9007258e-01, -1.2419176e+00,\n",
       "       -3.1372221e+00, -3.2730191e+00,  5.5726153e-01, -5.0289059e-01,\n",
       "        1.0976368e+00, -1.1940066e+00, -2.0536981e+00,  2.4708271e+00,\n",
       "        2.3215792e+00,  3.9515114e+00,  1.9344985e+00, -2.2881236e+00,\n",
       "        2.2857480e+00, -2.0793555e+00,  1.4051611e+00,  3.4353054e+00,\n",
       "       -4.0642748e+00, -1.7742591e+00, -9.2830986e-01, -3.3688855e+00,\n",
       "       -1.1665313e+00, -1.2051904e+00, -2.6467364e+00,  1.9879489e+00,\n",
       "        2.2966270e+00, -7.6591301e-01, -1.3570929e+00,  1.1703807e+00,\n",
       "       -3.3427026e+00, -2.9934566e+00, -1.3248649e-01,  2.1944325e+00,\n",
       "        2.6791778e-01,  4.2409101e-01, -9.6809852e-01, -1.5632174e+00,\n",
       "       -4.7623760e-01,  5.6784534e+00,  2.5199026e-01, -6.0443032e-01,\n",
       "        1.0090823e+00,  3.4446137e+00, -6.5510547e-01, -3.1890869e-03,\n",
       "       -3.0715849e+00, -5.2878947e+00, -1.7584171e+00, -1.5329256e+00,\n",
       "       -5.3499866e+00, -1.0618308e+00,  2.6962829e+00, -3.6501017e+00,\n",
       "       -2.1071072e+00,  2.2717593e+00,  1.2878987e+00,  1.1714743e+00,\n",
       "        3.7443321e+00, -1.8240033e+00, -1.7704272e-01,  2.0444602e-01,\n",
       "        1.1341991e+00, -9.0729839e-01,  9.8190302e-01,  1.6666508e+00,\n",
       "        6.5491515e-01, -2.2530131e+00,  3.2176647e+00,  1.7959783e+00,\n",
       "        3.8473268e+00, -6.4272642e-01,  3.4636095e+00,  2.3742988e+00,\n",
       "        5.7586348e-01,  1.2168137e+00, -7.5332409e-01, -6.6907585e-02,\n",
       "       -4.4247751e+00,  1.7353830e+00, -2.2154385e-01, -5.4202777e-01,\n",
       "       -1.4106377e+00, -1.9397678e+00,  1.5056914e-01, -6.7675710e-01,\n",
       "        5.6524807e-01,  1.2527372e+00, -1.4114931e-01, -1.3739679e+00,\n",
       "        3.8914137e+00, -7.6377034e-01,  2.5704491e-01, -2.3369253e+00,\n",
       "       -7.9854631e-01,  3.2153473e+00, -3.7160110e-01, -3.5700724e+00,\n",
       "       -4.2050381e+00,  4.3426590e+00,  3.0403098e-01,  8.9187384e-01,\n",
       "       -2.3941011e+00,  3.3869023e+00, -9.7409093e-01, -1.5861154e-01,\n",
       "       -1.0184568e+00, -3.2727485e+00,  5.0032675e-01,  5.5998909e-01,\n",
       "        6.6206801e-01, -1.3837624e+00,  2.1286530e+00,  1.5778497e-01,\n",
       "        4.7869635e+00,  6.1913490e-01,  8.7511677e-01, -9.0844351e-01,\n",
       "        2.2507012e+00, -2.1769102e+00, -3.1189983e+00, -2.1515417e-01,\n",
       "        1.2990170e+00,  1.7738521e+00,  1.4425747e+00,  7.7466977e-01,\n",
       "       -2.4886351e+00, -3.1322062e+00,  2.1663506e+00,  1.2077025e+00,\n",
       "       -3.8625842e-01,  3.6903386e+00,  2.5741642e+00, -3.2162442e+00,\n",
       "       -2.8196404e+00,  1.1723533e+00, -4.5414877e+00,  8.3162114e-02,\n",
       "        4.4177790e+00, -1.0515897e+00, -5.8245552e-01, -2.4943449e+00,\n",
       "       -1.3123009e+00, -1.6056761e+00,  2.1978316e+00, -1.3308780e+00,\n",
       "       -3.2645881e+00, -1.7368064e+00, -1.0470686e+00,  5.6208199e-01,\n",
       "       -1.3331319e+00, -2.1710587e-01,  7.8037381e-02, -2.8359869e+00,\n",
       "        3.9966097e+00, -2.3359561e+00,  4.4406095e+00,  9.2199910e-01,\n",
       "        1.8405354e+00, -1.4217708e-01, -2.5370903e+00,  4.4677889e-01,\n",
       "       -2.6407881e+00,  2.8341045e+00, -2.0936103e+00,  2.2192593e+00,\n",
       "       -3.9036951e+00, -1.7797251e+00, -2.5024056e+00, -1.8103858e+00,\n",
       "       -1.2463319e+00, -1.8056867e+00,  3.1768143e+00, -1.3673112e+00,\n",
       "        1.6270926e+00,  2.9640353e-01,  1.1769069e+00, -4.0237942e+00,\n",
       "        1.2830796e+00,  7.3743945e-01, -1.5790038e+00, -6.2885582e-01,\n",
       "       -1.3065978e+00,  3.8313127e-01,  1.3829571e+00, -1.8237269e+00,\n",
       "       -9.7858250e-01, -2.7129970e+00, -1.0652671e+00, -2.8615797e+00,\n",
       "       -4.6720648e+00, -1.4338567e+00, -9.4852984e-01,  9.2590487e-01,\n",
       "        1.1519080e+00, -2.9140708e+00, -1.3796065e+00, -1.6181043e+00,\n",
       "        4.3922871e-01,  3.0415926e+00,  1.2639761e-02,  2.4731612e-01,\n",
       "        3.0445487e+00, -8.5691381e-01,  1.6705543e+00,  3.9914627e+00,\n",
       "        7.3528919e+00,  1.4130155e+00,  1.7977958e+00, -1.5637093e+00,\n",
       "       -5.6399792e-01,  1.6338763e+00, -1.8785927e+00, -1.4506738e+00,\n",
       "       -4.3422100e-01,  2.6310127e+00, -1.5538837e+00, -2.7540658e+00,\n",
       "        3.5133500e+00,  7.0669353e-02,  1.7619373e+00, -5.6331806e+01,\n",
       "        1.2512673e+00,  8.8990748e-02,  1.5667250e+00, -2.2545500e+00,\n",
       "       -1.2714671e+00, -1.1603472e+00, -2.0844429e+00, -2.1886188e-01,\n",
       "       -2.1011992e+00, -1.8377926e+00, -1.9723501e+00, -1.5968513e+00,\n",
       "       -8.6788779e-01, -2.4148057e+00,  2.0578523e+00,  3.7220011e+00,\n",
       "       -1.3561741e+00,  2.3822465e+00,  3.9003894e+00, -8.5912812e-01,\n",
       "       -1.4592311e+00, -9.9676681e-01,  2.7626854e-01,  6.0642147e-01,\n",
       "       -1.4845750e+00,  2.8438702e+00, -5.3183718e+00, -2.4839878e+00,\n",
       "       -3.6258271e+00,  2.2938998e+00, -1.5128180e+00,  8.1248069e-01,\n",
       "        1.5821509e+00,  9.7442305e-01,  1.1890862e+00, -4.2878419e-01,\n",
       "       -1.8754601e-03,  4.2467991e-01, -5.2131718e-01, -3.2253951e-02,\n",
       "       -2.1171241e+00,  2.6398985e+00, -3.4531558e-01,  1.3635888e+00,\n",
       "       -1.1640739e+00, -4.2659658e-01,  9.4151193e-01,  2.2891264e+00,\n",
       "        1.4473779e+00,  1.8811458e+00, -9.4884741e-01, -3.1121764e+00,\n",
       "        1.0977330e+00,  2.0080719e+00,  9.4589806e-01, -7.7491385e-01,\n",
       "       -3.3371935e+00, -3.3734059e+00,  4.0720582e-02,  1.8066956e+00,\n",
       "        2.6082139e+00, -8.0177603e+00,  1.1773885e+00,  3.4037334e-01,\n",
       "        2.4417028e+00, -3.8981562e+00, -2.6017043e-01, -7.9008299e-01,\n",
       "        2.9555650e+00,  2.1079996e+00, -1.0191572e+00, -1.9104371e+00,\n",
       "        2.4574356e-01,  3.1398113e+00, -3.2392961e-01,  2.6172106e+00,\n",
       "       -2.0620324e-01,  1.2185669e+00,  1.7352449e+00,  3.4549433e-01,\n",
       "        8.2377315e-01, -2.4560084e+00, -8.0592895e-01, -3.7939987e-01,\n",
       "       -2.3144469e+00, -1.5159173e+00,  6.2773126e-01, -8.4099489e-01,\n",
       "       -1.1776805e+00, -2.1863987e+00,  4.5686870e+00, -1.1757734e+00,\n",
       "       -1.7649227e+00, -2.1823964e+00, -4.1383996e+00,  3.1191461e+00,\n",
       "       -3.8886881e+00, -1.9736755e+00, -1.0572683e+00, -2.8814614e-01,\n",
       "       -1.3760420e+00,  3.2799816e+00, -1.7470241e-02, -1.4461246e+00,\n",
       "        1.9872670e+00, -4.8107314e-01, -3.9146898e+00,  1.2801113e+00,\n",
       "       -1.0600286e+00, -6.6472018e-01, -1.4049498e+00, -1.0537040e-01,\n",
       "       -2.4595015e+00, -3.2958651e+00, -2.5188537e+00, -2.7769573e+00,\n",
       "        3.1162364e+00,  1.0647461e+00, -2.1229821e-01, -3.0299659e+00,\n",
       "        2.2934723e+00, -1.2990534e+00, -1.2638789e+00, -1.0385491e+00,\n",
       "        1.5554888e+00,  5.8653075e-01, -1.7718971e-01, -3.1588750e+00,\n",
       "        3.6504909e-01,  1.2820141e+00,  1.9572002e-01,  3.4703579e+00,\n",
       "       -1.8240080e+00, -4.2404756e-02, -3.9181645e+00, -6.2433124e-01,\n",
       "       -3.5940733e+00,  4.4836074e-01,  6.4461088e-01, -2.1159024e+00,\n",
       "       -1.5598724e+00,  3.0196078e+00,  3.3306370e+00,  3.1200197e+00,\n",
       "        7.8605592e-02,  3.3566399e+00, -5.3840840e-01,  2.0576580e+00,\n",
       "        8.4790951e-01,  2.1177685e+00,  1.9885027e+00,  2.3595719e+00,\n",
       "       -3.1009338e+00,  3.1085889e+00, -3.2719400e+00, -1.5477544e+00,\n",
       "       -1.9902719e+00,  6.8437427e-01, -2.2179490e-01, -2.4584502e-02,\n",
       "       -1.1010987e+00, -1.6936593e+00,  3.6385906e+00,  1.2423582e+00,\n",
       "        5.0567961e+00, -2.8494325e+00,  1.5497303e-01,  8.5192603e-01,\n",
       "        3.3292955e-01,  2.5143776e+00, -6.6727889e-01,  1.9594125e-01,\n",
       "        2.4584857e-01, -1.3883424e-01, -3.6848605e+00,  1.5819247e+00,\n",
       "       -1.6135231e+00, -1.3215855e+00, -3.3040922e+00, -1.0440323e+00,\n",
       "        5.6788349e-01, -9.3262696e-01,  2.4930079e+00,  5.9524953e-01,\n",
       "        1.3669851e+00, -9.0945399e-01,  1.9101517e+00,  2.8538201e+00,\n",
       "        6.4260149e-01, -2.8383460e+00,  4.5372927e-01, -8.9942777e-01,\n",
       "       -2.0484352e+00, -1.5879128e+00, -9.8043460e-01,  3.6646626e+00,\n",
       "        4.5501029e-01, -1.1821237e+00,  5.8477467e-01, -1.1718513e+00,\n",
       "        7.0925331e-01,  1.3076484e+00, -3.9845440e+00,  1.4733566e+00,\n",
       "       -2.9625583e+00, -3.6505842e-01, -3.9932859e+00,  9.3578005e-01,\n",
       "        3.4513772e-01,  2.4970877e+00,  1.2475128e+00, -6.0676098e-02,\n",
       "        6.6765022e-01,  1.1983831e+00,  2.9071558e-01, -6.0563505e-01,\n",
       "        6.0672241e-01,  1.4155574e+01,  8.7134010e-01, -1.8690730e+00,\n",
       "        2.5130031e+00,  7.6981270e-01, -2.5649723e-01,  1.8816979e+00,\n",
       "       -2.0795293e+00, -4.5055766e+00,  5.9254748e-01, -2.1716759e+00,\n",
       "       -2.8585296e+00,  2.0426524e-01, -8.7569124e-01, -1.1098325e-01,\n",
       "       -1.4795125e+00, -3.8055405e-01,  9.7532010e-01,  3.1810322e+00,\n",
       "        1.2957344e+00,  4.7024465e-01,  1.9070505e+00, -1.4646397e+00,\n",
       "       -1.2817620e+00,  6.3147211e+00, -1.7619433e+00, -6.0244906e-01,\n",
       "       -3.1169093e-01,  2.2333879e+00, -3.8937330e+00, -3.8779960e+00,\n",
       "        1.3735726e+00, -1.4022113e+00,  4.1200118e+00, -1.3044143e+00,\n",
       "        1.1229486e+00, -1.8995985e+00,  2.6234734e-01, -2.1998906e+00,\n",
       "        1.5306678e+00,  1.4326789e+00, -1.4480218e-01, -1.5287441e-01,\n",
       "        1.4590502e+00, -1.4625766e+00, -5.4879600e-01, -1.8375821e+00,\n",
       "        1.2858694e+00, -2.3433638e-01, -1.4596672e+00,  2.3418760e+00,\n",
       "       -7.7758783e-01, -4.4778118e+00,  1.3596536e+00,  5.5769081e+00,\n",
       "       -1.6148343e+00,  2.1220393e+00,  2.2578828e+00,  6.2644253e+00,\n",
       "       -2.8273506e+00,  1.4194127e+00, -2.1699102e+00, -5.5306494e-01,\n",
       "        7.0193863e-01,  3.6330252e+00, -7.5559103e-01,  6.0951495e-01,\n",
       "        2.2326608e+00, -2.6487629e+00, -6.0916930e-01,  3.3578057e+00,\n",
       "       -7.6730406e-01,  6.5866315e-01,  9.4022453e-01, -1.2294950e+00,\n",
       "        5.0447357e-01, -1.4594779e+00,  2.4046302e+00, -1.4065125e+00,\n",
       "       -4.8109946e+00,  4.2542772e+00, -1.4154634e+00,  1.0794113e+00,\n",
       "       -1.1639750e+00,  8.8878065e-01, -9.0986609e-01, -1.5540843e+00,\n",
       "       -2.2517297e-01,  9.4857299e-01,  8.9819247e-01,  1.2258546e+00,\n",
       "        1.7252132e+00,  2.3300240e+00, -1.3225424e+00,  9.5325440e-02,\n",
       "        7.1601713e-01,  1.5167152e+00,  1.9725852e+00,  3.5290116e-01,\n",
       "       -2.9984052e+00, -2.8389466e-01,  1.0749145e+00,  2.1163800e+00,\n",
       "       -1.4686182e+00,  1.1238919e+00,  2.5050476e-01, -2.5056539e+00,\n",
       "       -5.1593220e-01, -1.7440257e+00, -1.4399934e+00, -1.9293545e+00,\n",
       "       -1.3825371e+00,  2.5243723e-01,  3.2441652e+00, -1.4127371e+00,\n",
       "        1.7829170e+00, -1.7325859e+00, -1.8601656e-02, -1.2671387e-01,\n",
       "       -1.3661909e+00, -2.4770610e+00, -1.7852325e+00, -6.7364693e-01,\n",
       "       -1.0618575e+00, -1.9013325e+00,  3.2520094e+00, -1.3219069e+00,\n",
       "       -7.5531662e-01,  4.0314186e-01, -2.1375787e+00, -1.7014313e+00,\n",
       "       -5.6101972e-01,  9.7538531e-01, -1.6543138e+00, -3.6467891e+00,\n",
       "        1.2888930e+00, -1.4256825e+00,  2.6336286e+00, -2.0720419e-01,\n",
       "       -3.9481792e-01,  3.7570860e+00, -3.8357861e+00,  1.0071132e+00,\n",
       "       -2.3441191e+00,  3.2795243e+00,  4.1756086e+00, -6.4615542e-01,\n",
       "       -5.1339493e+00, -1.5026526e+00, -3.3817558e+00,  8.6823833e-01,\n",
       "       -3.8978434e-01, -1.9611555e+00,  8.7615132e-02, -1.6960771e+00,\n",
       "        1.5902566e+00, -2.6575639e+00,  9.1471672e-02,  3.0289176e+00,\n",
       "       -7.7275991e-01, -1.5701786e-02, -1.2099563e+00, -4.9465599e+00,\n",
       "       -1.5197073e+00,  1.6732992e+00,  1.4403059e+00, -1.0171409e+00,\n",
       "       -4.1838322e+00, -2.0683482e+00,  3.8448596e+00, -1.6121529e+00,\n",
       "       -1.1367065e+00,  2.2122860e+00,  4.4993830e+00, -3.3627987e-01,\n",
       "       -1.2405773e+00,  1.3042667e+00, -1.1173258e+00, -6.8121952e-01,\n",
       "        1.6448307e+00, -1.5070166e-01, -3.1206000e-01,  6.9001839e-02,\n",
       "       -1.8225302e+00, -3.7828193e+00, -2.3261352e+00, -1.7486703e+00,\n",
       "       -1.9670390e-02, -1.9671769e+00,  2.8369102e+00,  1.2171748e+00,\n",
       "       -6.0679299e-01, -1.0846927e+00,  4.1568532e+00, -2.2208667e+00,\n",
       "       -8.2260275e-01,  7.1416014e-01, -4.2237558e+00,  3.3039403e+00,\n",
       "        2.7529080e+00,  8.6433995e-01,  7.4932271e-01, -2.9921987e+00,\n",
       "       -1.8405838e+00,  4.9057841e-01,  3.3964820e+00, -1.1889307e+00,\n",
       "       -1.1790679e+00,  1.3674181e+00,  2.6087921e+00,  2.6711531e+00,\n",
       "       -9.0169179e-01, -6.3450251e+00, -1.8184218e+00,  2.6233366e+00,\n",
       "       -1.2914131e+00,  2.7336040e+00,  9.3885416e-01,  1.4653670e+00,\n",
       "        1.6114219e+00,  7.8003472e-01,  9.7708225e-01, -1.8881757e+00,\n",
       "        1.8619999e+00, -2.4667984e-01, -1.1067487e+00, -4.1220222e+00,\n",
       "       -1.8021458e+00,  2.0964627e+00,  3.7516659e-01, -1.6199917e-01,\n",
       "       -3.4912059e+00, -1.7271870e+00, -4.0209575e+00, -3.3957654e-01,\n",
       "       -2.8472641e+00,  2.2181592e+00,  3.1609446e-01,  1.2831833e+00,\n",
       "        1.3637497e+00, -2.7740941e+00,  1.6966090e+00, -2.9228812e-01,\n",
       "        1.5422961e+00,  2.1877158e-01,  7.5507611e-01, -1.3161430e+00,\n",
       "       -2.5506726e-01,  2.0417223e+00, -4.1040301e-02,  1.8366364e-01,\n",
       "        5.1438057e-01, -2.0085950e+00, -1.2958627e+00,  1.1358243e+00,\n",
       "       -3.6013317e-01,  2.2643743e+00, -3.2011297e+00,  3.3882322e+00,\n",
       "        8.8739347e-01, -2.2398489e+00, -1.6406391e+00,  9.7168094e-01,\n",
       "       -2.1170907e+00, -5.4719651e-01, -1.8364561e+00,  8.9777166e-01,\n",
       "       -3.1342834e-01, -8.2396901e-01, -1.6983945e+00,  3.4725213e+00,\n",
       "        1.1961882e+00,  7.7454722e-01,  8.9408380e-01,  1.0106236e+00,\n",
       "        5.7904828e-01,  4.3254924e+00, -2.5943859e+00,  1.1281104e+00,\n",
       "       -1.4332250e+00,  5.9840155e-01, -2.1843262e+00, -9.5976734e-01,\n",
       "       -2.3718734e+00, -4.8139095e-01, -2.0060658e-02,  1.3934039e+00,\n",
       "        2.0212684e+00,  2.7970109e+00,  3.3554101e-01, -3.6619514e-01,\n",
       "        1.3643111e+00, -5.6208506e-02, -1.3549960e-01, -1.3406677e+00,\n",
       "       -7.3653311e-01, -3.2736287e+00,  3.6519468e-01, -1.7658801e+00,\n",
       "       -1.2505858e+00,  3.8131371e-01,  3.1511221e+00,  3.1327462e+00,\n",
       "       -3.0437207e+00,  1.7272880e+00,  2.4538751e+00, -1.3583162e+00,\n",
       "        6.2583499e+00, -1.2605146e+00,  2.0508542e+00,  2.2458038e+00,\n",
       "       -2.1192026e+00,  2.6480391e+00,  1.1440415e+00,  4.0398705e-01,\n",
       "       -5.9819460e+00, -2.4303894e+00,  4.1538697e-01, -2.1172292e+00,\n",
       "        9.3021512e-02,  2.2966540e+00,  1.2198675e+00, -7.3053360e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 235.73it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_length = []\n",
    "root_dir = './data/tree-rep-profiles_o2o/DEV/'\n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir +fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        for hl_f in hl_list:\n",
    "            with open(hl_f,encoding='ISO 8859-1') as f: #X2\n",
    "                lines = f.read()\n",
    "            lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "            lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "            lines = re.sub('=\\\\n','',lines)\n",
    "            lines = lines.splitlines()\n",
    "            dev_length.append(len(lines))\n",
    "#             print(len(lines))\n",
    "#             break\n",
    "test_length = []\n",
    "root_dir = './data/tree-rep-profiles_o2o/TEST/'\n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "for fam in tqdm(fam_dir):\n",
    "    tree_dir = next(os.walk(root_dir +fam))[1]\n",
    "    for tree in tree_dir:\n",
    "        in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "        hl_list = next(os.walk(in_directory))[2]\n",
    "        hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "        hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "        for hl_f in hl_list:\n",
    "            with open(hl_f,encoding='ISO 8859-1') as f: #X2\n",
    "                lines = f.read()\n",
    "            lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "            lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "            lines = re.sub('=\\\\n','',lines)\n",
    "            lines = lines.splitlines()\n",
    "            test_length.append(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ans,length in zip(ans_v,dev_length):\n",
    "    assert ans[:length][-1] != 0.49756694\n",
    "    assert ans[:length+1][-1] < 0.49756694\n",
    "    assert ans[:length+1][-1] > 0.49756693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[:length+1][-1] > 0.49756693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49756694"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[:105][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49756694"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_t[-1][:200][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
