{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys,glob\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "from keras.utils import *\n",
    "from keras.utils.generic_utils import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.preprocessing.image import *\n",
    "from multiprocessing import *\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.manifold import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn import preprocessing\n",
    "import sent2vec\n",
    "import re\n",
    "import string\n",
    "import unicodedata as udata\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('model/o2o_o2m_Sent2Vec_lower_woParam_0616_768.bin') # sent2vec model\n",
    "all_df = pd.read_csv('./data/tree-rep-profiles-partial/process2family_df.csv')\n",
    "api_li = ['LoadLibrary',\n",
    "'CreateProcess',\n",
    "'OpenProcess',\n",
    "'ExitProcess',\n",
    "'TerminateProcess',\n",
    "'WinExec',\n",
    "'CreateRemoteThread',\n",
    "'CreateThread',\n",
    "'CopyFile',\n",
    "'CreateFile',\n",
    "'DeleteFile',\n",
    "'RegSetValue',\n",
    "'RegCreateKey',\n",
    "'RegDeleteKey',\n",
    "'RegDeleteValue',\n",
    "'RegQueryValue',\n",
    "'RegEnumValue',\n",
    "'WinHttpConnect',\n",
    "'WinHttpOpen',\n",
    "'WinHttpOpenRequest',\n",
    "'WinHttpReadData',\n",
    "'WinHttpSendRequest',\n",
    "# 'WinHttpWriteData', #少了\n",
    "'InternetOpen',\n",
    "'InternetConnect',\n",
    "'HttpSendRequest',\n",
    "'GetUrlCacheEntryInfo']\n",
    "api_li = [x.lower() for x in api_li] #lowrer case?\n",
    "len(api_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LoadLibrary': 1,\n",
       " 'CreateProcess': 2,\n",
       " 'OpenProcess': 3,\n",
       " 'ExitProcess': 4,\n",
       " 'TerminateProcess': 5,\n",
       " 'WinExec': 6,\n",
       " 'CreateRemoteThread': 7,\n",
       " 'CreateThread': 8,\n",
       " 'CopyFile': 9,\n",
       " 'CreateFile': 10,\n",
       " 'DeleteFile': 11,\n",
       " 'RegSetValue': 12,\n",
       " 'RegCreateKey': 13,\n",
       " 'RegDeleteKey': 14,\n",
       " 'RegDeleteValue': 15,\n",
       " 'RegQueryValue': 16,\n",
       " 'RegEnumValue': 17,\n",
       " 'WinHttpConnect': 18,\n",
       " 'WinHttpOpen': 19,\n",
       " 'WinHttpOpenRequest': 20,\n",
       " 'WinHttpReadData': 21,\n",
       " 'WinHttpSendRequest': 22,\n",
       " 'InternetOpen': 23,\n",
       " 'InternetConnect': 24,\n",
       " 'HttpSendRequest': 25,\n",
       " 'GetUrlCacheEntryInfo': 26}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode_dict = {}\n",
    "# for i,v in enumerate(api_li):\n",
    "#     encode_dict[v] = i+1\n",
    "# pickle.dump(file=open('data/tree-rep-profiles-partial/encode_dict.pkl','wb'),obj=encode_dict)\n",
    "# encode_dict\n",
    "encode_dict = pickle.load(open('data/tree-rep-profiles-partial/encode_dict.pkl','rb'))\n",
    "encode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loadlibrary': 1,\n",
       " 'createprocess': 2,\n",
       " 'openprocess': 3,\n",
       " 'exitprocess': 4,\n",
       " 'terminateprocess': 5,\n",
       " 'winexec': 6,\n",
       " 'createremotethread': 7,\n",
       " 'createthread': 8,\n",
       " 'copyfile': 9,\n",
       " 'createfile': 10,\n",
       " 'deletefile': 11,\n",
       " 'regsetvalue': 12,\n",
       " 'regcreatekey': 13,\n",
       " 'regdeletekey': 14,\n",
       " 'regdeletevalue': 15,\n",
       " 'regqueryvalue': 16,\n",
       " 'regenumvalue': 17,\n",
       " 'winhttpconnect': 18,\n",
       " 'winhttpopen': 19,\n",
       " 'winhttpopenrequest': 20,\n",
       " 'winhttpreaddata': 21,\n",
       " 'winhttpsendrequest': 22,\n",
       " 'internetopen': 23,\n",
       " 'internetconnect': 24,\n",
       " 'httpsendrequest': 25,\n",
       " 'geturlcacheentryinfo': 26}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower = True:\n",
    "\n",
    "encode_dict_lower = {}\n",
    "for i,v in encode_dict.items():\n",
    "    encode_dict_lower[i.lower()] = v\n",
    "encode_dict_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3.kazy',\n",
       " '4.zbot',\n",
       " '6.allaple',\n",
       " '11.virtob',\n",
       " '12.loadmoney',\n",
       " '13.graftor',\n",
       " '14.vobfus',\n",
       " '17.expiro',\n",
       " '18.sytro',\n",
       " '21.ramnit',\n",
       " '22.domaiq',\n",
       " '27.softpulse',\n",
       " '29.delf',\n",
       " '30.mplug',\n",
       " '32.browsefox',\n",
       " '33.autoit',\n",
       " '37.parite',\n",
       " '38.msil',\n",
       " '41.outbrowse',\n",
       " '42.sirefef',\n",
       " '43.mira',\n",
       " '44.screensaver',\n",
       " '45.firseria',\n",
       " '47.somoto',\n",
       " '51.shodi',\n",
       " '54.soltern',\n",
       " '55.zygug',\n",
       " '59.megasearch',\n",
       " '60.conjar',\n",
       " '63.jorik',\n",
       " '64.installerex',\n",
       " '65.shipup',\n",
       " '66.bdmj',\n",
       " '71.eggnog',\n",
       " '75.fesber',\n",
       " '78.vilsel',\n",
       " '81.picsys',\n",
       " '84.directdow',\n",
       " '88.clickdownload',\n",
       " '103.yantai',\n",
       " '104.mresmon',\n",
       " '111.vbran',\n",
       " '124.avmh',\n",
       " '151.downloadadmin']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = './data/tree-rep-profiles-partial/normal/'\n",
    "av_col = all_df.columns[1:].tolist()\n",
    "fam_dir = next(os.walk(root_dir))[1]\n",
    "fam_dir = [x.split('_')[0] for x in fam_dir]\n",
    "av_df_col = []\n",
    "for name in av_col:\n",
    "    if name in fam_dir:\n",
    "        av_df_col.append(name)\n",
    "# av_df_col = ['profile'] + av_df_col\n",
    "print(len(av_df_col))\n",
    "av_df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fam_data(root_dir,samples_pid,av_df_col=av_df_col):\n",
    "# root_dir = './data/tree-rep-profiles-partial/normal/'\n",
    "# samples_pid = samples_pid_train\n",
    "    name_df = pd.DataFrame(0,columns=av_df_col,index=samples_pid)\n",
    "    for pid in tqdm(samples_pid):\n",
    "        paths = glob.glob(root_dir +'*/*/'+pid )\n",
    "        name_li = []\n",
    "        for path in paths:\n",
    "            name_li.append(path.split('/')[4].split('_')[0])\n",
    "        name_df.loc[pid,list(set(name_li))] = 1\n",
    "    return name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_pid_train = pickle.load(file=open('./data/tree-rep-profiles-partial/TRAIN_pidNames.pkl','rb'))\n",
    "samples_pid_valid = pickle.load(file=open('./data/tree-rep-profiles-partial/DEV_pidNames.pkl','rb'))\n",
    "samples_pid_test = pickle.load(file=open('./data/tree-rep-profiles-partial/TEST_pidNames.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7771/7771 [01:42<00:00, 76.10it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 305.19it/s]\n",
      "100%|██████████| 264/264 [00:00<00:00, 301.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# root_dir = './data/tree-rep-profiles-partial/normal/'\n",
    "# train_fam_df = create_fam_data(root_dir,samples_pid_train)\n",
    "# root_dir = './data/tree-rep-profiles-partial/DEV/'\n",
    "# valid_fam_df = create_fam_data(root_dir,samples_pid_valid)\n",
    "# root_dir = './data/tree-rep-profiles-partial/TEST/'\n",
    "# test_fam_df = create_fam_data(root_dir,samples_pid_test)\n",
    "train_fam_df, valid_fam_df,test_fam_df = pickle.load(open('data/tree-rep-profiles-partial/fam_df.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byterep_ans(byterep_list,max_length):\n",
    "    '''\n",
    "    Input: byte sequenxe (list of pickles path) \n",
    "    '''\n",
    "    rep_list=[]\n",
    "    for byterep in byterep_list:\n",
    "        rep = pickle.load(open(byterep,'rb'))\n",
    "        if len(rep) < max_length:\n",
    "            for _ in range(max_length-len(rep)):\n",
    "                rep.append(0)\n",
    "        rep_list.append(rep)\n",
    "    return rep_list\n",
    "\n",
    "# 寫不同模組 來處理x2 y1 y2\n",
    "def Sent2vec_emb(root_dir,all_df,max_length,model=model,dim=768,encode_dict=encode_dict,av_df_col=av_df_col):\n",
    "    '''\n",
    "    Create sent2vec profile embedding\n",
    "    Input: root_dir=>fam-tree-profile\n",
    "    Output: \n",
    "    '''\n",
    "    sent_pad = [0]*dim\n",
    "    all_profiles = []\n",
    "    all_api_name = []\n",
    "    all_fam_ans = []\n",
    "    all_byterep=[]\n",
    "    sen2vec_dim_normalize=[]\n",
    "    sen2vec_length_normalize = []\n",
    "    fam_dir = next(os.walk(root_dir))[1]\n",
    "    for fam in tqdm(fam_dir):\n",
    "        tree_dir = next(os.walk(root_dir + fam))[1]\n",
    "        for tree in tree_dir:\n",
    "            in_directory = root_dir + fam +  '/' + tree + '/'\n",
    "            hl_list = next(os.walk(in_directory))[2]\n",
    "            hl_list = list(filter(lambda f: f.endswith(\".profile\"), hl_list))\n",
    "            byterep_list = [x.split('.')[0]+'_byterep.pickle' for x in hl_list]\n",
    "            hl_list = [os.path.join(in_directory, f) for f in hl_list]\n",
    "            byterep_list = [os.path.join(in_directory, f) for f in byterep_list]\n",
    "            rep_list = byterep_ans(byterep_list,max_length)\n",
    "            for profile in hl_list:\n",
    "                pid = profile.split('/')[-1]#.split('.')[0]\n",
    "#                 profile_df = all_df[all_df.profile == pid]\n",
    "                profile_df = all_df[all_df.index == pid]\n",
    "                fam_ans = list(profile_df.values[0])\n",
    "#                 fam_ans = list(profile_df[av_df_col].values[0])\n",
    "\n",
    "                profile_emb = []\n",
    "                func_name_emb = []\n",
    "                with open(profile,encoding='ISO 8859-1') as f:\n",
    "                    lines = f.read()\n",
    "                lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "                lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c]+','', lines)\n",
    "                lines = lines.splitlines()\n",
    "                for line in lines:\n",
    "                    temp = re.sub(dil,\" \",line.lower()) # lower? 跟先前一致\n",
    "                    temp = temp.split(\" \")\n",
    "                    temp = list(filter(None, temp))\n",
    "                    temp = ' '.join(temp)\n",
    "                    func_name = temp.split(' ')[0]\n",
    "                    if func_name not in api_li:\n",
    "                        print('=ERROR:=',profile,'=>',temp)\n",
    "                    func_name_emb.append(encode_dict[func_name])\n",
    "                    emb = model.embed_sentence(temp)\n",
    "                    emb = emb[0]\n",
    "                    profile_emb.append(emb)\n",
    "                    sen2vec_dim_normalize.append(emb)\n",
    "                    sen2vec_length_normalize.append(len(lines))\n",
    "                if len(lines) < max_length:\n",
    "                    for _ in range(max_length-len(lines)):\n",
    "                        profile_emb.append(sent_pad)\n",
    "                all_profiles.append(profile_emb)\n",
    "                all_api_name.append(func_name_emb)\n",
    "                all_fam_ans.append(fam_ans)\n",
    "            all_byterep.extend(rep_list)\n",
    "    all_api_name = pad_sequences(all_api_name,maxlen=max_length,padding='post',value=0)\n",
    "    return np.array(all_profiles) , np.array(all_api_name) , np.array(all_fam_ans) , np.array(all_byterep),np.array(sen2vec_dim_normalize),sen2vec_length_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [03:47<00:00,  7.23s/it]\n",
      "  0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train of sent2vec vector: (11141, 213, 768) (11141, 213) (11141, 44) (11141, 213)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:07<00:00,  4.74it/s]\n",
      "  2%|▏         | 1/44 [00:00<00:06,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid of sent2vec vector: (437, 213, 768) (437, 213) (437, 44) (437, 213)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:07<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test of sent2vec vector: (424, 213, 768) (424, 213) (424, 44) (424, 213)\n"
     ]
    }
   ],
   "source": [
    "dil= r\"[,.;\\-+^()/@#?!&$:{}\\\\*%~\\'\\\"\\=\\_]+\\ *\" #等號、底線被保留，注意要跟先前一致\n",
    "max_length = 213\n",
    "\n",
    "root_dir = './data/tree-rep-profiles-partial/normal/'\n",
    "train_emb, train_emb_api, train_fam_ans, train_rep_ans, train_norm,train_norm_len = Sent2vec_emb(root_dir,train_fam_df,max_length,encode_dict=encode_dict_lower)\n",
    "print('train of sent2vec vector:',train_emb.shape,train_emb_api.shape,train_fam_ans.shape,train_rep_ans.shape)\n",
    "\n",
    "root_dir = './data/tree-rep-profiles-partial/DEV/'\n",
    "valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans, valid_norm,valid_norm_len  = Sent2vec_emb(root_dir,valid_fam_df,max_length,encode_dict=encode_dict_lower)\n",
    "print('valid of sent2vec vector:',valid_emb.shape,valid_emb_api.shape,valid_fam_ans.shape,valid_rep_ans.shape)\n",
    "\n",
    "root_dir = './data/tree-rep-profiles-partial/TEST/'\n",
    "test_emb, test_emb_api,test_fam_ans,test_rep_ans, test_norm,test_norm_len = Sent2vec_emb(root_dir,test_fam_df,max_length,encode_dict=encode_dict_lower)\n",
    "print('test of sent2vec vector:',test_emb.shape,test_emb_api.shape,test_fam_ans.shape,test_rep_ans.shape)\n",
    "\n",
    "for val in test_emb:\n",
    "    if len(val) != max_length:\n",
    "        print(len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize dim-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768,), (768,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alls = np.concatenate([test_norm,valid_norm,train_norm])\n",
    "mean = np.mean(alls,axis=0)\n",
    "std = np.std(alls,axis=0)\n",
    "mean.shape , std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dim(emb,length,mean=mean,std=std):\n",
    "    sent_emb = (emb[:length,:] - mean)/std\n",
    "    final_emb = np.concatenate([sent_emb, emb[length:,:]],axis=0)\n",
    "    return final_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb_norm = []\n",
    "valid_emb_norm = []\n",
    "train_emb_norm = []\n",
    "for emb,length in zip(train_emb,train_norm_len):\n",
    "    emb_norm = normalize_dim(emb,length)\n",
    "    train_emb_norm.append(emb_norm)\n",
    "for emb,length in zip(valid_emb,valid_norm_len):\n",
    "    emb_norm = normalize_dim(emb,length)\n",
    "    valid_emb_norm.append(emb_norm)\n",
    "for emb,length in zip(test_emb,test_norm_len):\n",
    "    emb_norm = normalize_dim(emb,length)\n",
    "    test_emb_norm.append(emb_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11141, 213, 768) (437, 213, 768) (424, 213, 768)\n"
     ]
    }
   ],
   "source": [
    "train_emb = np.array(train_emb_norm)\n",
    "valid_emb = np.array(valid_emb_norm)\n",
    "test_emb = np.array(test_emb_norm)\n",
    "print(train_emb.shape,valid_emb.shape,test_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(file=open('data/tree-rep-profiles-partial/TRAIN_sent2vec_vec.pkl','wb'),protocol=4,obj=(train_emb, train_emb_api, train_fam_ans, train_rep_ans))\n",
    "pickle.dump(file=open('data/tree-rep-profiles-partial/DEV_sent2vec_vec.pkl','wb'),protocol=4,obj=(valid_emb, valid_emb_api,valid_fam_ans,valid_rep_ans))\n",
    "pickle.dump(file=open('data/tree-rep-profiles-partial/TEST_sent2vec_vec.pkl','wb'),protocol=4,obj=(test_emb, test_emb_api,test_fam_ans,test_rep_ans))\n",
    "# pickle.dump(file=open('data/tree-rep-profiles-partial/fam_df.pkl','wb'),protocol=4,obj=(train_fam_df, valid_fam_df,test_fam_df))\n",
    "pickle.dump(file=open('data/tree-rep-profiles-partial/norm_mean_std.pkl','wb'),protocol=4,obj=(mean,std))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
