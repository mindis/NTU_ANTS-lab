{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisite packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil,pickle,tqdm,sys,random,re,string,pause, datetime,glob\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "# from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "from keras.metrics import *\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from collections import Counter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import unicodedata as udata\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "from keras import backend \n",
    "from imblearn.ensemble import *\n",
    "from imblearn.combine import *\n",
    "# from python.keras import backend \n",
    "# Embedding(10,20)\n",
    "from keras_transformer.extras import ReusableEmbedding, TiedOutputEmbedding\n",
    "from keras_transformer.position import TransformerCoordinateEmbedding\n",
    "from keras_transformer.transformer import TransformerACT, TransformerBlock\n",
    "from keras_transformer.bert import (\n",
    "    BatchGeneratorForBERT, masked_perplexity,\n",
    "    MaskedPenalizedSparseCategoricalCrossentropy)\n",
    "\n",
    "import keras_metrics as km\n",
    "from keras_trans_mask import RemoveMask, RestoreMask\n",
    "\n",
    "from keras_multi_head import *\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "from sklearn.metrics.pairwise import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.manifold import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neede files in same dir.\n",
    "from models import transformer_bert_model\n",
    "from bpe import BPEEncoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/tree-rep-profiles_o2o/'\n",
    "train_pid_paths = pickle.load(open(root_dir + 'train_pid_paths.pkl','rb'))\n",
    "valid_pid_paths = pickle.load(open(root_dir + 'valid_pid_paths.pkl','rb'))\n",
    "test_pid_paths = pickle.load(open(root_dir + 'test_pid_paths.pkl','rb'))\n",
    "exp_pid_paths = pickle.load(open(root_dir + 'exp_pid_paths.pkl','rb'))\n",
    "# o2o_pid_paths = pickle.load(open(root_dir + 'o2o_pid_paths.pkl','rb'))\n",
    "\n",
    "train_emb_api,train_emb , train_rep_ans = pickle.load(open(root_dir + 'pids_train.pkl','rb'))\n",
    "valid_emb_api,valid_emb, valid_rep_ans = pickle.load(open(root_dir + 'pids_valid.pkl','rb'))\n",
    "test_emb_api,test_emb ,test_rep_ans = pickle.load(open(root_dir + 'pids_test.pkl','rb'))\n",
    "# o2o_api,o2o_emb,o2o_rep = pickle.load(open(root_dir + 'pids_o2o.pkl','rb'))\n",
    "exp_api,exp_emb = pickle.load(open(root_dir + 'pids_exp.pkl','rb'))\n",
    "emb_matrix = pickle.load(open('data/api_emb_matrix.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_statistics(all_length):\n",
    "    '''\n",
    "    input: length list of elements\n",
    "    output1: mean、std、mode、min、q1、median(q2)、q3、max、iqr、outlier、far out\n",
    "    output2: statistics graph、10%~90% form\n",
    "    '''\n",
    "    stat_dict = {}\n",
    "    stat_dict['mean'] = np.mean(all_length)\n",
    "    stat_dict['std'] = np.std(all_length)\n",
    "    stat_dict['mode'] = np.argmax(np.bincount(all_length))\n",
    "    stat_dict['min'] = np.min(all_length)\n",
    "    stat_dict['q1'] = np.quantile(all_length,0.25)\n",
    "    stat_dict['median'] = np.quantile(all_length,0.5)\n",
    "    stat_dict['q3'] = np.quantile(all_length,0.75)\n",
    "    stat_dict['max'] = np.max(all_length)\n",
    "    stat_dict['iqr'] = stat_dict['q3'] - stat_dict['q1']\n",
    "    stat_dict['outlier'] = stat_dict['q3'] + 1.5*stat_dict['iqr']\n",
    "    stat_dict['far_out'] = stat_dict['q3'] + 3*stat_dict['iqr']\n",
    "    for i in [10,20,30,40,50,60,70,80,90,100]:\n",
    "        stat_dict[str(i)+'%'] = np.percentile(all_length,i)\n",
    "    return pd.DataFrame.from_dict(stat_dict,orient='index',columns=['length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load trained NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_acc(y_true, y_pred):\n",
    "    return binary_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './model/o2o_stage_gru_selfatt/XXX.h5' #訓練好的模型\n",
    "model = load_model(model_save_path+'_all.h5',custom_objects={'bin_acc': bin_acc, 'SeqSelfAttention': SeqSelfAttention})\n",
    "# model = load_model(model_save_path+'_all.h5',custom_objects={'bin_acc': bin_acc,'RemoveMask':RemoveMask,'RestoreMask':RestoreMask})#, 'SeqSelfAttention': SeqSelfAttention}) #如果上面的load失敗的話，可以用此\n",
    "model.load_weights(model_save_path)\n",
    "model.summary()\n",
    "thr = 0.51 #0.482 #設定0004.於驗證資料集中所得到最佳的threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = Model(inputs=model.inputs,  outputs=model.layers[5].output) \n",
    "#.layers[5].中的數字可能要調整，讓emb_model最後一層剛好是最終的embedding相加(Add)\n",
    "emb_model.load_weights(model_save_path,by_name=True)\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representative execution pattern vector\n",
    "* 一個profile所有important call invocation於向量空間中的表示法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要記得先手動更改以下參數!! (依據不同dataset load近來的參數名稱會不同)\n",
    "def avg_train_vector(root_dir,thr,train_pid_paths=exp_pid_paths,train_emb_api=exp_api,\n",
    "                     train_emb=exp_emb,loner=False):\n",
    "    '''\n",
    "    train_pid_paths: 每一筆profile路徑位置\n",
    "    train_emb_api: api embedding vectors\n",
    "    train_emb: SentVec embedding vectors\n",
    "    loner: False=>train/dev/test Set ; True: Loner Set\n",
    "    '''\n",
    "    no_ind_rep = 0\n",
    "    no_ind_rep_li = []\n",
    "    tree_vec = []\n",
    "    empty_hash_li = []\n",
    "    problem_hash_li = []\n",
    "    un_aliase_path = './data/tree-rep-profiles_o2o/EXP_rev/'\n",
    "    if not os.path.exists(root_dir):\n",
    "        os.makedirs(root_dir,exist_ok=True)\n",
    "    for path,api,emb in tqdm(zip(train_pid_paths,train_emb_api,train_emb)): #tqdm\n",
    "        if loner:\n",
    "            if path not in (claim_path + unk_path):\n",
    "                continue\n",
    "            recent_fam = path.split('/')[-3] #fakeav_fakealeart\n",
    "            recent_hash = path.split('/')[-1].split('_')[0]\n",
    "            if '_' in recent_fam:\n",
    "                all_path_un_aliase = glob.glob(un_aliase_path+'*/*/'+current_hash+'*')\n",
    "                recent_fam = all_path_un_aliase[0].split('/')[4]\n",
    "                \n",
    "        else:\n",
    "            recent_fam = path.split('/')[-3].split('_')[0]\n",
    "        recent_tree = path.split('/')[-2]\n",
    "        hash_id = path.split('/')[-1][:5]\n",
    "        pid_id = path.split('/')[-1].split('_')[-1].split('.')[0][-3:]\n",
    "        with open(path,encoding='ISO 8859-1') as f: #X2\n",
    "            lines = f.read()\n",
    "        lines = re.sub(r'[^\\x00-\\x7F]+','', lines)\n",
    "        lines = re.sub(r'[\\x1e\\x7f\\x15\\x10\\x0c\\x1c]+','', lines)\n",
    "        lines = re.sub('=\\\\n','',lines)\n",
    "        lines = re.sub('y\\\\n','',lines)\n",
    "        lines = lines.splitlines()\n",
    "        length = len(lines)\n",
    "        if loner:\n",
    "            if length < 2:\n",
    "                empty_hash_li.append(path)\n",
    "                continue\n",
    "        else:\n",
    "            if length < 11:\n",
    "                print('No common rep. Should not in a tree:',path)\n",
    "                problem_hash_li.append(path)\n",
    "                continue\n",
    "        try:\n",
    "            assert api[:length][-1] != 0\n",
    "            assert api[:length+1][-1] == 0\n",
    "        except AssertionError:\n",
    "            print('Mask assertion ERR:',path)\n",
    "            continue\n",
    "        api_e = np.expand_dims(api, axis=0)\n",
    "        emb_e = np.expand_dims(emb, axis=0)\n",
    "        final_emb_e = emb_model.predict([api_e,emb_e]) #(1,length,768)\n",
    "        final_emb = final_emb_e[0][:length] #(length,768)\n",
    "        byte_rep_e = model.predict([api_e,emb_e])\n",
    "        byte_rep = byte_rep_e[0][:length] # (length,1)\n",
    "        byte_rep_thr = []\n",
    "        for num in byte_rep:\n",
    "            if num[0] < thr:\n",
    "                byte_rep_thr.append(0)\n",
    "            else:\n",
    "                byte_rep_thr.append(1) # high than threshold, keep it\n",
    "        byte_rep_thr = np.array(byte_rep_thr)\n",
    "        byte_rep_thr = np.expand_dims(byte_rep_thr,axis=-1)\n",
    "        mul_emb = np.multiply(final_emb,byte_rep_thr)\n",
    "        rep_emb = [] #超過threshold的emb才留下來\n",
    "        for emb in mul_emb:\n",
    "            if (sum(emb) <0) or (sum(emb)>0):\n",
    "                rep_emb.append(emb)\n",
    "        rep_emb_avg = np.average(rep_emb,axis=0) #把自己各api的embedding取平均，應該一定要有，不然就代表這個profile不該屬於這棵tree \n",
    "        try:\n",
    "            if not (rep_emb_avg <0 or rep_emb_avg>0):\n",
    "                no_ind_rep+=1\n",
    "                no_ind_rep_li.append(path)\n",
    "                continue #沒有individual REP\n",
    "        except ValueError: #正常profile\n",
    "            pass\n",
    "        rep_emb_avg = np.expand_dims(rep_emb_avg,axis=0)\n",
    "        pickle.dump(file=open(root_dir+recent_fam+'_'+recent_tree+'_'+hash_id+'_'+pid_id+'.pkl','wb'),obj=rep_emb_avg)\n",
    "    print('No ind REP#:',no_ind_rep)\n",
    "    if loner:\n",
    "        return no_ind_rep_li,empty_hash_li,problem_hash_li\n",
    "    return no_ind_rep_li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "產生train/valid/test的r，用以下，要更改root_dir、及上述function的輸入變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/valid/test要改三次跑三次\n",
    "root_dir = './data/tree-rep-profiles_o2o/trainingProfile_REP/' #最終輸出r的資料夾，會自動創立資料夾，存放pkl，記得最後的斜線\n",
    "no_ind_rep = avg_train_vector(root_dir=root_dir,thr=thr,loner=False)\n",
    "with open(root_dir + 'no_rep_file.txt','w') as fp:\n",
    "    for path in no_ind_rep:\n",
    "        fp.write(str(path.split('/')[-3:])+'\\n') #no rep的profile path紀錄檔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "產生loner的r，用以下，要更改root_dir(輸出r的資料夾)、及上述function的輸入變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loner\n",
    "loner_dir = './data/tree-rep-profiles_o2o/lonerProfile_REP/'\n",
    "no_ind_rep,empty_files,problem_files = avg_train_vector(root_dir=loner_dir,thr=0.482,loner=True)\n",
    "with open(loner_dir + 'no_rep_file.txt','w') as fp:\n",
    "    for path in no_ind_rep:\n",
    "        fp.write(str(path.split('/')[-1:])+'\\n') #-no rep的profile path紀錄檔\n",
    "with open(loner_dir + 'empty_file.txt','w') as fp:\n",
    "    for path in empty_files:\n",
    "        fp.write(str(path.split('/')[-1:])+'\\n') #空的或太短的profile\n",
    "with open(loner_dir + 'error_file.txt','w') as fp:\n",
    "    for path in problem_files:\n",
    "        fp.write(str(path.split('/')[-1:])+'\\n') #有問題的profile(assertion failed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
